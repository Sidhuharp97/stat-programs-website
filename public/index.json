[{"authors":null,"categories":null,"content":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\n","date":1636275600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1636275600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/statistical-programs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/statistical-programs/","section":"authors","summary":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.","tags":null,"title":"Statistical Programs","type":"authors"},{"authors":null,"categories":null,"content":"Julia Piaskowski is a consulting statistician at the University of Idaho.\n","date":1618444800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1618444800,"objectID":"ea76c2c583835370cabcc577a3ff91a8","permalink":"/author/julia-piaskowski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/julia-piaskowski/","section":"authors","summary":"Julia Piaskowski is a consulting statistician at the University of Idaho.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cc81c076516116756d3f5a4a4fb9202","permalink":"/author/william-price/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/william-price/","section":"authors","summary":"","tags":null,"title":"William Price","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb2df55dd082c12a119ff230831261b","permalink":"/author/xin-dai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-dai/","section":"authors","summary":"","tags":null,"title":"Xin Dai","type":"authors"},{"authors":null,"categories":null,"content":"\r\r Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data. The workshop draws heavily from our book on this subject.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n\rcheck your system\r\r\rSAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SASÂ® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nData sets\nThe following files will be used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n\rbegin the workshop\r\r\rThis workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"5bdfbb99e430a3c1a2a4056386e4765a","permalink":"/workshops/spatial-workshop/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/","section":"workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials","type":"book"},{"authors":null,"categories":null,"content":"\r\r Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n\rcheck your system\r\r\rSAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SASÂ® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n\rbegin the workshop\r\r\rThis workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"4a1872e9499ca9ac572da527f1327e4b","permalink":"/draft-workshops/spatial_workshop_new/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/","section":"draft-workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials II","type":"book"},{"authors":null,"categories":null,"content":"\r\r Table of Contents\r What you will learn Program overview Courses in this program Meet your instructor FAQs  \r\rWhat you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program \rPython basics\rBuild a foundation in Python. \r\rVisualization\r\r\rStatistics\rIntroduction to statistics for data science. \r\rMeet your instructor Statistical Programs FAQs Are there prerequisites?\rThere are no prerequisites for the first course.\n How often do the courses run?\rContinuously, at your own pace.\n \rBegin the course\r\r\r","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"3455929a3b128d6d930e25725df43dc4","permalink":"/draft-workshops/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/draft-workshops/example/","section":"draft-workshops","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"ðŸ“Š Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"\r Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()\r If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()\r If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation\r\u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting\r\u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling\r\u0026quot;SpATS\u0026quot;, # for fitting splines\r\u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;spaMM\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling\rinstall.packages(package_list)\rsapply(package_list, require, character.only = TRUE)\r \rPlease note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please attempt installation in advance of the workshop. The packages have all been successfully installed if after the sapply(package_list, require, character.only = TRUE) is run, the R output is \u0026ldquo;TRUE\u0026rdquo; for each package. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.\r\r\rLibrary Information    package usage     dplyr, tidyr, standard data manipulation   purrr for repeat functions   nlme mixed linear models with options for spatial covariates   lme4 mixed linear models with crossed random effects   ggplot, desplot standard plotting packge and extension for plotting block outlines   SpATS spline-fitting   sp preparation of spatial objects   spdep Moran\u0026rsquo;s I test   gstat for fitting empirical variogram   spaMM fits Matern covariances structure for mixed linear models   emmeans extracts marginal means from linear model objects    ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"15841bfb95305ff58eb70b526b39d8a2","permalink":"/workshops/spatial-workshop/prep-work/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/prep-work/","section":"workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()\r If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()\r If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation\r\u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting\r\u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling\r\u0026quot;SpATS\u0026quot;, # for fitting splines\r\u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling\rinstall.packages(package_list)\rsapply(package_list, require, character.only = TRUE)\r Please note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please install these in advance of the workshop. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.\nDownload files for workshop The following files are used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"9a50ee534254c7b08141160344374595","permalink":"/draft-workshops/spatial_workshop_new/prep-work/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/prep-work/","section":"draft-workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Agricultural field trials \r\rUniversity Research Farm\r The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.\n\r Field variation Agricultural field trials often employ popular experimental designs such as randomized complete block design to account for environmental heterogeneity. However, those techniques are quite often inadequate to fully account for spatial heterogeneity that arises due to field position, soil conditions, disease, wildlife impacts and more.\nHere is the a map of a wheat variety trial conducted in Idaho with a chloropleth map indicating plot yield. Each square represents a plot.\n\r Blocking in a field trial Block is not always sufficient to account for spatial variation. Here is the same Idaho variety trial with block information overlaid:\n\r The block arrangement is clearly not aligning with the field variation.\nWhen Spatial Variation is not Fully Accounted For  The treatment rankings can be wrong. Here is a plot of the cultivar ranks for yield from the Idaho variety trial when analysed with a standard linear mixed model and the same model augmented with spatial covariates.  \r  Error terms are often correlated with each other, invalidating the downstream analysis  \r  high error/low precision/wide confidence intervals/low experimental power  Blocking versus spatial statistics \r\ranother distracted boyfriend meme!\r Researchers do not have to abandon blocking when incorporating spatial covariates into analysis of a field trial. In fact, using blocking or other experimental designs combined with spatial modelling has been shown improve the quality of the final estimates.\n","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"a31a33acbb99d2f0e16a5caf445f69ea","permalink":"/workshops/spatial-workshop/why-spatial/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/why-spatial/","section":"workshops","summary":"Agricultural field trials \r\rUniversity Research Farm\r The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.","tags":null,"title":"Why Care about Spatial Variation?","type":"book"},{"authors":null,"categories":null,"content":"\rThis workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis\r\r\rSpatial autocorrelation refers to similarity between points that are close to one another. That correlation is expected to decline with distance. Note that is different from experiment-wide gradients, such as a salinity gradient or position on a slope.\nPlotting One of the easiest ways to diagnose spatial autocorrelation is by plotting data by its spatial position and using a heat map to indicate values of a response variable.\n\r While there is always ambiguity associated with using plots for decision making, early exploration of these plots can be helpful in understanding the extent of spatial correlation.\nMoran\u0026rsquo;s I Moran\u0026rsquo;s I, sometimes called \u0026ldquo;Global Moran\u0026rsquo;s I\u0026rdquo; can be used for conducting a hypothesis test on whether there is correlation between spatial units located adjacent to one another.\n$$ I = \\frac{N}{W}\\frac{\\sum_i \\sum_j w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2} \\qquad i \\neq j$$\nWhere N is total number of spatial locations indexed by $i$ and $j$, x is the variable of interest, $w_{ij}$ are a spatial weights between each $i$ and $j$, and W is the sum of all weights.\nThe expected values of Moran\u0026rsquo;s I is $-1/(N-1)$. Values greater than the expected value indicate positive spatial correlation (areas close to each other are similar), while values less than that indicate dissimilarity as spatial distance between points decreases.\nDefining Neighbors There are several options for defining adjacent neighbors and how to weight each neighbor\u0026rsquo;s influence. The two common configurations for defining neighbors are the rook and queen configurations. These are exactly what their chess analogy suggests: \u0026ldquo;rook\u0026rdquo; defines neighbors in an row/column fashion, while \u0026ldquo;queen\u0026rdquo; defines neighbors in a row/column configuration an also neighbors located diagonally at a 45 degree angle from the row/column neighbors. Determining this can be complicated when working with irregularly-placed data (e.g. counties), but is quite unambiguous for lattice data common in planned field experiments.\n\r Setting the values for weights is a function of both how neighbors are defined and their proximity to the unit of interest. However, a very popular method is to define each neighbor as equal fractions that sum to one, e.g. in rook formation, each neighbor is weighted 0.25 (assuming an interior plot with 4 neighbors).\nEmpirical variogram This is one of the most useful methods of determining the extent of spatial variability and will be covered in the following sections.\nCode for this section R\r# load libraries\rlibrary(dplyr); library(ggplot2); library(desplot); library(spdep); library(sf); library(nlme)\r# read in data and prepare it\rNin \u0026lt;- read.csv(\u0026quot;stroup_nin_wheat.csv\u0026quot;) %\u0026gt;% mutate(col.width = col * 1.2, row.length = row * 4.3) %\u0026gt;% mutate(name = case_when(is.na(as.character(rep)) ~ NA_character_, TRUE ~ as.character(gen))) %\u0026gt;% arrange(col, row)\rNin_na \u0026lt;- filter(Nin, !is.na(rep))\r# make exploratory plot\rggplot(Nin, aes(x = row, y = col)) +\rgeom_tile(aes(fill = yield), col = \u0026quot;white\u0026quot;) +\rgeom_tileborder(aes(group = 1, grp = rep), lwd = 1.2) +\rlabs(x = \u0026quot;row\u0026quot;, y = \u0026quot;column\u0026quot;, title = \u0026quot;field plot layout\u0026quot;) + theme_classic() +\rtheme(axis.text = element_text(size = 12),\raxis.title = element_text(size = 14),\rlegend.title = element_text(size = 14),\rlegend.text = element_text(size = 12))\r## conduct moran's I test ##\r# set neighbors with convenience function for grids\rxy_rook \u0026lt;- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type=\u0026quot;rook\u0026quot;, torus = FALSE, legacy = FALSE) # run linear mixed model and extract residuals\rnin.lme \u0026lt;- lme(fixed = yield ~ gen, random = ~1|rep,\rdata = Nin, na.action = na.exclude)\rresid_lme \u0026lt;- residuals(nin.lme)\rnames(resid_lme) \u0026lt;- Nin$plot\r# two version of the Moran's I test: moran.test(resid_lme, nb2listw(xy_rook), na.action = na.exclude)\rmoran.mc(resid_lme, nb2listw(xy_rook), 999, na.action = na.exclude)\r  SAS\r# read in data\rproc format;\rinvalue has_NA\r'NA' = .;\r;\rfilename NIN url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/stroup_nin_wheat.csv\u0026quot;;\rdata alliance;\rinfile NIN firstobs=2 delimiter=',';\rinformat yield has_NA.;\rinput entry $ rep $ yield col row;\rRow = 4.3*Row;\rCol = 1.2*Col;\rif yield=. then delete;\rrun;\r# heatmap\rproc sgplot data=alliance;\rHEATMAPPARM y=Row x=Col COLORRESPONSE=yield/ colormodel=(blue yellow green); run;\r# linear mixed model\rproc mixed data=alliance;\rclass Rep Entry;\rmodel Yield = Entry / outp=residuals;\rrandom Rep;\rrun;\r# Moran's I\rproc variogram data=residuals plots(only)=moran ;\rcompute lagd=1.2 maxlag=30 novariogram autocorr(assum=nor) ;\rcoordinates xc=row yc=col;\rvar resid;\rrun;\r  ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fb80fd2ad66cac061f57b377665b0236","permalink":"/workshops/spatial-workshop/diagnosis/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/diagnosis/","section":"workshops","summary":"This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis\r\r\rSpatial autocorrelation refers to similarity between points that are close to one another.","tags":null,"title":"Diagnosing Spatial Autocorrelation","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n\r 1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples?\rLists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive?\rYes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"5c1f7f41e31a8d8e08da3d62bb15dbae","permalink":"/draft-workshops/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/python/","section":"draft-workshops","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):\n$$\\gamma(h) = \\frac{1}{2|N(h)|}\\sum_{N(h)}(x_i - x_j)^2$$\nSemivariances are binned for distance intervals. The average values for semivariance and distance interval can be fit to mathematical models designed to explain how semivariance changes over distance.\nThree important concepts of an empirical variogram are nugget, sill and range\n\r\rExample Empirical Variogram\r  range = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance  2 other concepts:\n partial sill = sill - nugget nugget effect = the nugget/sill ratio, interpreted opposite of $r^2$ (the closer it is to 1, the less the amount of spatial autocorrelation)  Correlated Error Models Many equations exist for modelling semivariance patterns. A deep knowledge of these is not required to fit an empirical variogram to a model. Here are a few popular examples.\nExponential\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0 \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r}) } \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n\r\rTheoretical Exponential Variogram\r Gaussian\n(a squared version of the exponential model)\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0, \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r})^2} \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n\r\rTheoretical Gaussian Variogram\r MatÃ©rn\n\u0026lt;/An extremely complicated mathematical model/\u0026gt;\n\r\rEmpirical MatÃ©rn Variogram\r There are many more models: Cauchy, logistic, spherical, sine, \u0026hellip;.\n\rFor more information on these models, see this workshop\u0026rsquo;s accompanying online book on this topic and additional SAS resources.\r\r\rVariogram fitting Picking the right model is done both by comparing the sum of squares of error for different models and by\nNot all variables have spatial autocorrelation\n\r Not all fitted variogram models are worthy\n\r\rVariogram gone bad\r Code for this section The following scripts build upon work done in previous section(s).\nR\r# load libraries\rlibrary(gstat); library(spaMM)\r# set up spatial object\rNin_spatial \u0026lt;- Nin_na\rcoordinates(Nin_spatial) \u0026lt;- ~ col.width + row.length # add attribte\rclass(Nin_spatial)\r# establish max distance for variogram estimation\rmax_dist = 0.6*max(dist(coordinates(Nin_spatial)))\r# calculate empirical variogram\rresid_var1 \u0026lt;- gstat::variogram(yield ~ rep + gen, cutoff = max_dist,\rwidth = max_dist/15, # 15 is the number of bins\rdata = Nin_spatial)\rplot(resid_var1) # empirical variogram\r#Note: To fit a large number of models, the function 'autofitVariogram()' from the package automap can be used (is it calling gstat::variogram)\r# starting value for the nugget\rnugget_start \u0026lt;- min(resid_var1$gamma) # initialise the model (this does not do much)\rNin_vgm_exp \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential\rNin_vgm_gau \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian\rNin_vgm_mat \u0026lt;- vgm(model = \u0026quot;Mat\u0026quot;, nugget = nugget_start) # Matern\r# actually do some fitting! Nin_variofit_exp \u0026lt;- fit.variogram(resid_var1, Nin_vgm_exp)\rNin_variofit_gau \u0026lt;- fit.variogram(resid_var1, Nin_vgm_gau)\rNin_variofit_mat \u0026lt;- fit.variogram(resid_var1, Nin_vgm_mat, fit.kappa = T)\rplot(resid_var1, Nin_variofit_exp, main = \u0026quot;Exponential model\u0026quot;)\rplot(resid_var1, Nin_variofit_gau, main = \u0026quot;Gaussian model\u0026quot;)\rplot(resid_var1, Nin_variofit_mat, main = \u0026quot;Matern model\u0026quot;) attr(Nin_variofit_exp, \u0026quot;SSErr\u0026quot;)\rattr(Nin_variofit_gau, \u0026quot;SSErr\u0026quot;)\rattr(Nin_variofit_mat, \u0026quot;SSErr\u0026quot;)\r# parameters:\rNin_variofit_gau\rnugget \u0026lt;- Nin_variofit_gau$psill[1] # measurement error (other random error)\rrange \u0026lt;- Nin_variofit_gau$range[2] # distance to establish independence between data points\rsill \u0026lt;- sum(Nin_variofit_gau$psill) # maximum semivariance\r  SAS\r# calculate semivariance and compute empirical variogram\rproc variogram data=residuals plots(only)=(semivar);\rcoordinates xc=Col yc=Row;\rcompute lagd=1.2 maxlags=30;\rvar resid;\rrun;\r# fit models to the empirical variogram\rproc variogram data=residuals plots(only)=(fitplot);\rcoordinates xc=Col yc=Row;\rcompute lagd=1.2 maxlags=30;\rmodel form=auto(mlist=(gau, exp, pow, sph) nest=1);\rvar resid;\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"cd8303e2c52504ec61991e37816c126c","permalink":"/workshops/spatial-workshop/variograms/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/variograms/","section":"workshops","summary":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):","tags":null,"title":"Empirical Variograms","type":"book"},{"authors":null,"categories":null,"content":"\r 1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful?\rLorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart\rimport plotly.express as px\rdata_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;)\rfig = px.bar(data_canada, x='year', y='pop')\rfig.show()\r ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"dd726e22044eb744e57cecdc3ff1c5b8","permalink":"/draft-workshops/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/visualization/","section":"draft-workshops","summary":"","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model. In RCBD design, often the treatments are treated as fixed and the block effect as random.\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ block\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)$\nHere is an expanded version of the last term:\n$$ \\epsilon_{ij} ~\\sim N \\Bigg( 0, \\left[ { \\begin{array}{ccc} \\sigma \u0026amp; \\cdots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n0 \u0026amp; \\cdots \u0026amp; \\sigma \\end{array} } \\right] \\Bigg) $$\nThis is a mathematically representation of iid, independent and identically distributed, an assumption of linear models. When there is spatial autocorrelation, observations closer to one another are correlated, so the off-diagonals in the variance-covariance matrix are not zero.\nSpatial models seek to mathematically model this covariance so it is properly accounted for during hypothesis testing and prediction.\nCode for this section The following scripts build upon work done in previous section(s).\nR\rlibrary(emmeans); library()\r# (nlme and gstat should already be loaded)\rlibrary(spaMM) # for running `corMatern()`\r# standard linear model\rnin_lme \u0026lt;- lme(yield ~ gen, random = ~1|rep,\rdata = Nin,\rna.action = na.exclude)\r# extract the esimated marginal means for variety\rpreds_lme \u0026lt;- as.data.frame(emmeans(nin_lme, \u0026quot;gen\u0026quot;))\r# use information from the variogram fitting for intialising the parameters\rnugget \u0026lt;- Nin_variofit_gau$psill[1] range \u0026lt;- Nin_variofit_gau$range[2] sill \u0026lt;- sum(Nin_variofit_gau$psill) nugget.effect \u0026lt;- nugget/sill\r# initalise the covariance structure (from the nlme package)\rcor.gaus \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row.length + col.width, nugget = T, fixed = F,\rtype = \u0026quot;gaussian\u0026quot;, metric = \u0026quot;euclidean\u0026quot;)\r# update the rcbd model\rnin_gaus \u0026lt;- update(nin_lme, corr = cor.gaus)\r# extract predictions for 'gen'\rpreds_gaus \u0026lt;- as.data.frame(emmeans(nin_gaus, \u0026quot;gen\u0026quot;)\r# a similar procedure can be follow for other models\r# but we are going to take a shortcut and not specify the parameters\r# exponential\rcor.exp \u0026lt;- corSpatial(form = ~ row.length + col.width, nugget = T, fixed = F)\rnin_exp \u0026lt;- update(nin_lme, corr = cor.exp)\rpreds_exp \u0026lt;- as.data.frame(emmeans(nin_exp, \u0026quot;gen\u0026quot;))\r# Matern structure\rcor.mat \u0026lt;- corMatern(form = ~ row.length + col.width, nugget = T, fixed = F)\rnin_matern \u0026lt;- update(nin_lme, corr = cor.mat)\rpreds_mat \u0026lt;- as.data.frame(emmeans(nin_matern, \u0026quot;gen\u0026quot;)\r  SAS\rproc mixed data=alliance ;\rclass entry rep;\rmodel yield = entry ;\rrandom rep;\rlsmeans entry/cl;\rods output LSMeans=NIN_RCBD_means;\rtitle1 'NIN data: RCBD';\rrun;\rproc mixed data=alliance maxiter=150;\rclass entry;\rmodel yield = entry /ddfm=kr;\rrepeated/subject=intercept type=sp(gau) (Row Col) local;\rparms (11) (22) (19);\rlsmeans entry/cl;\rods output LSMeans=NIN_Spatial_means;\rtitle1 'NIN data: Gaussian Spatial Adjustment';\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"057408dc907a581ba7ba45d742b73dbd","permalink":"/workshops/spatial-workshop/correlated-error-models/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/correlated-error-models/","section":"workshops","summary":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model.","tags":null,"title":"Linear Models with Correlated Errors","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n\r 1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n\rThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.\r\r\rQuiz What is the parameter $\\mu$?\rThe parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"b99996333fbb4c63e90c11bb44f0be2b","permalink":"/draft-workshops/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/stats/","section":"draft-workshops","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":null,"categories":null,"content":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation. However, sometimes that is accurately describe a field trial. There can be experiment-wide gradients due to position on a slope, proximity to an influential environmental factor (e.g. a road), and so on. In these instances, those gradients should be modelled as a trend.\nBlocking Blocking is one example of modelling an experiment wide-trend:\n\r The expectation is that each block will capture and model existing variation within it. This becomes difficult to justify as blocks become large.\nRows \u0026amp; Ranges Recall the RCBD model from the previous section:\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\nTrials rows and ranges can likewise be modelled directly through expansion of that model (and omitting block since it full represented by column):\n$$Y_ijk = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ row $\\gamma_k$ is the effect due to the $k^{th}$ range (or column)\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)\nCode for Trends The following scripts build upon work done in previous section(s).\nR\r# load libraries\rlibrary(lme4)\r# exploratory plots boxplot(yield ~ rep, data = Nin, xlab = \u0026quot;block\u0026quot;, col = \u0026quot;red2\u0026quot;)\rboxplot(yield ~ row, data = Nin, xlab = \u0026quot;row\u0026quot;, col = \u0026quot;dodgerblue2\u0026quot;)\rboxplot(yield ~ col, data = Nin, xlab = \u0026quot;column\u0026quot;, col = \u0026quot;gold\u0026quot;)\r## row/column model ##\r# data prep\rNin$rowF = as.factor(Nin$row)\rNin$colF = as.factor(Nin$col)\r# specify model\rnin.rc \u0026lt;- lmer(yield ~ gen + (1|colfF) + (1|rowF),\rdata = Nin, na.action = na.exclude)\r# extract random effects for row and column\rranef(nin_rc)\r# extract predictions\rnin_rc \u0026lt;- as.data.frame(emmeans(nin.rc, \u0026quot;gen\u0026quot;))\r  SAS\r# exploratory boxplots\rproc sgplot data=alliance;\rvbox yield/category=rep FILLATTRS=(color=red) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\rproc sgplot data=alliance;\rvbox yield/category=Col FILLATTRS=(color=yellow) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\rproc sgplot data=alliance;\rvbox yield/category=Row FILLATTRS=(color=blue) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\r# row/column model\rproc mixed data=alliance ;\rclass entry rep;\rmodel yield = entry row col/ddfm=kr;\rrandom rep;\rlsmeans entry/cl;\rods output LSMeans=NIN_row_col_means;\rtitle1 'NIN data: RCBD';\rrun;\r  Splines Polynomial splines are an additional method for spatial adjustment and represent a more non-parametric method that does not rely on estimation or modeling of variograms. Instead, it uses the raw data and residuals to fit a surface to the spatial data and adjust the variance covariance matrix accordingly.\nCode for Splines The following scripts build upon work done in previous section(s).\nR\rnin_spline \u0026lt;- SpATS(response = \u0026quot;yield\u0026quot;, spatial = ~ PSANOVA(col, row, nseg = c(10,20),\rdegree = 3, pord = 2), genotype = \u0026quot;gen\u0026quot;, random = ~ rep, # + rowF + colF, data = Nin, control = list(tolerance = 1e-03, monitoring = 0))\rpreds_spline \u0026lt;- predict(nin_spline, which = \u0026quot;gen\u0026quot;) %\u0026gt;% dplyr::select(gen, emmean = \u0026quot;predicted.values\u0026quot;, SE = \u0026quot;standard.errors\u0026quot;)\r  SAS\rproc glimmix data=alliance ;\rclass entry rep;\reffect sp_r = spline(row col);\rmodel yield = entry sp_r/ddfm=kr;\rrandom row col/type=rsmooth;\rlsmeans entry/cl;\rods output LSMeans=NIN_smooth_means;\rtitle1 'NIN data: RCBD';\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"b280dc26c3a20f98ee5ed0cf8a9403e6","permalink":"/workshops/spatial-workshop/trend-modelling/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/trend-modelling/","section":"workshops","summary":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation.","tags":null,"title":"Modelling Spatial Trends","type":"book"},{"authors":null,"categories":null,"content":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model. Some approaches include:\n Comparing model fitness (e.g. AIC, BIC, log likelihood). Although the methods are not nested, hence precluding a log likelihood ratio test, we can compare raw values for each fit statistic. Be careful doing this in R since linear modelling packages use different estimation procedures for maximum likelihood and REML estimation that are not comparable. Comparing post-hoc power (that is, the p-values for the treatments) Comparing standard error of the estimates (i.e. precision)  \rComparing changes in the coefficient of variation (CV, $\\sigma/\\mu$) is not recommended because in many spatial models, field variation has been re-partitioned to the error term when it was (erroneously) absorbed by the other experimental effects. As a result, the CV can increase in spatial models even when inclusion of spatial covariates results in better model fit.\r\r\rUnfortunately, there is no one method for unambiguously returning the the best estimates and true ranks of the treatments. Likewise, there is no one spatial method that works best in all situations and field trials.\nCode for this section R\rlibrary(tidyr)\r# remove some objects we don't need (and will interfere with downstream processes)\rrm(nin_variofit, nin_vgm)\rrm(nin_vgm, nin_variofit, nugget, sill, range, nugget.effect)\r# assemble objects into a list\rnlme_mods \u0026lt;- list(nin_lme, nin_exp, nin_gaus, nin_matern)\rnames(nlme_mods) \u0026lt;- c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;)\r# extract log likelihood, AIC, BIC\rdata.frame(loglik = sapply(nlme_mods, logLik), AIC = sapply(nlme_mods, AIC),\rBIC = sapply(nlme_mods, AIC, k = log(nrow(Nin_na)))) %\u0026gt;% arrange(desc(loglik))\r# (higher is better for loglik, lower is better for AIC and BIC)\r# compare post-hoc power\r# conduct ANOVA\ranovas \u0026lt;- lapply(nlme_mods[-7], function(x){ aov \u0026lt;- as.data.frame(anova(x))[2,]})\r# bind all the output together\ra \u0026lt;- bind_rows(anovas) %\u0026gt;% mutate(model = c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;, \u0026quot;row-col\u0026quot;)) %\u0026gt;% arrange(desc(`p-value`)) %\u0026gt;% select(c(model, 1:4)) rownames(a) \u0026lt;- 1:nrow(a)\ra\r## compare precision of estimates\rall.preds \u0026lt;- mget(ls(pattern = \u0026quot;^preds_*\u0026quot;))\rerrors \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;SE\u0026quot;)\rpred.names \u0026lt;- gsub(\u0026quot;preds_\u0026quot;, \u0026quot;\u0026quot;, names(errors))\rerror_df \u0026lt;- bind_cols(errors)\rcolnames(error_df) \u0026lt;- pred.names\rboxplot(error_df, ylab = \u0026quot;standard errors\u0026quot;, xlab = \u0026quot;linear model\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;)\r# compare predictions preds \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;emmean\u0026quot;)\rpreds_df \u0026lt;- bind_cols(preds)\rcolnames(preds_df) \u0026lt;- pred.names\rpreds_df$gen \u0026lt;- preds_exp$gen\r# plot changes in rank\rlev \u0026lt;- c(\u0026quot;lme\u0026quot;, \u0026quot;exp\u0026quot;, \u0026quot;gaus\u0026quot;, \u0026quot;mat\u0026quot;)\rpivot_longer(preds_df, cols = !gen, names_to = \u0026quot;model\u0026quot;, values_to = \u0026quot;emmeans\u0026quot;) %\u0026gt;% mutate(model = factor(model, levels = lev)) %\u0026gt;% ggplot(aes(x = model, y = emmeans, group = gen)) +\rgeom_point(size = 5, alpha = 0.5, col = \u0026quot;navy\u0026quot;) +\rgeom_line() +\rylab(\u0026quot;yield means for gen\u0026quot;) + theme_minimal()\r  SAS\rdata NIN_RCBD_means (drop=tvalue probt alpha estimate stderr lower upper df);\rset NIN_RCBD_means;\rRCB_est = estimate;\rRCB_se = stderr;\rrun;\rdata NIN_Spatial_means (drop=tvalue probt alpha estimate stderr lower upper df);\rset NIN_Spatial_means;\rSp_est = estimate;\rSp_se = stderr;\rrun;\rproc sort data=NIN_RCBD_means;\rby entry;\rrun;\rproc sort data=NIN_Spatial_means;\rby entry;\rrun;\rdata compare;\rmerge NIN_RCBD_means NIN_Spatial_means;\rby entry;\rrun;\rproc rank data=compare out=compare descending;\rvar RCB_est Sp_est;\rranks RCB_Rank Sp_Rank;\rrun;\rproc sort data=compare;\rby Sp_rank;\rrun;\rproc print data=compare(obs=15);\rvar entry rcb_est Sp_est rcb_se sp_se rcb_rank sp_rank;\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"006196c4ae04f5e62ccfd0b0e4974438","permalink":"/workshops/spatial-workshop/model-comparison/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/model-comparison/","section":"workshops","summary":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model.","tags":null,"title":"Comparing Models","type":"book"},{"authors":null,"categories":null,"content":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible. Often, the primary goal of the studies using this design is to rank or select genotypes.\nAugmented models are analyzed in a fundamentally different method than RCBD models due to the large number of unreplicated observations. To adjust for the lack of replication, only a select set of treatments, usually of known performance, are replicated in the experiments. The error estimated from these replicated treatments is used in the analysis to evaluate the remaining genotypes.\nThere are multiple way to specify an augmented model depending on what the researcher wants to know.\nModel specification #1 $$ Y_{ij} = \\tau_i + \\beta(\\tau)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\tau_i$ is the effect of each check and the average effect of all unreplicated treatments $ \\beta(\\tau)_{ij}$ is is the effect of the $j^{th}$ unreplicated treatment nested within the overall effect of unreplicated treatments  This model evaluates:\n The difference between all checks and the average of the unreplicated treatments. The difference between the unreplicated treatments.  Model specification #2 $$ Y_{ij} = \\delta_i + \\gamma(\\delta)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\delta_i$ is the average effect of all checks and the average effect of all unreplicated treatments (so there are only 2 treatment levels) $ \\gamma(\\delta)_{ij}$ is is the effect of the $j^{th}$ treatment nested within the either unreplicated treatments or the check observations  This model evaluates:\n The difference between the average of the checks and the average of the unreplicated treatments The difference between all treatments  These models are described more in depth in BurgueÃ±o et al, 2018, along with a helpful discussion on when to treat any of these effects as fixed or random\nThe data used here refer to a wheat genotype evaluation study carried out near Lind Washington. The study looked at 922 unreplicated genotypes (â€˜nameâ€™) accompanied by 9 replicated check wheat cultivars.\nCode for this section The following scripts build upon work done in previous section(s).\nR\r# (if not already loaded)\rlibrary(dplyr); library(nlme); library(ggplot2)\rlibrary(gstat); library(sp)\r# read in data\raug_data_origin \u0026lt;- read.csv(\u0026quot;data/augmented_lind.csv\u0026quot;, na.strings = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;999999\u0026quot;)) %\u0026gt;% slice(-1) %\u0026gt;% # first line not needed\rmutate(yieldkg = yieldg/1000) # to prevent overflow\r# summarise the genoytypic data by checks/not checks\rgen_sum \u0026lt;- group_by(aug_data_origin, name) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(delta = case_when(\rcounts \u0026gt; 1 ~ \u0026quot;check\u0026quot;,\rcounts == 1 ~ \u0026quot;unrep\u0026quot;))\r# need info on just the checks\rchecks \u0026lt;- gen_sum %\u0026gt;% filter(delta == \u0026quot;check\u0026quot;) # more summarise steps for different augmented modes\rgen_sum2 \u0026lt;- gen_sum %\u0026gt;% mutate(gamma = name) %\u0026gt;% mutate(tau = case_when(\rdelta == \u0026quot;check\u0026quot; ~ gamma,\rdelta == \u0026quot;unrep\u0026quot; ~ \u0026quot;unreplicate_obs\u0026quot;)) %\u0026gt;% mutate(beta = case_when(\rdelta == \u0026quot;unrep\u0026quot; ~ gamma,\rdelta == \u0026quot;check\u0026quot; ~ gamma))\r# merge original data set with info on treatment levels\raug_data \u0026lt;- aug_data_origin %\u0026gt;% select(name, prow, pcol, yieldkg, yieldg) %\u0026gt;%\rmutate(row = prow*11.7, col = pcol*5.5) %\u0026gt;% full_join(gen_sum2, by = \u0026quot;name\u0026quot;) ## modelling\raug1 \u0026lt;- lme(fixed = yieldg ~ tau,\rrandom = ~ 1|tau/beta,\rdata = aug_data, na.action = na.exclude)\r# extract residuals\raug_data$res \u0026lt;- residuals(aug1)\r# plot residual chloroepleth map:\rggplot(aug_data, aes(y = row, x = col)) +\rgeom_tile(aes(fill = res)) +\rscale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;black\u0026quot;) +\rscale_x_continuous(breaks = seq(1,max(aug_data$row), 1)) +\rscale_y_continuous(breaks = 1:max(aug_data$col)) +\rcoord_equal() +\rtheme_void() # add spatial covariates\raug_spatial \u0026lt;- aug_data %\u0026gt;% filter(!is.na(res))\rcoordinates(aug_spatial) \u0026lt;- ~ col + row\rmax_dist = 0.5*max(dist(coordinates(aug_spatial)))\raug_vario \u0026lt;- gstat::variogram(res ~ 1, cutoff = max_dist,\rwidth = max_dist/10, data = aug_spatial)\r# optional to run: nugget_start \u0026lt;- min(aug_vario$gamma)\raug_vgm \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start)\raug_variofit \u0026lt;- fit.variogram(aug_vario, aug_vgm)\rplot(aug_vario, aug_variofit, main = \u0026quot;Exponential model\u0026quot;)\rcor_exp \u0026lt;- corSpatial(form = ~ row + col, nugget = T, fixed = F,\rtype = \u0026quot;exponential\u0026quot;)\raug1_sp \u0026lt;- update(aug1, corr = cor_exp)\r# spatial parameters:\raug1_sp$modelStruct$corStruct\r# extract BLUPs for unreplicated lines:\raug1_blups \u0026lt;- ranef(aug1_sp)$beta %\u0026gt;% rename(yieldg = '(Intercept)')\r# look at variance components\rVarCorr(aug1_sp)\r##### OR #######\r# another formulation\r# delta estimates effects of replicated versus unreplicated genotypes\r# gamma estimates the effecs of all genotypes evaluated in the trial\raug2 \u0026lt;- lme(fixed = yieldkg ~ delta,\rrandom = ~ 1|delta/gamma,\rdata = aug_data, na.action = na.exclude)\raug2_sp \u0026lt;- update(aug2, corr = cor_exp)\r# spatial parameters:\raug2_sp$modelStruct$corStruct\r# extract BLUPs for unreplicated lines:\raug_blups2 \u0026lt;- ranef(aug2_sp)$gamma %\u0026gt;% rename(yieldg = '(Intercept)')\r# look at variance components\rVarCorr(aug1_sp)\r  SAS\rfilename AUG url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/AB19F5_LIND.csv\u0026quot;;\rPROC IMPORT OUT= WORK.augmented\rDATAFILE= AUG\rDBMS=CSV REPLACE;\rGETNAMES=YES;\rDATAROW=2; RUN;\rdata augmented;\rset augmented;\rif yieldg = 999999 or yieldg=. then delete; /* Remove missing values */\rprow=prow*11.7; /*convert row and column indices to feet */\rpcol=pcol*5.5;\rrun;\rproc freq noprint data=augmented;\rtables name/out=controls;\rrun;\rdata controls;\rset controls;\rif count \u0026gt;1;\rrun;\rproc sort data=controls;\rby name;\rrun;\rproc sort data=augmented;\rby name;\rrun;\rdata augmented;\rmerge augmented controls;\rby name;\rif count=. then d2=2; /* Unreplicated */\relse d2=1; /* Replicated */\ryieldkg=yieldg/1000;\rrun;\rPROC mixed data=augmented;\rclass name d2;\rmodel yieldkg = d2/noint outp=residuals ddf=229 229;\rlsmeans d2;\r*lsmeans name(d2)/slice = d2;\rrun;\rproc sgplot data=residuals;\rHEATMAPPARM y=pRow x=pCol COLORRESPONSE=resid/ colormodel=(cx014458 cx1E8C6E cxE1FE01); title1 'Field Map';\rrun;\rproc variogram data=residuals plots(only)=(fitplot);\rwhere yieldkg ^= .;\rcoordinates xc=pcol yc=pRow;\rcompute lagd=6.6 maxlags=25;\rmodel form=auto(mlist=(gau, exp, pow, sph) nest=1);\rvar resid;\rrun;\rPROC mixed data=augmented;\rclass name d2;\rmodel yieldkg = d2 name(d2)/outp=adjresiduals ddf=229 229;\rlsmeans d2;\rrepeated/subject=intercept type=sp(pow)(prow pcol) local;\rods output SolutionR =parms;\rparms (0.074) (0.0051)(0.475) ;\r*lsmeans name(d2)/slice = d2; # alot of output!!\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"561372141df0a39b09e92d9d036e8b87","permalink":"/workshops/spatial-workshop/augmented/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/augmented/","section":"workshops","summary":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible.","tags":null,"title":"Augmented Designs","type":"book"},{"authors":null,"categories":null,"content":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time. However, investigating spatial correlation in a field trial and controlling for it if necessary using any of the methods developed for this is recommended over doing nothing.\nThere is no denying that work is needed to develop scripts that automate this process so researchers can routinely incorporate spatial covariance into field trial analysis. Many current R tools are unwieldy to use and have insufficient options to support variety trial analysis.\nUntil this situation is improved, it is probably wisest to focus on using spatial models that are well-supported at this time. Any of the options implemented in the nlme package (or that work with that package) are decent choices with excellent support for extracting least-squares means, running ANOVA, and standard model diagnostics. Furthermore, nlme supports generalized linear models. INLA is established is supported by a large and growing user base, and breedR is likewise well established.\nOther resources   Incorporating Spatial Analysis into Agricultural Field Experiments, a more comprehensive version of this tutorial\n  CRAN task view on analysis of spatial data\n  Other R packages\n     package usage     breedR mixed modelling with AR1xAR1 estimation   inla Bayesian modelling with options for spatial covariance structure   Mcspatial nonparametric spatial analysis, (no longer on CRAN)   ngspatial spatial models with a focus on generalized linear models   sommer mixed models, including an AR1xAr1 model   spamm MatÃ©rn covariance structure   spANOVA spatial lag models for field trials   spatialreg spatial functions for areal data    The package sommer implements a version of the AR1xAR1 covariance structure. However, it does not estimate the parameter $\\rho$. The user must specify the $\\rho$ and that value is not optimized in the restricted maximum likelihood estimation. Both BreedR and inla implement an AR1xAR1 covariance structure. Additional, SAS and the proprietary software asreml can implement a mixed model with this covariance structure.\nBooks for the deep dive \r   Statistics for Spatial Data\n  Applied Spatial Data Analysis with R, available for free\n  Spatio-Temporal Statistics With R (also free)\n  Spatial Data Analysis in Ecology and Agriculture Using R\n  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"c08d0a8717c5c9f76fe45f925c14f37a","permalink":"/workshops/spatial-workshop/conclusion/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/conclusion/","section":"workshops","summary":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time.","tags":null,"title":"Final thoughts","type":"book"},{"authors":["Statistical Programs"],"categories":null,"content":"Here is the slide set for the workshop. A more extensive resource is also available [here](Spatial Recipes for Field Trials).\n","date":1636275600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636275600,"objectID":"2f9bb2ee0e0bf534d0410e8050c5d500","permalink":"/draft-workshops/spatial_workshop_new/slide-set/","publishdate":"2021-11-07T09:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/slide-set/","section":"draft-workshops","summary":"A one-day introductory workshop on how to integrate spatial covariates into analysis of field trials laid out in a lattice pattern using SAS and R.","tags":["spatial statistics","field experiments"],"title":"Slide set of spatial recipes","type":"draft-workshops"},{"authors":null,"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"6871f3ea19b37a37cb6962d10767b6a6","permalink":"/project/spatial-stats-field-trials/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/spatial-stats-field-trials/","section":"project","summary":"Our extended resource on including spatial models in gridded field trials","tags":["Demo"],"title":"Incorporating Spatial Covariates into Planned Field Experienments","type":"project"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Install R: You can download R here. Get the correct R distribution for your operating system. Once downloaded, click on downloaded file, and follow the installation instructions.\nNote that R is updated several times per year. If your installation is a year old or more, consider updating your version of R to the latest version.\nInstall RStudio Rstudio is not R, rather, it is a user interface for accessing R. It is a complicated interface with many features for developers. Despite its complexity, RStudio is nevertheless a very helpful R user interface for users of all abilities. It can downloaded here. For most users, the free version of \u0026ldquo;RStudio Desktop\u0026rdquo; should be chosen. Once downloaded, click on downloaded file, and follow the installation instructions.\nInstall Rtools (optional) Only Windows users need to consider this step. This app is for compiling R packages with C, C++ and Fortran code. It is a separate piece of software that has to be downloaded and installed (it is not an R package). Rtools is not needed by all users and if you don\u0026rsquo;t know if you need this, it is absolutely fine to skip this step. If you do think you need this, You can find it here. Download and install.\nSetting up RStudio Setup (optional) This is an optional step, but it is highly recommended. This step will prevent RStudio from saving all of your objects in a session to .Rdata file that is then automatically loaded whenever you open R.\ninstall.packages(\u0026quot;usethis\u0026quot;); library(usethis)\rusethis::use_blank_slate()\r You can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nWhy is automatic loading of an .Rdata file not recommended? Because it makes your work less reproducible. You may have created test objects that will unexpectedly interfere with downstream operations or analysis. You may have changed the original data source, but an older version is saved in the .Rdata file. More explanation is given by RStudio.\nIF you are used to opening R and seeing all of your previous objects automatically loaded into the objects pane, this will be an adjustment. The solution is to save your processes into .R scripts that capture all information from packages loaded, file import, all data manipulations and other operations important. If these steps are slow and there is a need to access intermediate objects, these can be saved in tabular formats readable by many applications (e.g. .txt or .csv) or saved as a specific R object (see saveRDS() in the R help files) and reloaded in another session.\nSet up version control (optional) If you use Git or SVN, you can perform Git operations directions from RStudio and interact with remote repositories. If you don\u0026rsquo;t use version control, this step can be skipped. If you do use version control, the command line or other third-party software (e.g. Gitkraken) are fine to use instead or in addition to RStudio\u0026rsquo;s interface. The implementation of git in R is very minimal and supports only a limited number of actions, so you are likely to need other software to perform complicated git actions. It is useful for file additions, commits, pushes and pulls.\nYou can set up Git by going to Tools \u0026ndash;\u0026gt; Global Options \u0026ndash;\u0026gt; Git/SVN.\nThis is not the right space to provide detailed instructions for using git as an R user, but Jenny Bryan has written a very helpful tutorial covering this subject.\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"8b70804b6afa070131995606b8772ebd","permalink":"/post/getting-r-setup/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/getting-r-setup/","section":"post","summary":"Some instructions for R installation and your R setup to support reproducible research.","tags":["R","reproducible research"],"title":"Getting R Set Up","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Make sure your Rstudio session is not saving .RData automatically: Note: this step requires the \u0026lsquo;usethis\u0026rsquo; package; please install this package if you do not already have it installed.\nStep 1 is to disable automatic saving of your objects to a .RData file. This file is automatically loaded when R restarts. Since we often create all sorts of miscellaneous objects during a session with a clear record of why, loading all objects without a clear sense of their provenance is often not reproducible by other.\nusethis::use_blank_slate()\r You can read more about this function in its documentation.\nYou can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nSave all code you run in an .R or .Rmd file This is your source code. It\u0026rsquo;s as real and as important as your input data. This file should capture a set of actions that can be repeated by another person (e.g. your PI, other colleagues yourself in the future) including packages loaded, files imported, all data manipulations and the outputs from these actions (e.g. visualisations, analytical outcomes). The idea is to capture your thought process and specific actions so this can be repeated in full. In most analyses, it is extremely likely* you will revisit a project and need to repeat what has already been done! Keeping a record of actions will save you considerable time because you will not have to attempt to recall and/or reconstruct exactly what you did in previous sessions.\n*This is almost guaranteed to happen!\nRegularly restart your R session Yes, that means wiping all the loaded packaged and objects from the session (if you followed the first recommendation in these instructions), but the upside is that your analysis are reproducible. This means future you can repeat those analyses and get the same results back you did earlier.\nYou can restart R by manually closing and opening RStudio. You can also restart the R session with RStudio by navigating to the menu item Session \u0026ndash;\u0026gt; Restart R.\nUse R projects This is optional, but it will make your life easier. Whenever you start a new analytical endeavor in R, create an R project by navigating to File \u0026ndash;\u0026gt; New Project in RStudio. There are many options available for setting the [project directory (where the .Rproj file lives), the type of project (e.g. R package, Shiny app or blank), and options to initialise a git repo. The simplest option is to choose New Project (no special type) in a dedicated directory. The main advantage of projects is that by opening an .Rproj file, the working directory is automatically set to that directory. If you are using a cloud solution for working across different computers or working with collaborators, this will make things easier because you can use relative paths for importing data and outputting files. There would be no more need for this at the top of your script:\nsetwd(\u0026quot;specific/path/to/my/computer\u0026quot;)\r Additionally, for setting up gitbooks through \u0026lsquo;bookdown\u0026rsquo;, R packages, Shiny apps, and other complicated R endeavors, the automated set-up through R projects can be immensely helpful. This is sometimes referred to as \u0026ldquo;project-oriented workflow.\u0026rdquo; In addition to using R projects with a dedicated directory for each research project, I also prefer to have a consistent directory structure for each project like this one:\ntop-level-directory\râ”‚ README.md\râ”‚\râ””â”€â”€â”€data\râ”‚ â”‚ file011.txt\râ”‚ â”‚ file012.txt\râ”‚ â”‚\râ”‚ â””â”€â”€â”€spatial_files\râ”‚ â”‚ file208.dbf\râ”‚ â”‚ file208.shp\râ”‚ â”‚ file208.shx\râ”‚ â””â”€â”€â”€scripts\râ”‚ â”‚ eda.R\râ”‚ â”‚ analysis.R\râ”‚ â”‚ plots.R\râ”‚ â”‚ final_report.Rmd\r|\râ””â”€â”€â”€outputs\râ”‚ â”‚ plot1.png\râ”‚ â”‚ blups.csv\r|\râ””â”€â”€â”€extra\râ”‚ some_paper.pdf\râ”‚ ...\r I put all raw data needed for analysis into the \u0026lsquo;data\u0026rsquo; directory, any and all programming scripts in the \u0026lsquo;scripts\u0026rsquo; directory, all outputs (plots, tables, intermediate data object) in the \u0026lsquo;outputs\u0026rsquo; directory and everything else ends up \u0026lsquo;extra\u0026rsquo;. Naturally, there are many different directory structures to use and this is just one example. Find something that works best for your needs!\nUse the \u0026lsquo;here\u0026rsquo; package. This is also optional. It works like R projects for setting the working directory. However, for an R project to work, you have to open the .Rproj file in RStudio. What if you or your collaborators prefer to open R files directly and start using those? Here will look for the next directory level which there is a .Rproj file and set the working directory there.\nIf you want to import a file, \u0026ldquo;datafile.csv\u0026rdquo; that located in the data directory. Your .R script is actually located in the \u0026lsquo;scripts\u0026rsquo; directory. Normally, if you try to read that in, you need to specify the full path to \u0026ldquo;mydata.csv\u0026rdquo; or set the working directory and use a relative path. Again, these paths will not work if you switch computers or your collaborators are running these scripts on their own systems. This system gets even more complicated when working with an .Rmd file. Here\u0026rsquo;s an alternative approach that works the same across files and systems:\nFirst, make sure you have .Rproj file to define the top-level directory.\nlibrary(here)\rmydata \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;datafile.csv\u0026quot;))\r This code will construct this path: \u0026ldquo;data/datafile.csv\u0026rdquo; and execute that command under the assumption that wherever that .rproj is located (going up one directory at a time until it finds it) is where the working directory is set. Putting library(here) into every .R or .Rmd file in a project will resolve these issues.\nUse R environments. Again: optional, but it will make your life easier.\nOften in academia, I might do an analysis, move on to something else and then have to return that analysis months or years later. I probably will have updated R and some or all of the packages used in that analysis. As a result of these updates, my original code may not work at all or may not do the intended actions. What I need are both the older version of R and the older packages. The package \u0026lsquo;renv\u0026rsquo; is a solution. It captures the versions of R and the loaded packages. It also builds a custom package library for your package (and caches this information across other projects using renv).\nStart here: (you need to also be using Rprojects since renv is searching for .Rproj file)\nlibrary(renv)\rrenv::init()\r If you have a mature project that\u0026rsquo;s not undergoing any further development at this time, this is all you need to do.\nIf you continue to develop your project and install new packages, update your R environment like thus to ensure new or updated packaged are included:\nrenv::snapshot()\r If you\u0026rsquo;re familiar with Packrat, this is a replacement for that. This is particularly helpful for things that may have a long life span, like Shiny apps. The renv package has extensive documentation worth reading.\nFinal Comments There are many more resources and recommendations for conducting reproducible research in R. There an entire CRAN task view devoted to this! three-elk-FARM-2001\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"32bd8072205d33315c2c1a506db82c8c","permalink":"/post/reproducible-r/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/reproducible-r/","section":"post","summary":"A few steps you can take to make your workflow in R more reproducible and less painful for you to deal with.","tags":["R","Reproducible Research"],"title":"Quick Tricks and Tips for Reproducible Research in R","type":"post"},{"authors":null,"categories":null,"content":"","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"91e1b7a95d5e6b1ad524b3787bac8fdc","permalink":"/project/reproducible-research/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/project/reproducible-research/","section":"project","summary":"Gosh, reproducible research is important. Let's do more of it!","tags":["Reproducible Research"],"title":"Reproducible Research","type":"project"},{"authors":["Statistical Programs"],"categories":null,"content":"","date":1617982200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617982200,"objectID":"7a023a5229b3c480b15defa8bfb626a1","permalink":"/talks/spatial_seminar_20210409/","publishdate":"2021-04-09T15:30:00Z","relpermalink":"/talks/spatial_seminar_20210409/","section":"talks","summary":"A brief introduction into how to integrate spatial covariates into ANOVA-based analysis of field trials laid out in a lattice pattern.","tags":["spatial statistics","field experiments"],"title":"Routine Incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"talks"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"=\rul {\rcolor: #282828;\rfont-size: 40px;\r}\r\r A Road in Auvers After the Rain by Vincent Van Gogh\n\r  ### Goal: Make everyone feel more comfortable using spatial stats when analyzing field experimental data. (you don\u0026rsquo;t have to be a geospatial statistics expert)\n\rWhere to Find This Information This Presentation:\nhttps://github.com/IdahoAgStats/lattice-spatial-analysis-talk\r A longer tutorial:\nhttps://idahoagstats.github.io/guide-to-field-trial-spatial-analysis\r What Are Barriers to Using Spatial Stats?  Perceived lack of need Unsure of benefits No training in the topic/intimidated by the statistical methodology Limited time to devote to statistical analysis Unclear what would happen to blocking if spatial stats are used very few resources for easy implementation  Spatial Variation in Agricultural Fields Univeristy of Idaho's Parker Farm (Moscow, Idaho)\n\rSpatial Variation in Agricultural Fields Blocking in Agricultural Fields Blocking versus Spatial Analysis This is not how this works. Blocking is compatible with spatial analysis and recommended for most (all?) field trials.\nThere Are Many Spatial Methods Available    areal data correlated error models     row and column trend exponential   nearest neighbor spherical   separable ARxAR models Gaussian   spatial error model Matern   spatial lag model Cauchy   ARIMA power   splines linear   GAMs many more\u0026hellip;    These Methods Work These Methods Can Be Complex  \u0026hellip;.But\nYou can also integrate spatial methods into gridded field trials without:\n having to know anything about map projections, shapefiles or other geospatial terminology possessing a deep understanding of linear modeling techniques or empirical variograms being an R or SAS programming expert  Knowing these things is helpful, but not essential.\nA Typical Experiment  Experimental treatments fully crossed effects Blocking scheme along the expected direction of field variation  Analysis A typical linear model: $Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$\nResponse = trial mean + treatment effect + block effect + leftover error\nWe Assume:  The error terms, or residuals, are independent of another with a shared distribution:  $$\\epsilon_i \\sim N(0,\\sigma_e)$$\nEach block captures variation unique to that block and there is no other variation related to spatial position of the experimental plots.   **How often is #2 evaluated?** \rExample Analysis Average Yield by Row, Column and Block Standard Analysis of Kimberly, 2013 Wheat Variety Trial  36 soft white winter wheat cultivars 4 blocks 2 missing data points the linear model:  $Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$\nlibrary(nlme)\rlm1 \u0026lt;- lme(yield ~ cultivar, random = ~ 1|block, data = mydata, na.action = na.exclude)\r What Do The Residuals Look Like? plot(lm1)\r What Do The Residuals Look Like Spatially? What Do The Residuals Look Like Spatially? Global Moran\u0026rsquo;s Test for Spatial Autocorrelation $H_0$: There is no spatial autocorrelation $H_a:$ There is spatial autocorrelation!\nThis uses a simple weighting matrix that weights all neighbors that share a plot border (the chess-based \u0026ldquo;rook\u0026rdquo; formation) equally.\n## ## Monte-Carlo simulation of Moran I\r## ## data: mydata$residuals ## weights: weights ## omitted: 88, 97 ## number of simulations + 1: 1000 ## ## statistic = 0.15869, observed rank = 997, p-value = 0.003\r## alternative hypothesis: greater\r Handling Spatial Autocorrelation in Areal Data Areal data = finite region divided into discrete sub-regions (plots) with aggregated outcomes\nOptions:\n model row and column trends  good for known gradients (hill slope, salinity patterns)   assume plots close together are more similar than plots far apart. The errors terms can be modelled based on proximity, but there is no trial-wide trend  autoregressive models (AR) models utilizing \u0026ldquo;gaussian random fields\u0026rdquo; for continuously varying data (e.g. point data) Smoothing splines nearest neighbor    Basic Linear Model $$Y_{ij} = \\mu + A_i + \\epsilon_{ij}$$ $$\\epsilon_i \\sim N(0,\\sigma)$$\nIf N = 4:\n$$e_i ~\\sim N \\Bigg( 0, \\left[ {\\begin{array}{ccc} \\sigma^2 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\ 0 \u0026amp; \\sigma^2 \u0026amp; 0 \u0026amp; 0\\ 0 \u0026amp; 0 \u0026amp; \\sigma^2 \u0026amp; 0\\\n0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\sigma^2 \\end{array} } \\right] \\Bigg) $$\nThe variance-covariance matrix indicates a shared variance and all off-diagonals are zero, that is, the errors are uncorrelated.\nLinear Model with Autoregressive (AR) Errors Same linear model: $$Y_{ij} = \\mu + A_i + \\epsilon_{ij}$$\nDifferent variance structure:\n$$e_i ~\\sim N \\Bigg( 0, = \\sigma^2 \\left[ {\\begin{array}{cc} 1 \u0026amp; \\rho \u0026amp; \\rho^2 \u0026amp; \\rho^3 \\\n\\rho \u0026amp; 1 \u0026amp; \\rho \u0026amp; \\rho^2 \\\n\\rho^2 \u0026amp; \\rho \u0026amp; 1 \u0026amp; \\rho \\\n\\rho^3 \u0026amp; \\rho^2 \u0026amp; \\rho \u0026amp; 1 \\ \\end{array} } \\right] \\Bigg) $$\n $\\rho$ is a correlation parameter ranging from -1 to 1 where 0 is no correlation and values approaching 1 indicate spatial correlation. The \u0026ldquo;one\u0026rdquo; in AR1 means that only the next most adjacent point is considered. There can be AR2, AR3, \u0026hellip;, ARn models.  The Separable AR1 x AR1 model   AR1xAR1 assumes correlation in two directions, row and column. It estimates $\\sigma$, $\\rho_{column}$, and $\\rho_{row}$ often a good choice since plot are rectangular and hence autocorrelation will differ by direction (\u0026ldquo;anistropy\u0026rdquo;)  More Notes on Separable AR1xAR1  From a statistical standpoint, it\u0026rsquo;s one of the more intuitive models The implementation in R is a little shaky  several packages, all hard to use and incompatible with other R packages   It is implemented in SAS Some proprietary software implements this (AsREML), others do not (Agrobase)  Semivariance and Empirical Variograms A measure of spatial correlation based on all pairwise correlations in a data set, binned by distance apart:\n$\\gamma^2(h) = \\frac{1}{2} Var[Z(s+h)-Z(s)]$\n$Z(s)$ = observed data at point $s$.\n$Z(s)$ = observed data at another point $h$ distance from point $s$.\nFor a data set with $N$ observation, there are this many pairwise points:\n$\\frac{N(N-1)}{2}$\nEmpirical Variogram This uses semivariance to mathematically relate spatial correlations with distance\nrange = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance\nSemivariance \u0026amp; Empirical Variograms  There are many difference mathematical models for explaining semivariance:  exponential, Gaussian, MatÃ©rn, spherical, \u0026hellip;   It is usually used for kriging, or prediction of a new point through spatial interpolation It can also be used in a linear model where local observations are used to predict a data point in addition to treatment effects Bonus: R and SAS are really good at this!  Adding Semivariance to a Linear Model Copy data into new object so we can assign it a new class (and remove missing data):\nlibrary(gstat); library(sp); library(dplyr)\rmydata_sp \u0026lt;- mydata %\u0026gt;% filter(!is.na(yield))\r Establish coordinates for data set to make it an sp object (\u0026ldquo;spatial points\u0026rdquo;):\ncoordinates(mydata_sp) \u0026lt;- ~ row + range\r Set the maximum distance for looking at pairwise correlations:\nmax_dist \u0026lt;- 0.5*max(dist(coordinates(mydata_sp)))\r Adding Semivariance to a Linear Model Calculate a sample variogram:\nsemivar \u0026lt;- variogram(yield ~ block + cultivar, data = mydata_sp,\rcutoff = max_dist, width = max_dist/12)\rnugget_start \u0026lt;- min(semivar$gamma)\r Adding Semivariance to a Linear Model The empirical variogram:\nplot(semivar)\r Adding Semivariance to a Linear Model Set up models for fitting variograms:\nvgm1 \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential\rvgm2 \u0026lt;- vgm(model = \u0026quot;Sph\u0026quot;, nugget = nugget_start) # spherical\rvgm3 \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian\r Fit the variogram models to the data:\nvariofit1 \u0026lt;- fit.variogram(semivar, vgm1)\rvariofit2 \u0026lt;- fit.variogram(semivar, vgm2)\rvariofit3 \u0026lt;- fit.variogram(semivar, vgm3)\r Adding Semivariance to a Linear Model Look at the error terms to see which model is the best at minimizing error.\n## [1] \u0026quot;exponential: 26857.3\u0026quot;\r ## [1] \u0026quot;spherical: 26058.3\u0026quot;\r ## [1] \u0026quot;Gaussian: 41861.0\u0026quot;\r The spherical model is the best at minimizing error.\nAdding Semivariance to a Linear Model plot(semivar, variofit2, main = \u0026quot;Spherical model\u0026quot;)\r Adding Semivariance to a Linear Model Extract the nugget and sill information from the spherical variogram:\nnugget \u0026lt;- variofit2$psill[1] range \u0026lt;- variofit2$range[2] sill \u0026lt;- sum(variofit2$psill) nugget.effect \u0026lt;- nugget/sill # the nugget/sill ratio\r Adding Semivariance to a Linear Model Build a correlation structure in nlme:\ncor.sph \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row + range, nugget = T, fixed = F,\rtype = \u0026quot;spherical\u0026quot;, metric = \u0026quot;euclidean\u0026quot;)\r Update the Model:\nlm_sph \u0026lt;- update(lm1, corr = cor.sph)\r Compare Models - Log likelihood logLik(lm1)\r ## 'log Lik.' -489.0572 (df=38)\r logLik(lm_sph)\r ## 'log Lik.' -445.4782 (df=40)\r Compare Models - Post-hoc Power anova(lm1)[2,]\r ## numDF denDF F-value p-value\r## cultivar 35 103 1.6411 0.029\r anova(lm_sph)[2,]\r ## numDF denDF F-value p-value\r## cultivar 35 103 2.054749 0.0028\r Compare Model Predictions library(emmeans)\rlme_preds \u0026lt;- as.data.frame(emmeans(lm1, \u0026quot;cultivar\u0026quot;)) %\u0026gt;% mutate(model = \u0026quot;mixed model\u0026quot;)\rsph_preds \u0026lt;- as.data.frame(emmeans(lm_sph, \u0026quot;cultivar\u0026quot;)) %\u0026gt;% mutate(model = \u0026quot;mixed model + spatial\u0026quot;)\rpreds \u0026lt;- rbind(lme_preds, sph_preds)\r Compare Model Predictions Highest yielding wheat: \u0026lsquo;Stephens\u0026rsquo; (released in 1977)\nWhere Was Stephens Located in the Trial? More Notes  When models omit blocking, the predictions may be unchanged or they may worsen. This varies by the agronomic field, but in general, blocking a field trial and including block in the statistical model improves your experimental power and controls experimental error. There is no single spatial model that fits all However, using any spatial model is usually better than none at all When you use spatial covariates, your estimates are better and more precise. This really does help you!  What\u0026rsquo;s Next:  Track row and range information in your trial data set. Look at the tutorial! (we will also add SAS code) Try out a few models and see how it impacts your results.  The Seminar Was Brought to you by\u0026hellip;Statistical Programs!!! Statistical consulting to support the College of Agriculture and Life Sciences.\nBill Price, Director, bprice@uidaho.edu, AgSci307\nJulia Piaskowski, jpiaskowski@uidaho.edu, AgSci 305\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"207697934e1ff33ee3b594c2b0f9405a","permalink":"/slides/spatial_seminar_20210409/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial_seminar_20210409/","section":"slides","summary":"=\rul {\rcolor: #282828;\rfont-size: 40px;\r}\r\r A Road in Auvers After the Rain by Vincent Van Gogh\n\r  ### Goal: Make everyone feel more comfortable using spatial stats when analyzing field experimental data.","tags":null,"title":"Routine incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"slides"}]