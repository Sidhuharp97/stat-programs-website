[{"authors":null,"categories":null,"content":"Julia Piaskowski is a consulting statistician at the University of Idaho.\n","date":1654041600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1654041600,"objectID":"ea76c2c583835370cabcc577a3ff91a8","permalink":"/authors/jpiaskowski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jpiaskowski/","section":"authors","summary":"Julia Piaskowski is a consulting statistician at the University of Idaho.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":"","date":1618444800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1618444800,"objectID":"7cc81c076516116756d3f5a4a4fb9202","permalink":"/authors/bprice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bprice/","section":"authors","summary":"","tags":null,"title":"William Price","type":"authors"},{"authors":null,"categories":null,"content":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\n","date":1617982200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617982200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.","tags":null,"title":"Statistical Programs","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb2df55dd082c12a119ff230831261b","permalink":"/authors/xdai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xdai/","section":"authors","summary":"","tags":null,"title":"Xin Dai","type":"authors"},{"authors":null,"categories":null,"content":"   Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data. The workshop draws heavily from our book on this subject.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n check your system   SAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SAS® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nData sets\nThe following files will be used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n begin the workshop   This workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"42964fbe9a75f645e695045d6732fb2b","permalink":"/events/workshops/spatial-workshop/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/","section":"events","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials","type":"book"},{"authors":null,"categories":null,"content":"  Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()  If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()  If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation \u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting \u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling \u0026quot;SpATS\u0026quot;, # for fitting splines \u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;spaMM\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling install.packages(package_list) sapply(package_list, require, character.only = TRUE)   Please note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please attempt installation in advance of the workshop. The packages have all been successfully installed if after the sapply(package_list, require, character.only = TRUE) is run, the R output is \u0026ldquo;TRUE\u0026rdquo; for each package. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.   Library Information    package usage     dplyr, tidyr, standard data manipulation   purrr for repeat functions   nlme mixed linear models with options for spatial covariates   lme4 mixed linear models with crossed random effects   ggplot, desplot standard plotting packge and extension for plotting block outlines   SpATS spline-fitting   sp preparation of spatial objects   spdep Moran\u0026rsquo;s I test   gstat for fitting empirical variogram   spaMM fits Matern covariances structure for mixed linear models   emmeans extracts marginal means from linear model objects    ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"bf2061f5cff3726c45083203a42ce02c","permalink":"/events/workshops/spatial-workshop/prep-work/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/prep-work/","section":"events","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Agricultural field trials   University Research Farm  The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.\n  Field variation Agricultural field trials often employ popular experimental designs such as randomized complete block design to account for environmental heterogeneity. However, those techniques are quite often inadequate to fully account for spatial heterogeneity that arises due to field position, soil conditions, disease, wildlife impacts and more.\nHere is the a map of a wheat variety trial conducted in Idaho with a chloropleth map indicating plot yield. Each square represents a plot.\n  Blocking in a field trial Block is not always sufficient to account for spatial variation. Here is the same Idaho variety trial with block information overlaid:\n  The block arrangement is clearly not aligning with the field variation.\nWhen Spatial Variation is not Fully Accounted For  The treatment rankings can be wrong. Here is a plot of the cultivar ranks for yield from the Idaho variety trial when analysed with a standard linear mixed model and the same model augmented with spatial covariates.     Error terms are often correlated with each other, invalidating the downstream analysis     high error/low precision/wide confidence intervals/low experimental power  Blocking versus spatial statistics   another distracted boyfriend meme!  Researchers do not have to abandon blocking when incorporating spatial covariates into analysis of a field trial. In fact, using blocking or other experimental designs combined with spatial modelling has been shown improve the quality of the final estimates.\n","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"293cc0dc0aabfb925693ff94f983848e","permalink":"/events/workshops/spatial-workshop/why-spatial/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/why-spatial/","section":"events","summary":"Agricultural field trials   University Research Farm  The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.","tags":null,"title":"Why Care about Spatial Variation?","type":"book"},{"authors":null,"categories":null,"content":" This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis   Spatial autocorrelation refers to similarity between points that are close to one another. That correlation is expected to decline with distance. Note that is different from experiment-wide gradients, such as a salinity gradient or position on a slope.\nPlotting One of the easiest ways to diagnose spatial autocorrelation is by plotting data by its spatial position and using a heat map to indicate values of a response variable.\n  While there is always ambiguity associated with using plots for decision making, early exploration of these plots can be helpful in understanding the extent of spatial correlation.\nMoran\u0026rsquo;s I Moran\u0026rsquo;s I, sometimes called \u0026ldquo;Global Moran\u0026rsquo;s I\u0026rdquo; can be used for conducting a hypothesis test on whether there is correlation between spatial units located adjacent to one another.\n$$ I = \\frac{N}{W}\\frac{\\sum_i \\sum_j w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2} \\qquad i \\neq j$$\nWhere N is total number of spatial locations indexed by $i$ and $j$, x is the variable of interest, $w_{ij}$ are a spatial weights between each $i$ and $j$, and W is the sum of all weights.\nThe expected values of Moran\u0026rsquo;s I is $-1/(N-1)$. Values greater than the expected value indicate positive spatial correlation (areas close to each other are similar), while values less than that indicate dissimilarity as spatial distance between points decreases.\nDefining Neighbors There are several options for defining adjacent neighbors and how to weight each neighbor\u0026rsquo;s influence. The two common configurations for defining neighbors are the rook and queen configurations. These are exactly what their chess analogy suggests: \u0026ldquo;rook\u0026rdquo; defines neighbors in an row/column fashion, while \u0026ldquo;queen\u0026rdquo; defines neighbors in a row/column configuration an also neighbors located diagonally at a 45 degree angle from the row/column neighbors. Determining this can be complicated when working with irregularly-placed data (e.g. counties), but is quite unambiguous for lattice data common in planned field experiments.\n  Setting the values for weights is a function of both how neighbors are defined and their proximity to the unit of interest. However, a very popular method is to define each neighbor as equal fractions that sum to one, e.g. in rook formation, each neighbor is weighted 0.25 (assuming an interior plot with 4 neighbors).\nEmpirical variogram This is one of the most useful methods of determining the extent of spatial variability and will be covered in the following sections.\nCode for this section R # load libraries library(dplyr); library(ggplot2); library(desplot); library(spdep); library(sf); library(nlme) # read in data and prepare it Nin \u0026lt;- read.csv(\u0026quot;stroup_nin_wheat.csv\u0026quot;) %\u0026gt;% mutate(col.width = col * 1.2, row.length = row * 4.3) %\u0026gt;% mutate(name = case_when(is.na(as.character(rep)) ~ NA_character_, TRUE ~ as.character(gen))) %\u0026gt;% arrange(col, row) Nin_na \u0026lt;- filter(Nin, !is.na(rep)) # make exploratory plot ggplot(Nin, aes(x = row, y = col)) + geom_tile(aes(fill = yield), col = \u0026quot;white\u0026quot;) + geom_tileborder(aes(group = 1, grp = rep), lwd = 1.2) + labs(x = \u0026quot;row\u0026quot;, y = \u0026quot;column\u0026quot;, title = \u0026quot;field plot layout\u0026quot;) + theme_classic() + theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14), legend.title = element_text(size = 14), legend.text = element_text(size = 12)) ## conduct moran's I test ## # set neighbors with convenience function for grids xy_rook \u0026lt;- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type=\u0026quot;rook\u0026quot;, torus = FALSE, legacy = FALSE) # run linear mixed model and extract residuals nin.lme \u0026lt;- lme(fixed = yield ~ gen, random = ~1|rep, data = Nin, na.action = na.exclude) resid_lme \u0026lt;- residuals(nin.lme) names(resid_lme) \u0026lt;- Nin$plot # two version of the Moran's I test: moran.test(resid_lme, nb2listw(xy_rook), na.action = na.exclude) moran.mc(resid_lme, nb2listw(xy_rook), 999, na.action = na.exclude)   SAS # read in data proc format; invalue has_NA 'NA' = .; ; filename NIN url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/stroup_nin_wheat.csv\u0026quot;; data alliance; infile NIN firstobs=2 delimiter=','; informat yield has_NA.; input entry $ rep $ yield col row; Row = 4.3*Row; Col = 1.2*Col; if yield=. then delete; run; # heatmap proc sgplot data=alliance; HEATMAPPARM y=Row x=Col COLORRESPONSE=yield/ colormodel=(blue yellow green); run; # linear mixed model proc mixed data=alliance; class Rep Entry; model Yield = Entry / outp=residuals; random Rep; run; # Moran's I proc variogram data=residuals plots(only)=moran ; compute lagd=1.2 maxlag=30 novariogram autocorr(assum=nor) ; coordinates xc=row yc=col; var resid; run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"bbf124f1c429d14c54ba274fe608c4a2","permalink":"/events/workshops/spatial-workshop/diagnosis/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/diagnosis/","section":"events","summary":"This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis   Spatial autocorrelation refers to similarity between points that are close to one another.","tags":null,"title":"Diagnosing Spatial Autocorrelation","type":"book"},{"authors":null,"categories":null,"content":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):\n$$\\gamma(h) = \\frac{1}{2|N(h)|}\\sum_{N(h)}(x_i - x_j)^2$$\nSemivariances are binned for distance intervals. The average values for semivariance and distance interval can be fit to mathematical models designed to explain how semivariance changes over distance.\nThree important concepts of an empirical variogram are nugget, sill and range\n  Example Empirical Variogram   range = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance  2 other concepts:\n partial sill = sill - nugget nugget effect = the nugget/sill ratio, interpreted opposite of $r^2$ (the closer it is to 1, the less the amount of spatial autocorrelation)  Correlated Error Models Many equations exist for modelling semivariance patterns. A deep knowledge of these is not required to fit an empirical variogram to a model. Here are a few popular examples.\nExponential\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0 \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r}) } \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n  Theoretical Exponential Variogram  Gaussian\n(a squared version of the exponential model)\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0, \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r})^2} \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n  Theoretical Gaussian Variogram  Matérn\n\u0026lt;/An extremely complicated mathematical model/\u0026gt;\n  Empirical Matérn Variogram  There are many more models: Cauchy, logistic, spherical, sine, \u0026hellip;.\n For more information on these models, see this workshop\u0026rsquo;s accompanying online book on this topic and additional SAS resources.   Variogram fitting Picking the right model is done both by comparing the sum of squares of error for different models and by\nNot all variables have spatial autocorrelation\n  Not all fitted variogram models are worthy\n  Variogram gone bad  Code for this section The following scripts build upon work done in previous section(s).\nR # load libraries library(gstat); library(spaMM) # set up spatial object Nin_spatial \u0026lt;- Nin_na coordinates(Nin_spatial) \u0026lt;- ~ col.width + row.length # add attribte class(Nin_spatial) # establish max distance for variogram estimation max_dist = 0.6*max(dist(coordinates(Nin_spatial))) # calculate empirical variogram resid_var1 \u0026lt;- gstat::variogram(yield ~ rep + gen, cutoff = max_dist, width = max_dist/15, # 15 is the number of bins data = Nin_spatial) plot(resid_var1) # empirical variogram #Note: To fit a large number of models, the function 'autofitVariogram()' from the package automap can be used (is it calling gstat::variogram) # starting value for the nugget nugget_start \u0026lt;- min(resid_var1$gamma) # initialise the model (this does not do much) Nin_vgm_exp \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential Nin_vgm_gau \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian Nin_vgm_mat \u0026lt;- vgm(model = \u0026quot;Mat\u0026quot;, nugget = nugget_start) # Matern # actually do some fitting! Nin_variofit_exp \u0026lt;- fit.variogram(resid_var1, Nin_vgm_exp) Nin_variofit_gau \u0026lt;- fit.variogram(resid_var1, Nin_vgm_gau) Nin_variofit_mat \u0026lt;- fit.variogram(resid_var1, Nin_vgm_mat, fit.kappa = T) plot(resid_var1, Nin_variofit_exp, main = \u0026quot;Exponential model\u0026quot;) plot(resid_var1, Nin_variofit_gau, main = \u0026quot;Gaussian model\u0026quot;) plot(resid_var1, Nin_variofit_mat, main = \u0026quot;Matern model\u0026quot;) attr(Nin_variofit_exp, \u0026quot;SSErr\u0026quot;) attr(Nin_variofit_gau, \u0026quot;SSErr\u0026quot;) attr(Nin_variofit_mat, \u0026quot;SSErr\u0026quot;) # parameters: Nin_variofit_gau nugget \u0026lt;- Nin_variofit_gau$psill[1] # measurement error (other random error) range \u0026lt;- Nin_variofit_gau$range[2] # distance to establish independence between data points sill \u0026lt;- sum(Nin_variofit_gau$psill) # maximum semivariance   SAS # calculate semivariance and compute empirical variogram proc variogram data=residuals plots(only)=(semivar); coordinates xc=Col yc=Row; compute lagd=1.2 maxlags=30; var resid; run; # fit models to the empirical variogram proc variogram data=residuals plots(only)=(fitplot); coordinates xc=Col yc=Row; compute lagd=1.2 maxlags=30; model form=auto(mlist=(gau, exp, pow, sph) nest=1); var resid; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"a0e4217f5d6273e1edb47a4a4044c93d","permalink":"/events/workshops/spatial-workshop/variograms/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/variograms/","section":"events","summary":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):","tags":null,"title":"Empirical Variograms","type":"book"},{"authors":null,"categories":null,"content":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model. In RCBD design, often the treatments are treated as fixed and the block effect as random.\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ block\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)$\nHere is an expanded version of the last term:\n$$ \\epsilon_{ij} ~\\sim N \\Bigg( 0, \\left[ { \\begin{array}{ccc} \\sigma \u0026amp; \\cdots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n0 \u0026amp; \\cdots \u0026amp; \\sigma \\end{array} } \\right] \\Bigg) $$\nThis is a mathematically representation of iid, independent and identically distributed, an assumption of linear models. When there is spatial autocorrelation, observations closer to one another are correlated, so the off-diagonals in the variance-covariance matrix are not zero.\nSpatial models seek to mathematically model this covariance so it is properly accounted for during hypothesis testing and prediction.\nCode for this section The following scripts build upon work done in previous section(s).\nR library(emmeans); library() # (nlme and gstat should already be loaded) library(spaMM) # for running `corMatern()` # standard linear model nin_lme \u0026lt;- lme(yield ~ gen, random = ~1|rep, data = Nin, na.action = na.exclude) # extract the esimated marginal means for variety preds_lme \u0026lt;- as.data.frame(emmeans(nin_lme, \u0026quot;gen\u0026quot;)) # use information from the variogram fitting for intialising the parameters nugget \u0026lt;- Nin_variofit_gau$psill[1] range \u0026lt;- Nin_variofit_gau$range[2] sill \u0026lt;- sum(Nin_variofit_gau$psill) nugget.effect \u0026lt;- nugget/sill # initalise the covariance structure (from the nlme package) cor.gaus \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row.length + col.width, nugget = T, fixed = F, type = \u0026quot;gaussian\u0026quot;, metric = \u0026quot;euclidean\u0026quot;) # update the rcbd model nin_gaus \u0026lt;- update(nin_lme, corr = cor.gaus) # extract predictions for 'gen' preds_gaus \u0026lt;- as.data.frame(emmeans(nin_gaus, \u0026quot;gen\u0026quot;) # a similar procedure can be follow for other models # but we are going to take a shortcut and not specify the parameters # exponential cor.exp \u0026lt;- corSpatial(form = ~ row.length + col.width, nugget = T, fixed = F) nin_exp \u0026lt;- update(nin_lme, corr = cor.exp) preds_exp \u0026lt;- as.data.frame(emmeans(nin_exp, \u0026quot;gen\u0026quot;)) # Matern structure cor.mat \u0026lt;- corMatern(form = ~ row.length + col.width, nugget = T, fixed = F) nin_matern \u0026lt;- update(nin_lme, corr = cor.mat) preds_mat \u0026lt;- as.data.frame(emmeans(nin_matern, \u0026quot;gen\u0026quot;)   SAS proc mixed data=alliance ; class entry rep; model yield = entry ; random rep; lsmeans entry/cl; ods output LSMeans=NIN_RCBD_means; title1 'NIN data: RCBD'; run; proc mixed data=alliance maxiter=150; class entry; model yield = entry /ddfm=kr; repeated/subject=intercept type=sp(gau) (Row Col) local; parms (11) (22) (19); lsmeans entry/cl; ods output LSMeans=NIN_Spatial_means; title1 'NIN data: Gaussian Spatial Adjustment'; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"02c2e4c9ad0bb229f418ece4e4e7af01","permalink":"/events/workshops/spatial-workshop/correlated-error-models/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/correlated-error-models/","section":"events","summary":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model.","tags":null,"title":"Linear Models with Correlated Errors","type":"book"},{"authors":null,"categories":null,"content":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation. However, sometimes that is accurately describe a field trial. There can be experiment-wide gradients due to position on a slope, proximity to an influential environmental factor (e.g. a road), and so on. In these instances, those gradients should be modelled as a trend.\nBlocking Blocking is one example of modelling an experiment wide-trend:\n  The expectation is that each block will capture and model existing variation within it. This becomes difficult to justify as blocks become large.\nRows \u0026amp; Ranges Recall the RCBD model from the previous section:\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\nTrials rows and ranges can likewise be modelled directly through expansion of that model (and omitting block since it full represented by column):\n$$Y_ijk = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ row $\\gamma_k$ is the effect due to the $k^{th}$ range (or column)\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)\nCode for Trends The following scripts build upon work done in previous section(s).\nR # load libraries library(lme4) # exploratory plots boxplot(yield ~ rep, data = Nin, xlab = \u0026quot;block\u0026quot;, col = \u0026quot;red2\u0026quot;) boxplot(yield ~ row, data = Nin, xlab = \u0026quot;row\u0026quot;, col = \u0026quot;dodgerblue2\u0026quot;) boxplot(yield ~ col, data = Nin, xlab = \u0026quot;column\u0026quot;, col = \u0026quot;gold\u0026quot;) ## row/column model ## # data prep Nin$rowF = as.factor(Nin$row) Nin$colF = as.factor(Nin$col) # specify model nin.rc \u0026lt;- lmer(yield ~ gen + (1|colfF) + (1|rowF), data = Nin, na.action = na.exclude) # extract random effects for row and column ranef(nin_rc) # extract predictions nin_rc \u0026lt;- as.data.frame(emmeans(nin.rc, \u0026quot;gen\u0026quot;))   SAS # exploratory boxplots proc sgplot data=alliance; vbox yield/category=rep FILLATTRS=(color=red) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; proc sgplot data=alliance; vbox yield/category=Col FILLATTRS=(color=yellow) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; proc sgplot data=alliance; vbox yield/category=Row FILLATTRS=(color=blue) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; # row/column model proc mixed data=alliance ; class entry rep; model yield = entry row col/ddfm=kr; random rep; lsmeans entry/cl; ods output LSMeans=NIN_row_col_means; title1 'NIN data: RCBD'; run;   Splines Polynomial splines are an additional method for spatial adjustment and represent a more non-parametric method that does not rely on estimation or modeling of variograms. Instead, it uses the raw data and residuals to fit a surface to the spatial data and adjust the variance covariance matrix accordingly.\nCode for Splines The following scripts build upon work done in previous section(s).\nR nin_spline \u0026lt;- SpATS(response = \u0026quot;yield\u0026quot;, spatial = ~ PSANOVA(col, row, nseg = c(10,20), degree = 3, pord = 2), genotype = \u0026quot;gen\u0026quot;, random = ~ rep, # + rowF + colF, data = Nin, control = list(tolerance = 1e-03, monitoring = 0)) preds_spline \u0026lt;- predict(nin_spline, which = \u0026quot;gen\u0026quot;) %\u0026gt;% dplyr::select(gen, emmean = \u0026quot;predicted.values\u0026quot;, SE = \u0026quot;standard.errors\u0026quot;)   SAS proc glimmix data=alliance ; class entry rep; effect sp_r = spline(row col); model yield = entry sp_r/ddfm=kr; random row col/type=rsmooth; lsmeans entry/cl; ods output LSMeans=NIN_smooth_means; title1 'NIN data: RCBD'; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"63c256626ca5076fd4298c1fdbf11278","permalink":"/events/workshops/spatial-workshop/trend-modelling/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/trend-modelling/","section":"events","summary":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation.","tags":null,"title":"Modelling Spatial Trends","type":"book"},{"authors":null,"categories":null,"content":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model. Some approaches include:\n Comparing model fitness (e.g. AIC, BIC, log likelihood). Although the methods are not nested, hence precluding a log likelihood ratio test, we can compare raw values for each fit statistic. Be careful doing this in R since linear modelling packages use different estimation procedures for maximum likelihood and REML estimation that are not comparable. Comparing post-hoc power (that is, the p-values for the treatments) Comparing standard error of the estimates (i.e. precision)   Comparing changes in the coefficient of variation (CV, $\\sigma/\\mu$) is not recommended because in many spatial models, field variation has been re-partitioned to the error term when it was (erroneously) absorbed by the other experimental effects. As a result, the CV can increase in spatial models even when inclusion of spatial covariates results in better model fit.   Unfortunately, there is no one method for unambiguously returning the the best estimates and true ranks of the treatments. Likewise, there is no one spatial method that works best in all situations and field trials.\nCode for this section R library(tidyr) # remove some objects we don't need (and will interfere with downstream processes) rm(nin_variofit, nin_vgm) rm(nin_vgm, nin_variofit, nugget, sill, range, nugget.effect) # assemble objects into a list nlme_mods \u0026lt;- list(nin_lme, nin_exp, nin_gaus, nin_matern) names(nlme_mods) \u0026lt;- c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;) # extract log likelihood, AIC, BIC data.frame(loglik = sapply(nlme_mods, logLik), AIC = sapply(nlme_mods, AIC), BIC = sapply(nlme_mods, AIC, k = log(nrow(Nin_na)))) %\u0026gt;% arrange(desc(loglik)) # (higher is better for loglik, lower is better for AIC and BIC) # compare post-hoc power # conduct ANOVA anovas \u0026lt;- lapply(nlme_mods[-7], function(x){ aov \u0026lt;- as.data.frame(anova(x))[2,]}) # bind all the output together a \u0026lt;- bind_rows(anovas) %\u0026gt;% mutate(model = c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;, \u0026quot;row-col\u0026quot;)) %\u0026gt;% arrange(desc(`p-value`)) %\u0026gt;% select(c(model, 1:4)) rownames(a) \u0026lt;- 1:nrow(a) a ## compare precision of estimates all.preds \u0026lt;- mget(ls(pattern = \u0026quot;^preds_*\u0026quot;)) errors \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;SE\u0026quot;) pred.names \u0026lt;- gsub(\u0026quot;preds_\u0026quot;, \u0026quot;\u0026quot;, names(errors)) error_df \u0026lt;- bind_cols(errors) colnames(error_df) \u0026lt;- pred.names boxplot(error_df, ylab = \u0026quot;standard errors\u0026quot;, xlab = \u0026quot;linear model\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;) # compare predictions preds \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;emmean\u0026quot;) preds_df \u0026lt;- bind_cols(preds) colnames(preds_df) \u0026lt;- pred.names preds_df$gen \u0026lt;- preds_exp$gen # plot changes in rank lev \u0026lt;- c(\u0026quot;lme\u0026quot;, \u0026quot;exp\u0026quot;, \u0026quot;gaus\u0026quot;, \u0026quot;mat\u0026quot;) pivot_longer(preds_df, cols = !gen, names_to = \u0026quot;model\u0026quot;, values_to = \u0026quot;emmeans\u0026quot;) %\u0026gt;% mutate(model = factor(model, levels = lev)) %\u0026gt;% ggplot(aes(x = model, y = emmeans, group = gen)) + geom_point(size = 5, alpha = 0.5, col = \u0026quot;navy\u0026quot;) + geom_line() + ylab(\u0026quot;yield means for gen\u0026quot;) + theme_minimal()   SAS data NIN_RCBD_means (drop=tvalue probt alpha estimate stderr lower upper df); set NIN_RCBD_means; RCB_est = estimate; RCB_se = stderr; run; data NIN_Spatial_means (drop=tvalue probt alpha estimate stderr lower upper df); set NIN_Spatial_means; Sp_est = estimate; Sp_se = stderr; run; proc sort data=NIN_RCBD_means; by entry; run; proc sort data=NIN_Spatial_means; by entry; run; data compare; merge NIN_RCBD_means NIN_Spatial_means; by entry; run; proc rank data=compare out=compare descending; var RCB_est Sp_est; ranks RCB_Rank Sp_Rank; run; proc sort data=compare; by Sp_rank; run; proc print data=compare(obs=15); var entry rcb_est Sp_est rcb_se sp_se rcb_rank sp_rank; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"7d86f5dc4c457080b2f4f607d36689bd","permalink":"/events/workshops/spatial-workshop/model-comparison/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/model-comparison/","section":"events","summary":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model.","tags":null,"title":"Comparing Models","type":"book"},{"authors":null,"categories":null,"content":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible. Often, the primary goal of the studies using this design is to rank or select genotypes.\nAugmented models are analyzed in a fundamentally different method than RCBD models due to the large number of unreplicated observations. To adjust for the lack of replication, only a select set of treatments, usually of known performance, are replicated in the experiments. The error estimated from these replicated treatments is used in the analysis to evaluate the remaining genotypes.\nThere are multiple way to specify an augmented model depending on what the researcher wants to know.\nModel specification #1 $$ Y_{ij} = \\tau_i + \\beta(\\tau)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\tau_i$ is the effect of each check and the average effect of all unreplicated treatments $ \\beta(\\tau)_{ij}$ is is the effect of the $j^{th}$ unreplicated treatment nested within the overall effect of unreplicated treatments  This model evaluates:\n The difference between all checks and the average of the unreplicated treatments. The difference between the unreplicated treatments.  Model specification #2 $$ Y_{ij} = \\delta_i + \\gamma(\\delta)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\delta_i$ is the average effect of all checks and the average effect of all unreplicated treatments (so there are only 2 treatment levels) $ \\gamma(\\delta)_{ij}$ is is the effect of the $j^{th}$ treatment nested within the either unreplicated treatments or the check observations  This model evaluates:\n The difference between the average of the checks and the average of the unreplicated treatments The difference between all treatments  These models are described more in depth in Burgueño et al, 2018, along with a helpful discussion on when to treat any of these effects as fixed or random\nThe data used here refer to a wheat genotype evaluation study carried out near Lind Washington. The study looked at 922 unreplicated genotypes (‘name’) accompanied by 9 replicated check wheat cultivars.\nCode for this section The following scripts build upon work done in previous section(s).\nR # (if not already loaded) library(dplyr); library(nlme); library(ggplot2) library(gstat); library(sp) # read in data aug_data_origin \u0026lt;- read.csv(\u0026quot;data/augmented_lind.csv\u0026quot;, na.strings = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;999999\u0026quot;)) %\u0026gt;% slice(-1) %\u0026gt;% # first line not needed mutate(yieldkg = yieldg/1000) # to prevent overflow # summarise the genoytypic data by checks/not checks gen_sum \u0026lt;- group_by(aug_data_origin, name) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(delta = case_when( counts \u0026gt; 1 ~ \u0026quot;check\u0026quot;, counts == 1 ~ \u0026quot;unrep\u0026quot;)) # need info on just the checks checks \u0026lt;- gen_sum %\u0026gt;% filter(delta == \u0026quot;check\u0026quot;) # more summarise steps for different augmented modes gen_sum2 \u0026lt;- gen_sum %\u0026gt;% mutate(gamma = name) %\u0026gt;% mutate(tau = case_when( delta == \u0026quot;check\u0026quot; ~ gamma, delta == \u0026quot;unrep\u0026quot; ~ \u0026quot;unreplicate_obs\u0026quot;)) %\u0026gt;% mutate(beta = case_when( delta == \u0026quot;unrep\u0026quot; ~ gamma, delta == \u0026quot;check\u0026quot; ~ gamma)) # merge original data set with info on treatment levels aug_data \u0026lt;- aug_data_origin %\u0026gt;% select(name, prow, pcol, yieldkg, yieldg) %\u0026gt;% mutate(row = prow*11.7, col = pcol*5.5) %\u0026gt;% full_join(gen_sum2, by = \u0026quot;name\u0026quot;) ## modelling aug1 \u0026lt;- lme(fixed = yieldg ~ tau, random = ~ 1|tau/beta, data = aug_data, na.action = na.exclude) # extract residuals aug_data$res \u0026lt;- residuals(aug1) # plot residual chloroepleth map: ggplot(aug_data, aes(y = row, x = col)) + geom_tile(aes(fill = res)) + scale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;black\u0026quot;) + scale_x_continuous(breaks = seq(1,max(aug_data$row), 1)) + scale_y_continuous(breaks = 1:max(aug_data$col)) + coord_equal() + theme_void() # add spatial covariates aug_spatial \u0026lt;- aug_data %\u0026gt;% filter(!is.na(res)) coordinates(aug_spatial) \u0026lt;- ~ col + row max_dist = 0.5*max(dist(coordinates(aug_spatial))) aug_vario \u0026lt;- gstat::variogram(res ~ 1, cutoff = max_dist, width = max_dist/10, data = aug_spatial) # optional to run: nugget_start \u0026lt;- min(aug_vario$gamma) aug_vgm \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) aug_variofit \u0026lt;- fit.variogram(aug_vario, aug_vgm) plot(aug_vario, aug_variofit, main = \u0026quot;Exponential model\u0026quot;) cor_exp \u0026lt;- corSpatial(form = ~ row + col, nugget = T, fixed = F, type = \u0026quot;exponential\u0026quot;) aug1_sp \u0026lt;- update(aug1, corr = cor_exp) # spatial parameters: aug1_sp$modelStruct$corStruct # extract BLUPs for unreplicated lines: aug1_blups \u0026lt;- ranef(aug1_sp)$beta %\u0026gt;% rename(yieldg = '(Intercept)') # look at variance components VarCorr(aug1_sp) ##### OR ####### # another formulation # delta estimates effects of replicated versus unreplicated genotypes # gamma estimates the effecs of all genotypes evaluated in the trial aug2 \u0026lt;- lme(fixed = yieldkg ~ delta, random = ~ 1|delta/gamma, data = aug_data, na.action = na.exclude) aug2_sp \u0026lt;- update(aug2, corr = cor_exp) # spatial parameters: aug2_sp$modelStruct$corStruct # extract BLUPs for unreplicated lines: aug_blups2 \u0026lt;- ranef(aug2_sp)$gamma %\u0026gt;% rename(yieldg = '(Intercept)') # look at variance components VarCorr(aug1_sp)   SAS filename AUG url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/AB19F5_LIND.csv\u0026quot;; PROC IMPORT OUT= WORK.augmented DATAFILE= AUG DBMS=CSV REPLACE; GETNAMES=YES; DATAROW=2; RUN; data augmented; set augmented; if yieldg = 999999 or yieldg=. then delete; /* Remove missing values */ prow=prow*11.7; /*convert row and column indices to feet */ pcol=pcol*5.5; run; proc freq noprint data=augmented; tables name/out=controls; run; data controls; set controls; if count \u0026gt;1; run; proc sort data=controls; by name; run; proc sort data=augmented; by name; run; data augmented; merge augmented controls; by name; if count=. then d2=2; /* Unreplicated */ else d2=1; /* Replicated */ yieldkg=yieldg/1000; run; PROC mixed data=augmented; class name d2; model yieldkg = d2/noint outp=residuals ddf=229 229; lsmeans d2; *lsmeans name(d2)/slice = d2; run; proc sgplot data=residuals; HEATMAPPARM y=pRow x=pCol COLORRESPONSE=resid/ colormodel=(cx014458 cx1E8C6E cxE1FE01); title1 'Field Map'; run; proc variogram data=residuals plots(only)=(fitplot); where yieldkg ^= .; coordinates xc=pcol yc=pRow; compute lagd=6.6 maxlags=25; model form=auto(mlist=(gau, exp, pow, sph) nest=1); var resid; run; PROC mixed data=augmented; class name d2; model yieldkg = d2 name(d2)/outp=adjresiduals ddf=229 229; lsmeans d2; repeated/subject=intercept type=sp(pow)(prow pcol) local; ods output SolutionR =parms; parms (0.074) (0.0051)(0.475) ; *lsmeans name(d2)/slice = d2; # alot of output!! run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"9f6fc9d604ca07fb11f97f4120437c55","permalink":"/events/workshops/spatial-workshop/augmented/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/augmented/","section":"events","summary":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible.","tags":null,"title":"Augmented Designs","type":"book"},{"authors":null,"categories":null,"content":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time. However, investigating spatial correlation in a field trial and controlling for it if necessary using any of the methods developed for this is recommended over doing nothing.\nThere is no denying that work is needed to develop scripts that automate this process so researchers can routinely incorporate spatial covariance into field trial analysis. Many current R tools are unwieldy to use and have insufficient options to support variety trial analysis.\nUntil this situation is improved, it is probably wisest to focus on using spatial models that are well-supported at this time. Any of the options implemented in the nlme package (or that work with that package) are decent choices with excellent support for extracting least-squares means, running ANOVA, and standard model diagnostics. Furthermore, nlme supports generalized linear models. INLA is established is supported by a large and growing user base, and breedR is likewise well established.\nOther resources   Incorporating Spatial Analysis into Agricultural Field Experiments, a more comprehensive version of this tutorial\n  CRAN task view on analysis of spatial data\n  Other R packages\n     package usage     breedR mixed modelling with AR1xAR1 estimation   inla Bayesian modelling with options for spatial covariance structure   Mcspatial nonparametric spatial analysis, (no longer on CRAN)   ngspatial spatial models with a focus on generalized linear models   sommer mixed models, including an AR1xAr1 model   spamm Matérn covariance structure   spANOVA spatial lag models for field trials   spatialreg spatial functions for areal data    The package sommer implements a version of the AR1xAR1 covariance structure. However, it does not estimate the parameter $\\rho$. The user must specify the $\\rho$ and that value is not optimized in the restricted maximum likelihood estimation. Both BreedR and inla implement an AR1xAR1 covariance structure. Additional, SAS and the proprietary software asreml can implement a mixed model with this covariance structure.\nBooks for the deep dive     Statistics for Spatial Data\n  Applied Spatial Data Analysis with R, available for free\n  Spatio-Temporal Statistics With R (also free)\n  Spatial Data Analysis in Ecology and Agriculture Using R\n  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"f4c344325c395cf7d4a14875fb68e0e1","permalink":"/events/workshops/spatial-workshop/conclusion/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/events/workshops/spatial-workshop/conclusion/","section":"events","summary":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time.","tags":null,"title":"Final thoughts","type":"book"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"R is notoriously difficult, finicky and at times, needlessly challenging (note that I just said the same thing three times, but the repetition is for emphasis). R is also powerful, flexible and extraordinarily nimble. If you want to try out that great new thing, it\u0026rsquo;s probably possible to do it in R without programming it from scratch. But, I\u0026rsquo;m not doing you any favors by pretending that learning R is easy and painless. It is hard. R is a powerful programming language, and as usual, with great power, comes a lot of learning, practice and preparation to know how to use those powers.\nIn time, you may find you truly love solving the various logic puzzles R puts before you. Or not. Regardless, I wish you the best on your journey to learning R. This far from a comprehensive list (that\u0026rsquo;s not the point), but here are a few resources to help get you started.\nResources for the Beginner R User  RStudio Primers R for Excel Users R for Reproducible Research Short Introduction to R YaRrr! The Pirate’s Guide to R EdX short courses  Resources for the Intermediate R User  R 4 Data Science What They Forgot to Teach You (about R)  Miscellaneous very useful resources  RStudio cheatsheets R Graphics Cookbook (a ggplot resource) Rmarkdown: the Definitive Guide Rweekly Blog (great place to learn about updates to the R ecosystem) R Studio Resources (there\u0026rsquo;s so much here!) RStudio YouTube Channel Happy Git with R Advanced R (book, it is rather advanced)  ","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"22fbdc53a7beed0471add610775ef731","permalink":"/post/learning-r/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/post/learning-r/","section":"post","summary":"R is notoriously difficult, finicky and at times, needlessly challenging (note that I just said the same thing three times, but the repetition is for emphasis). R is also powerful, flexible and extraordinarily nimble.","tags":["R","beginner R user","learning R"],"title":"Resources for Learning R","type":"post"},{"authors":["Julia Piaskowski"],"categories":null,"content":"Poster presented at the 2022 Conference on Applied Statistics in Agriculture and Natural Resources held in Logan, Utah.\nThis is a part of an effort to establish a CRAN task view for agricultural research. Feel free to contribute to this at our GitHub repository. You can find a prose version of this poster here.\nAbstract The language ‘R’ has become quite popular for analysis among agriculture researchers. R is powerful and agile language that is constantly undergoing improvements. As an open-source language, its utility is constantly expanded upon by user-contributed packages and additional developer resources. These include whole software packages like Rstudio, as well as tutorials, and cheatsheets. Keeping up with changes and advancements in this dynamic ecosystem is difficult. We have sought to provide resources for R users in agricultural research. We have established a central website to gather all related information, www.agstats.io that includes workshops, tutorials and short instructional blog posts. We also have built an accompanying “R Universe” repository, https://idahoagstats.r-universe.dev intended to serve as a guide to existing R packages serving different facets of agricultural research. Our goal is to help agricultural researchers fully utilize the array of available R resources to reach their goals.\n","date":1652659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652659200,"objectID":"84a0a5e2304d15153614fe88f9ca71d8","permalink":"/events/presentations/agstats-conference-2022/","publishdate":"2022-05-15T00:00:00Z","relpermalink":"/events/presentations/agstats-conference-2022/","section":"events","summary":"A poster on current R resources to support agriculture research.","tags":["R","CRAN"],"title":"Tools and Resources for R Users in Agricultural Research","type":"events"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"Last updated: July 29, 2022\nAgriculture encompasses a broad breadth of disciplines. Many, many package in base R and contributed packages are useful to agricultural researchers. For that reason, this is not exhaustive list of packages useful to agricultural research. This is intended to cover major packages that in most cases, have been developed to support agricultural research and analytical needs.\nNote that some of these packages are on CRAN and others are on GitHub, Bioconductor, or R-forge.\nIf you think that a package is missing from this list, please let us know by filing an issue in the GitHub repository (preferred).\nPackages with general applications Agricultural \u0026amp; land use databases Data from the United States Department of Agriculture\u0026rsquo;s National Agricultural Statistical Service \u0026lsquo;Quick Stats\u0026rsquo; web API can be accessed with rnassqs or with tidyUSDA, which also offers some mapping capabilities. The USDA\u0026rsquo;s Cropland Data Layer API can be accessed with CropScapeR and cdlTools provides various utility functions for processing CDL data. The package rusda provides an interface to access the USDA-ARS Systematic Mycology and Microbiology Laboratory (SMML)\u0026rsquo;s four databases: Fungus-Host Distributions, Specimens, Literature and the Nomenclature database. USDA\u0026rsquo;s Agricultural Resource Management Survey (ARMS) data API can be accessed with rarms. The USDA\u0026rsquo;s Livestock Mandatory Reporting data API can be accessed with usdampr. FAOSTAT and faobulk can be used to access data from the FAOSTAT Database of the Food and Agricultural Organization (FAO) of the United Nations. NASA soil moisture active-passive (SMAP) data can be accessed and processed by smapr.\nFedData provides access to geospatial data from the United States Soil Survey Geographic (SSURGO) database, the Global Historical Climatology Network (GHCN), the Daymet gridded estimates of daily weather parameters for North America, the International Tree Ring Data Bank, and the National Land Cover Database. SSURGO data can also be accessed and processed with XPolaris. Most USDA-NRCS soils related databases and APIs can be accessed with soilDB. SISINTAR provides access to SiSINTA (Sistema de información de Suelos del INTA), a soil profile database for Argentina, and functions for processing the data. SILO weather data from the Queensland DES longpaddock website can be accessed with cropgrowdays. febr has utilities to access and process data from the Brazilian Soil Data Repository.\nPGRdup provides functions to aid the identification of probable/possible duplicates in Plant Genetic Resources (PGR) collections. rfieldclimate provides functionality and parsers to interact with the FieldClimate API.\nAgricultural data sets Agridat consists of a very large collection of agricultural data sets and example analyses; the package contains a vignette detailing additional data sets and extensive resources to support agricultural analysis. agritutorial provides a collection of agricultural data sets and analysis with particular attention to crop experiments. On GitHub, the repository agroBioData houses a collection of data sets supporting agriculture and applied biology (note that this is a collection of CSV files, not a package). The soybean nested associated mapping population data set can be accessed via SoyNAM. simplePhenotypes can be used for simulating pleiotropic, linked and epistatic phenotypes. USGS county data on fertilizer sales can be accessed with ggfertilizer. The FAOSTAT data set collection on the Food and Agriculture Biomass Input–Output model (FABIO) is available through fabio and described more in detail in Bruckner (2019). The R-forge subversion repository \u0026lsquo;cropcc\u0026rsquo; hosts several R packages with climate change/cropping data set. Additionally, many of the agriculture-focused packages listed in this guide also include data sets to illustrate their functionality (e.g. agricolae, AgroTech, ZeBook). Annual agriculture production data from the Peruvian Integrated System of Agricultural Statistics (SIEA) covering 2004 to 2014 can be accessed with cropdatape\nGeneral analytical packages supporting agricultural research The packages nlraa and AgroReg provides general linear and nonlinear regression functions specifically for agricultural applications. agriCensData is a flexible package for working with censored data (e.g. time to flowering, instrumentation values below the detection limit, disease scoring). The package biotools can conduct a wide array of multivariate analysis for agronomists including genetic covariance, optimal plot size, tests for spatial dependence, and tests for seed lot heterogeneity. grapesAgri1 houses a collection of shiny apps, GRAPES (General R-shiny based Analysis Platform Empowered by Statistics), that works as a graphical user interface for individuals to upload data files and analyse. Linear models and ANOVA for CRD and RCBD (2-way) model, correlation analysis, exploratory data analysis and other common hypothesis tests are supported.\nALUES implements methodology developed by the FAO and the International Rice Research Institute for evaluating land suitability for different crop production. AGPRIS (AGricultural PRoductivity in Space) provides functions for different spatial analyses in implemented in INLA and other spatial approaches. The package KenSyn has example data sets and analytical code supporting the book De L\u0026rsquo;analyse des Réseaux Expérimentaux à la Méta-analyse (French) or From Experimental Network to Meta-analysis (English).\nAgrotech provides functionality for making chemical application calculations and example data sets.\nDiscipline-specific packages Agricultural economics The task views for Econometrics, Empirical Finance, and TimeSeries provide information on packages and tools relevant to agriculture economics.\nSeveral packages have been developed specifically for agricultural price forecasting. vmdTDNN forecasts univariate time series data using variational mode decomposition based time delay neural network models as described by Dragomiretskiy 2014. stlELM also conducts univariate time series forecasting univariate time series, using seasonal-trend decomposition procedures based on loess (STL) combined with the extreme learning machine developed by Xiong et al 2018. eemdTDNN does the same, utilizing different decomposition based time delay neural network models. For method details, see Yu et al 2008.\nAgrometeorology The Hydrology CRAN Task View has many resources for accessing and processing weather and climate data. Meteor provides a set of functions for weather and climate data manipulation to support crop and crop disease modeling. Data from the Copernicus data set of agrometeorological indicators can be downloaded and extracted using ag5Tools. The frost package contains a compilation of empirical methods used by farmers and agronomic engineers to predict the minimum temperature to detect a frost event. agroclim contains functions to compute agroclimatic indices useful to zoning areas based on climatic variables and to evaluate the importance of temperature and precipitation for individual crops or in general for agricultural lands. cropgrowdays can be used for calculating growing degree days, cumulative rainfall, number of stress day, mean radiation, evapotranspiration and other variables. It also can be used to access SILO weather data from the Queensland DES longpaddock website. Climate crop zones in Brazil can be accessed and calculated with cropZoning using data sets from TerraClimate that are calibrated to weather stations run by the National Meteorological Institute of Brazil. The package acdcR (Agroclimatic Data by County) provides functions to calculate widely-used county-level variables in agricultural production or agroclimatic and weather analyses. LWFBrook90R provides an implementation of the soil vegetation atmosphere transport (SVAT) model LWF-BROOK90 to calculate daily evaporation (transpiration, interception, and soil evaporation) and soil water fluxes, along with soil water contents and soil water tension of a soil profile covered with vegetation. Leaf area index and soil moisture from microwave backscattering data based on the WCM model can be calculated with the WCM package.\nAgronomic trials Experimental design The package agricolae provides extensive resources for the planning and analysis of planned field experiments. Designs constructed by agricolae can be visualised with agricolaeplotr. The CRAN task for ExperimentalDesign provide additional information on experimental design for a wide variety of research problems. desplot is for plotting maps of agricultural trials laid out in grids. DiGGer was developed for rectangular field trials; its purpose is to help users determine the optimal experimental design based on the treatment structure and number of replicates. inti provides functionality for experimental design and manipulation and it is focused on FieldBook compatibility.\nHigh throughput phenotyping (HTP) statgenHTP is for analyzing data from HTP platform experiments, with some functions specifically designed to work with the proprietary software asreml. CropDetectR can be used to identify crop rows from image data. FWRGB can process plant images for downstream machine learning models to predict fresh biomass. pliman provides tools for image manipulation to quantify plant leaf area, disease severity, number of disease lesions, and obtain statistics of image objects such as grains, pods, pollen, leaves, and more.\nTrial analysis The package agricolae contains functions for analyzing many common designs in agriculture trials such as split plot, lattice, Latin square and some additional functions such AMMI and AUDPC calculations. Trials utilizing an incomplete block design can be analylsed used ispd. statgenSTA has functions for single trial analysis with and without spatial components. The proprietary software asreml provides an R version of their mixed model fitting functions for field trial analysis (note this is not open source and also requires an annual license). CRAN also contains an add-on package asremlPlus that provides several accessory functions to asreml. INLA provides tools for Bayesian inference of latent Gaussian models. It contains functions for modelling spatial variation, such as field experiments or farm locations. The gosset package provides the toolkit for a workflow to analyse experimental agriculture data, from data synthesis to model selection and visualisation. AgroR has general functions and a shiny app for analysis of common designs in agriculture: CRD, RCBD and Latin square.\nSpATS can be used to adjust for field spatial variation using p-splines. A localised method of spatial adjustment for unreplicated trials, moving grid adjustment, is implemented with mvngGrAd. ClimMobTools is the API Client for the ClimMob citizen science platform in R for agronomic trials.\nAnimal science usdampr provides access to the USDA\u0026rsquo;s Livestock Mandatory Reporting API. Many of the genetic packages described in this resource can also be applied to animals.\nBreeding \u0026amp; quantitative genetics See the R package repository Bionconductor for bioinformatics tools to support the processing of high-throughput genomic data.\nlmDiallel provides service functions for analysing data sets obtained from diallel experiments, as described in Onofri 2020. plantbreeding (available on R-forge: install.packages(\u0026quot;plantbreeding\u0026quot;, repos=\u0026quot;http://R-Forge.R-project.org\u0026quot;)) provides many convenience functions for working with populations and designs common in plant breeding including dialleles, line testers, augmented trials, the Carolina design, and more. st4gi and variability provides several common utility functions for genetic improvement of crops. Also, please see the subsection on \u0026ldquo;genotype-by-environment interactions\u0026rdquo; for packages integrating environmental and genomic data in an analytical framework. gpbStat provides functions for common plant breeding analyses including line-by-tester analysis (Arunachalam 1974) and diallel analysis (Griffing, 1956). heritability implements marker-based estimation of heritability when observations on genetically identical replicates are available. selection.index calculates a selection index using the Smith (1973) method.\nAlphaSimR is an implementation of the AlphaSim algorithm in R, providing functions for stochastic modelling of processes common to breeding programs such as selection and crossing. MoBPS has a suite of functions for simulating genetic gain and economic costs in a plant breeding program. isqg provides functions for high performance quantitative genetic simulations using a bitset-based algorithm.\nLinkage mapping \u0026amp; QTL analysis There are two notable and long-standing packages: (1) onemap, providing MapMaker/EXP like performance and extended functionality, and (2) qtl providing standard functionality for qtl mapping and accessory functions for simulating crosses. ASMap is for fast linkage mapping with the algorithm \u0026lsquo;MSTmap\u0026rsquo;. MapRtools is another linkage mapping package. Linkage maps can be visualized with LinkageMapView. For polyploids, the packages mappoly and polymapR can be used for linkage mapping and the packages qtlpoly and polyqtlR can be used for qtl estimation. diaQTL is for QTL and haplotype analysis of diallel populations (diploid and autotetraploid). statgenMPP can conduct QTL mapping in multi-parent populations.\nGWAS Genome-wide association study analysis can be conducted with statgenGWAS. GWAS models across very large number of SNPs or observations can be estimated with rMVP and megaLMM. Functions for autotetraploids are provided by GWASpoly, and these functions also work in diploid species. StageWise provides functions to conduct a 2-stage GWAS when the underlying phenotypic data are from multiple field trials. Variable selection for ultra-large dimensional GWAS data sets can be done with bravo, which implements a Bayesian algorithm, selection of variables with embedded screening SVEN. statgenIBD can calculate IBD probabilities for biparental, three and four-way crosses. For polyploids, polyBreedR provides convenience functions to facilitate the use of genome-wide markers for breeding autotetraploid species, and its functionality also extends to diploids.\nGenomic prediction Packages supporting genetic prediction using mixed models augmented with pedigree or genetic marker data include sommer, rrBLUP, BGLR, lme4gs, lme4qtl, pedigreemm, qgtools, cpgen, QTLrel, and the licensed software asreml. Many of these packages have built-in functionality for data preparation steps including data imputation and calculation of the relationship matrices. breedR is a general purpose package for performing quantitative genetic analyses. Genome feature mixed linear models using frequentist and Bayesian approaches can be implemented with qgg. pedmod provides linear modelling functions integrating kinship for categorical outcomes.\nSTGS implements several genomic selection models for single traits. GSelection implements genomic selection integrating additive and non-additive models. GSMX, multivariate genomic selection, estimates trait heritability and handles overfitting through cross validation. TSDFGS can estimate the optimal training population size and composition for genomic selection. BGGE conducts genomic prediction for continuous variables, focused on genotype-by-environment genomic selection models following the methods of Jarquín 2014. BMTME builds genomic selection prediction models that an be expanded to multiple traits and environments using Bayesian models developed by Montesinos-Lopéz (2016, 2018a, 2018b). BWGS, \u0026ldquo;Breed Wheat Genomic Selection\u0026rdquo;, provides a pipeline of functions for conducting genomic selection in hexaploid wheat.\nAGHmatrix provides extensive options for calculating pedigree and genomic (additive and dominance) relationships. The pedigree packages provides functionality for ordering pedigrees, calculating and inverting the A matrix and other related tasks.\nCrop growth models \u0026amp; crop modelling The apsimx package has functions to read, inspect, edit and run files for APSIM \u0026ldquo;Next Generation\u0026rdquo; (json) and APSIM \u0026ldquo;Classic\u0026rdquo; (xml). Files with an .apsim extension correspond to APSIM Classic, the files with an .apsimx extension correspond to APSIM Next Generation. rapsimng works with Next Generation APSIM files. DSSAT provides a comprehensive R interface to the Decision Support System for Agrotechnology Transfer Cropping Systems Model (DSSAT-CSM) documented by Jones (2003). This package provides cross-platform functions to read and write input files, run DSSAT-CSM, and read output files. The modelling framework Simplace (Scientific Impact assessment and Modelling PLatform for Advanced Crop and Ecosystem management) can be accessed using the R package simplace.\nMeteor provides a set of functions for weather and climate data manipulation to support crop and crop disease modeling. cropDemand can be used to estimate crop water demand in Brazilian production regions using the TerraClimate data set. Evapotranspiration can estimate potential and actual evapotranspiration using 21 different models. metrica has many convenience functions for comparing model predictions with ground truth data. ZeBook provides data sets and examples to accompany the book Working with Dynamic Crop Models.\nphenorice is an R implementation of the phenorice model for remote sensing of rice crop production. phenoriceR provides helper functions for processing data from the phenorice model. Recocrop estimates environmental suitability for plants using a limiting factor approach for plant growth following Hackett (1991). Rquefts provides an implementation of the QUEFTS (Quantitative Evaluation of the Native Fertility of Tropical Soils) model (Janssen 1990). Rwofost is an implementation of the WOFOST (\u0026ldquo;World Food Studies\u0026rdquo;) crop growth model (De Wit 2019).\nEntomology hnp Generates half-normal plots with simulation envelopes using different diagnostics from a range of different fitted models. A few example data sets are included. The package agriCensData provides functions for dealing with censored data. In addition, the survival CRAN Task View list CRAN resources for working with censored data.\nFood science For packages supporting sensory studies, see the Psychometrics CRAN task view. NutrienTrackeR provides convenience functions for calculating nutrient content (macronutrients and micronutrients) of foods using food composition data from several reference databases, including: \u0026lsquo;USDA\u0026rsquo; (United States), \u0026lsquo;CIQUAL\u0026rsquo; (France), \u0026lsquo;BEDCA\u0026rsquo; (Spain) and \u0026lsquo;CNF\u0026rsquo; (Canada).\nGenotype-by-environment interactions statgenGxE has several functions for handling various analytical approaches for addressing genotype-by-environment interactions. IBCF.MTME implements item-based collaborative filtering for multi-trait and multi-environment trials. The package gge is useful for producing GGE biplots, while bayesammi can conduct Bayesian estimation of additive main effects multiplicative interaction model. EnvRtype can be used for assembling climate data, data set preparation and environmental classification. FW implements Finlay-Wilkinson regression using a Gibbs sampler; spFW also conducts spatial Finlay-Wilkinson analysis for multi-environmental trials using a Bayesian hierarchical model. A wide variety of stability analysis statistics can be calculated via agrostab including coefficient of homeostaticity, specific adaptive ability, weighted homeostaticity index, superiority measure, regression on environmental index, Tai\u0026rsquo;s stability parameters, stability variance, ecovalence and other stability parameters.\nPlant pathology epifitter provides functions for analysis and visualization of plant disease progress curve data. epiphy is a toolbox for analyzing plant disease epidemics. It provides a common framework for plant disease intensity data recorded over time and/or space. hagis has functions for analysis of plant pathogen pathotype survey data. Functions provided calculate distribution of susceptibilities, distribution of complexities with statistics, pathotype frequency distribution, as well as diversity indices for pathotypes. Populations with mixed clonal/sexual reproductive strategies can be analyzed with poppr, which has population genetic analysis tools for hierarchical analysis of partially clonal populations. ascotraceR can simulate an Ascochyta blight infection in a chickpea field following the model developed by Diggle (2022)). Stochastic disease modelling of plant pathogens incorporating spatial and genetic information can be done with landsepi. Evolution of resistance genes under pesticide pressure can be simulated under different numbers of pests, modes of pest reproduction, resistance loci, number of pesticides and other facets with resevol.\nRural sociology See the CRAN task view for Psychometrics for general sociology packages. . Both the Survival CRAN task view and the agriCensData package provide tools for working with interval and censored data.\nSoil science and precision agriculture sharpshootR contains a compendium of utility functions supporting soils survey work including data management, summary, visualizations and conversions.For soil pedology, aqp provides a general toolkit for soil scientists: specialized data structures, soil profile summary, visualisation, color conversion, and more. SoilTaxonomy provides functions for parsing soil taxonomic terms. The \u0026ldquo;Spatial and Spatio-Temporal CRAN task views provide extensive resources in spatial statistics. pedometrics has many utility functions for common analyses of soil data.\nSoil water retention curves can be calculated by the soilwater packages using the Van Genuchten method for soil water retention and Mualem method for hydraulic conductivity. SoilR models soil organic matter decomposition in terrestrial ecosystems with linear and nonlinear models. Soil texture triangles can be graphed using soiltexture; this package can also classify and transform soil texture data. sorcering can be used to model soil organic carbon and soil organic nitrogen and to calculate N mineralisation rates. QI can be used to calculate potassium intensity and exchangeability. soiltestcorr has functions for conducting correlation analysis between soil test values and crop yield data. SoilTesting provides functions for calculating soil mineral concentrations from analytical lab results. fertplan provides fertilizer recommendations based on soil test results (note this packages is optimized for horticultural crop production in Italy). DMMF implements the daily based Morgan-Morgan-Finney (DMMF) soil erosion model (Choi et al., 2017) for estimating surface runoff and sediment budgets from a field or a catchment on a daily basis. Estimation and prediction of parameters of soil hydraulic property models can be accomplished with spsh.\nAgriculture image features from spectral data can extracted with agrifeature. It has functions to calculate gray level co-occurrence matrix (GLCM), RGB-based vegetative index (RGB VI) and normalized difference vegetation index (NDVI). Experimental units (e.g. plots) can be obtained from spectral images using rPAex. mapsRinteractive provides functions for working with soil point data in raster format. The suitability of specific soils for crop production can be analyzed using soilassessment, including soil fertility classes, soil erosion models and soil salinity classification. Suitability requirements are for crops grouped into cereal crops, nuts, legumes, fruits, vegetables, industrial crops, and root crops. mpspline2 implements a mass-preserving spline to soil attributes to make continuous down-profile estimates of attributes measured over discrete, often discontinuous depth intervals.\nWeed science The package drc offers versatile model fitting and after-fitting functions for dose-response curves. LW1949 implements the Litchefield and Wilcoxon (1949) dose-response analysis. PROSPER is a package for simulating weed population dynamics at the individual and population level under a range of conditions including herbicide resistance and herbicide pressure. For ecological studies and analytical applications, the CRAN task view for Environmetrics provides a list of existing R resources in this topic.\nAdditional links  InkaVerse: a collection of shiny apps and an R package (inti) to support field trials analyses for trials managed in FieldBook.  Relevant R-Forge Projects (not otherwise listed in this resource)  Global Soil Information Facilities: an initiative of the World Soil Information Institute and Africa Soil Information Service (ISRIC) team. Soil Spectroscopy and Chemometrics: methods for soil spectroscopy and chemometrics involving spectral data conversion, calibration and prediction methods. Digital Soil Informatic Tools: quantitative and numerical analysis of soils and phenomena and other information related to soils, with a focus on pedometrics. Reclim: a collection of weather reduction functions of kinetics for the introductory carbon balance model (ICBM), a compartmental soil organic carbon model. inspectr: handling and analysis of Vis-NIR spectra, with a focus on soil science applications. Crop \u0026amp; Disease Modelling: dynamic \u0026amp; mechanistic simulation of crop growth and development, and of plant diseases. biocro: functions and data for plant and crop modeling. Simulates C4 photosynthesis, biomass production and estimates parameters for plant physiological models using optimization techniques and Markov chains. Crop Climate Change Course: materials for the course \u0026ldquo;Analyzing the impact of climate change on crops and varieties\u0026rdquo;   ","date":1652400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652400000,"objectID":"fc20067a9011f2b776cfd94bed866532","permalink":"/post/keeping-up-with-r/","publishdate":"2022-05-13T00:00:00Z","relpermalink":"/post/keeping-up-with-r/","section":"post","summary":"Last updated: July 29, 2022\nAgriculture encompasses a broad breadth of disciplines. Many, many package in base R and contributed packages are useful to agricultural researchers. For that reason, this is not exhaustive list of packages useful to agricultural research.","tags":["R","CRAN","agriculture"],"title":"R packages for Agricultural Research","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"What do you do when you need to solve a problem in R? If you have been programming for any amount of time, you have learned that you are likely to encounter errors programming in R and resolving those coding errors can be challenging.\nFor most everyone, goggling an error message is the first step. This is not a bad choice as it often leads you to people who have already encountered the error and solved it. But, it can also lead you to a labyrinth of different, possible conflicting, possibly incomprehensible solutions. Additionally, it is not always the most time efficient method to solve a coding problem.\nSome Alternatives Check the documentation! There\u0026rsquo;s an ancient proverb about the importance of reading documentation: \u0026ldquo;you can spend 2 hours searching the web in order to save 15 minutes of reading the documentation.\u0026rdquo;\nPackage and function documentation can be very helpful. There are two main aspects of documentation:\n Function reference: this describes the arguments a function can take, the expected format for those arguments and information about the function object. It may also contain theoretical details that are needed to understand the argument options and examples. Documentation varies in quality greatly across R packages. It can occasionally be too bare bones to be useful, but often (especially in base R commands and tidyverse packages) the documentation is very detailed and helpful. Submission to CRAN requires that packages have a documentation file that lists all package function documentation in alphabetical order. Vignettes: these are tutorials accompanying how to use a package functions. These usually cover a subset of functions and include text explanations. They are basically long examples. Vignettes can be enormously helpful. They are not required for submission to CRAN, so they are not always available, especially for older legacy packages. You can find these on the package website (if it exists) or its CRAN link. Here are some vignettes from the package {tidyr}.  How do we find documentation? You find function documentation via the R console:\n?par # does an exact search on \u0026quot;par\u0026quot; ??plot # does a fuzzy match on \u0026quot;plot\u0026quot;  This will open help files for those items.\nSometimes, you will discover there are multiple options and possibly different help files associated with a function name (just run methods('mean') or methods('anova') to see what I mean).\nThese are functions that act differently depending on the R object type they are called to interact with (e.g. mean(some_dates) will behave differently than mean(some_numbers)). Which leads to the next point: all R objects have a class assigned to them. You can check this with class().\ny \u0026lt;- rnorm(20); x \u0026lt;- y + rnorm(20) m \u0026lt;- lm(y ~ x) class(y)  ## [1] \u0026quot;numeric\u0026quot;  class(m)  ## [1] \u0026quot;lm\u0026quot;  Understanding this difference between functions will help you understand which documentation files will help you solve your problem.\nMore importantly, once you know the object class, you can search for methods that exist for that class.\nmethods(class = \u0026quot;lm\u0026quot;)  ## [1] add1 alias anova case.names coerce ## [6] confint cooks.distance deviance dfbeta dfbetas ## [11] drop1 dummy.coef effects extractAIC family ## [16] formula hatvalues influence initialize kappa ## [21] labels logLik model.frame model.matrix nobs ## [26] plot predict print proj qr ## [31] residuals rstandard rstudent show simulate ## [36] slotsFromS3 summary variable.names vcov ## see '?methods' for accessing help and source code  From this, we can see a special plot() option exists (that provides several diagnostic plots), functions for extracting residuals (rstudent(), residuals()), a version of anova() written for object type \u0026ldquo;lm\u0026rdquo;, and much much more.\nRead your error messages Error messages can be obtuse and confusing (especially if you are new to programming). We have all have experienced (and will experience again) this error message:\nIt is telling us we are trying to subset (extract) information from a \u0026lsquo;closure\u0026rsquo; (which is a function). In essence, there was an attempt to do something like mean$myvar when mean() is a function, not a data.frame.\nHowever, error messages can also be trying to tell you something important. Here\u0026rsquo;s a recent experience of mine:\nOver time, these messages will become comprehensible. It\u0026rsquo;s still the same messages, but your R knowledge will help you understand them. But, becoming fluent in R error messages implies reading them and trying to understand them.\nSpecific places to ask for help Eventually, you may need to search forums or ask for help from kind strangers. If Google fails you, here are some other useful resources:\n  RStudio Community, a helpful forum that is only for R questions. It is run by RStudio and moderated (to an extent). This is one of the more useful sites to search or post on.   R4DS community, a friendly, welcoming community. Join their slack channel and ask a question.\n  Stack overflow, the long-established site of all questions programming. Can often be helpful.   Consider reading the source code This is best for advanced users, but it can hep you resolve very specific questions about a function. Reading source code will also help improve your own coding.\n** How to find source code: **\n Type the function name in the console without parentheses:  mean.default  ## function (x, trim = 0, na.rm = FALSE, ...) ## { ## if (!is.numeric(x) \u0026amp;\u0026amp; !is.complex(x) \u0026amp;\u0026amp; !is.logical(x)) { ## warning(\u0026quot;argument is not numeric or logical: returning NA\u0026quot;) ## return(NA_real_) ## } ## if (na.rm) ## x \u0026lt;- x[!is.na(x)] ## if (!is.numeric(trim) || length(trim) != 1L) ## stop(\u0026quot;'trim' must be numeric of length one\u0026quot;) ## n \u0026lt;- length(x) ## if (trim \u0026gt; 0 \u0026amp;\u0026amp; n) { ## if (is.complex(x)) ## stop(\u0026quot;trimmed means are not defined for complex data\u0026quot;) ## if (anyNA(x)) ## return(NA_real_) ## if (trim \u0026gt;= 0.5) ## return(stats::median(x, na.rm = FALSE)) ## lo \u0026lt;- floor(n * trim) + 1 ## hi \u0026lt;- n + 1 - lo ## x \u0026lt;- sort.int(x, partial = unique(c(lo, hi)))[lo:hi] ## } ## .Internal(mean(x)) ## } ## \u0026lt;bytecode: 0x7fa5749f17f8\u0026gt; ## \u0026lt;environment: namespace:base\u0026gt;   Sometimes this is not informative  c  ## function (...) .Primitive(\u0026quot;c\u0026quot;)  subset  ## function (x, ...) ## UseMethod(\u0026quot;subset\u0026quot;) ## \u0026lt;bytecode: 0x7fa573afcb48\u0026gt; ## \u0026lt;environment: namespace:base\u0026gt;  `[`  ## .Primitive(\u0026quot;[\u0026quot;)   Use {the package {lookup} to find what you need  remotes::install_github(\u0026quot;jimhester/lookup\u0026quot;) lookup::lookup(`[`)  {lookup} checks CRAN, Bioconductor and GitHub for source code! Prior to {lookup}, finding source code for R functions was challenging. Please note that this \u0026ldquo;lookup\u0026rdquo; is NOT the same same package called \u0026ldquo;lookup\u0026rdquo; found on CRAN. They have zero overlapping functionality.\n","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"d1ea94b9c1f54c54087ce03fd5215fab","permalink":"/post/help-in-r/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/post/help-in-r/","section":"post","summary":"How to find help when you run into trouble using R","tags":["R","help","documentation"],"title":"Finding Help in R","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"You may find yourself needing to do something repeatedly in R. Sure, you can cut-and-paste and change that one thing, or two things, or five things, but this quickly becomes cumbersome. The result can be a very long R file and the likelihood of making a mistake that you don\u0026rsquo;t notice increases (e.g. forgetting to change a variable or an argument).\nThere is the general rule of DRY: don\u0026rsquo;t repeat yourself. In practice, if something has to pasted more than twice, then consider writing a function to accomplish that aim instead.\nIntroduction to Writing Functions R functions follow a general structure:\nmy_function_name \u0026lt;- function(argument1, argument2) { final_output \u0026lt;- action(argument1, argument2) return(final_output) }  A classic function example is conversion of temperature from Fahrenheit to celsius:\nfahr_to_cel \u0026lt;- function(fahr) { # function that converts temperature in degrees Fahrenheit to celsius # input: fahr: numeric value representing temp in degrees fahrenheit # output: kelvin: numeric converted temp in celsius celsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius) }  This function takes a numeric value, temperature in Fahrenheit, and outputs another numeric value, that same value converted to celsius.\nFunction usage:\nfahr_to_cel(80)  ## [1] 26.66667  This function can be called for a large number of values at once:\n# create a vector of 100 numbers randomly sampled between 1 and 100. x1 \u0026lt;- sample(1:100, 100, replace = TRUE) x2 \u0026lt;- fahr_to_cel(x1)  If you provide the incorrect type of data, the function will not work:\nfahr_to_cel(\u0026quot;thirty\u0026quot;)  ## Error in fahr - 32: non-numeric argument to binary operator  A More Complex Example Often we want to do something more complicated. One thing I want to do frequently is build boxplots.\nFirst, simulate some data. This data set has two categorical variables, cat1 and cat2, and 4 different continuous variables generated through data simulation.\nmydata \u0026lt;- data.frame(cat1 = rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;D\u0026quot;), 10), cat2 = rep(c(\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;), each = 20), var1 = rnorm(40), var2 = runif(40), var3 = rlnorm(40), var4 = rbeta(40, 1, 5))  Next, write up an example of what you want to do. In this example, let\u0026rsquo;s create a boxplot:\nboxplot(var1 ~ cat1, data = mydata, main = NA, col = \u0026quot;orangered\u0026quot;)  Now, let\u0026rsquo;s put that in a function. Start with the basic function framework:\nboxplot_func = function() { }  Next, insert the function code. Start by cut-and-pasting the original boxplot command ran above:\nboxplot_func = function() { boxplot(var1 ~ cat1, data = mydata, main = NA, col = \u0026quot;orangered\u0026quot;) }  Decide on arguments you want to control and put that inside the function() parentheses. Probably the independent and dependent variable (x and y, respectively), as well as the data frame needed.\nPut those arguments inside function().\nboxplot_func = function(df, x, y) { boxplot(var1 ~ cat1, data = mydata, main = NA, col = \u0026quot;orangered\u0026quot;) }  Then indicate where those arguments are used in the function. They must be used in the function (otherwise, why have them?).\nboxplot_func = function(df, x, y) { boxplot(y ~ x, data = df, main = NA, col = \u0026quot;orangered\u0026quot;) }  However, if you try to use this function, it won\u0026rsquo;t work. The argument y ~ x is a special class of object in R called \u0026ldquo;formula\u0026rdquo; and the formatting and object type must match. Formulas are used widely in R for linear modelling and follow the exact same convention:\ny ~ x  Note that the information on either side of ~ can become more complicated. (but not in this function).\nSo, create a formula object using the functions formula() and paste() within the function and insert that into the basic boxplot code. If you don\u0026rsquo;t know how to use those function, type ?formula and ?paste into the console to learn more about them.\nboxplot_func = function(df, x, y) { f = formula(paste(y, \u0026quot;~\u0026quot;, x)) boxplot(f, data = df, main = NA, col = \u0026quot;orangered\u0026quot;) }  What if you want the ability to change the color? Insert a new argument and replace it in the function body:\nboxplot_func = function(x, y, color) { f = formula(paste(y, \u0026quot;~\u0026quot;, x)) boxplot(f, data = df, main = NA, col = color) }  If you want the option to set the some options or if you choose not to, have the function choose values automatically as defaults, that can be done by naming the argument in formula().\nboxplot_func = function(df = mydata, x, y, color = \u0026quot;springgreen\u0026quot;) { f = formula(paste(y, \u0026quot;~\u0026quot;, x)) boxplot(f, data = df, main = NA, col = color) }  Next step is to run the function as it is currently written (highlight the function code and click run). Next, make sure you add this function (i.e. boxplot_funct = function(...)) to your R environment by running it in the console. You can check it exists in your R global environment as thus:\nls()  ## [1] \u0026quot;boxplot_func\u0026quot; \u0026quot;fahr_to_cel\u0026quot; \u0026quot;mydata\u0026quot; \u0026quot;x1\u0026quot; \u0026quot;x2\u0026quot;  Now, call the function and make sure it does what we want?\nboxplot_func(mydata, \u0026quot;cat1\u0026quot;, \u0026quot;var1\u0026quot;)  boxplot_func(x = \u0026quot;cat2\u0026quot;, y = \u0026quot;var1\u0026quot;, col = \u0026quot;darkcyan\u0026quot;)  boxplot_func(mydata, \u0026quot;cat2\u0026quot;, \u0026quot;var4\u0026quot;, col = \u0026quot;khaki\u0026quot;)  What if it doesn\u0026rsquo;t do what we want? What if you get strange output? No output? Or strange error messages? Herein comes the world of debugging (another blog post for another day).\nError Checking and Error Messages You may have noticed earlier this strange error message:\nfahr_to_cel(\u0026quot;thirty\u0026quot;)  ## Error in fahr - 32: non-numeric argument to binary operator  This is a very confusing message. We most certainly provided a \u0026ldquo;non-numeric argument\u0026rdquo;, but what is a \u0026ldquo;binary operator\u0026rdquo;? Turns out that is a programming speak for a standard mathematical operations addition, subtraction, multiplication and division (called \u0026lsquo;binary\u0026rsquo; because they take two inputs). Still, we are likely to encounter more strange error messages written in programmer speak that confuse us or someone else using our functions. We can write custom error messages that are produced when certain errors occur.\nHere is the temperature conversion function again:\nfahr_to_cel \u0026lt;- function(fahr) { celsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius) }  Since they can only take numeric argument, maybe we can start for checking for this? There are a few options in do this. One of the easiest to use is stopifnot(). This functions takes the general form: stopifnot(\u0026quot;my custom error message\u0026quot; = test). What constitutes a \u0026lsquo;test\u0026rsquo; is an R expression that returns a TRUE or FALSE value after being evaluated. Examples of this are is.character(x), is.NA(x), x \u0026gt; 0 and so on. For each of these statements, the expectation is that R will true a TRUE or FALSE. If the test does not do this reliably (e.g. you may not be able to evaluate x \u0026gt; 0 if x is non-numeric), then a different test is needed.\nIn our case, we can use is.numeric().\nfahr_to_cel \u0026lt;- function(fahr) { stopifnot(\u0026quot;input is not numeric\u0026quot; = is.numeric(fahr)) celsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius) }  Let\u0026rsquo;s run some test cases:\nfahr_to_cel(30)  ## [1] -1.111111  fahr_to_cel(\u0026quot;thirty\u0026quot;)  ## Error in fahr_to_cel(\u0026quot;thirty\u0026quot;): input is not numeric  As expected, the first one worked and the second generated an error message.\nNaturally, this is a very trivial example, but if you write more complicated functions with the intent of them automatically accomplishing a goal for you, these error messages can be helpful.\nFunctions and Tidy Evaluation If you\u0026rsquo;ve worked with the tidyverse, you know it handles input a bit differently. In summary, quotes are used far less often. This makes writing function quite challening at times and required the use of the double curly braces, {{}} or the \u0026ldquo;bang-bang\u0026rdquo; operator !!.\nWhat if we wanted to do a boxplot function using ggplot?\nHere\u0026rsquo;s what the code would look like:\nlibrary(ggplot2) mydata \u0026lt;- data.frame(cat = rep(c(\u0026quot;AA\u0026quot;, \u0026quot;BB\u0026quot;), each = 50), obs = c(rnorm(50), runif(50))) ggplot(mydata, aes(x = cat, y = obs)) + geom_boxplot(aes(fill = cat), alpha = 0.5) + geom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + theme_classic()  But, if you try to write a function following the usual rules, it won\u0026rsquo;t work properly:\ngboxplot_func \u0026lt;- function(x1, y1) { ggplot(mydata, aes(x = x1, y = y1)) + geom_boxplot(aes(fill = x1), alpha = 0.5) + geom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + theme_classic() }  gboxplot_func(cat, obs)  ## Error in FUN(X[[i]], ...): object 'obs' not found  This one works, but the results are crazy.\ngboxplot_func(\u0026quot;cat\u0026quot;, \u0026quot;obs\u0026quot;)  Why are the results wonky? Because while this function see \u0026ldquo;mydata\u0026rdquo; has 100 observations, it cannot connect \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;obs\u0026rdquo; to the data frame.\nThis is where the special operators come in:\ngboxplot_func2 \u0026lt;- function(x1, y1) { ggplot(mydata, aes(x = {{x1}}, y = {{y1}})) + geom_boxplot(aes(fill = {{x1}}), alpha = 0.5) + geom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + theme_classic() }  gboxplot_func2(cat, obs)  The curly braces enable us to insert unquoted tidy variables and use ggplot.\nWhat if you have multiple options to specify in a single arguments? You can use the ... notation (in the final argument):\nvar_sum_funct(storms, day, name)[1:5,] # one grouping factor  ## # A tibble: 5 × 3 ## name Mean SD ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AL011993 8.75 13.7 ## 2 AL012000 7.75 0.5 ## 3 AL021992 25.6 0.548 ## 4 AL021994 20.3 0.516 ## 5 AL021999 2.75 0.5  var_sum_funct(storms, day, name, status)[1:5,] # many grouping factors  ## `summarise()` has grouped output by 'name'. You can override using the `.groups` argument.  ## # A tibble: 5 × 4 ## # Groups: name [5] ## name status Mean SD ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 AL011993 tropical depression 8.75 13.7 ## 2 AL012000 tropical depression 7.75 0.5 ## 3 AL021992 tropical depression 25.6 0.548 ## 4 AL021994 tropical depression 20.3 0.516 ## 5 AL021999 tropical depression 2.75 0.5  var_sum_funct(storms, day) # unusual example!  ## # A tibble: 1 × 2 ## Mean SD ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 15.9 9.01  This is a very brief introduction to tidy evaluation. More information on tidy evaluation is available for ggplot and dplyr.\n","date":1644278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"0e7a6a1fc49509cf3c1a0bdee74c6768","permalink":"/post/writing-r-functions/","publishdate":"2022-02-08T00:00:00Z","relpermalink":"/post/writing-r-functions/","section":"post","summary":"You may find yourself needing to do something repeatedly in R. Sure, you can cut-and-paste and change that one thing, or two things, or five things, but this quickly becomes cumbersome.","tags":["functions"],"title":"How to Write Custom Functions in R","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"Introduction ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility. The result is that it is easy to misspecify a model or make other mistakes. The information below is intended to serve as a guide through the R ANOVA wilderness.\nPackages Needed There are many packages to load. Here is a (very) brief summary of what each package does.\n   Package Purpose     car Anova() function to extract type III \u0026amp; II sums of squares   lme4 mixed models   nlme mixed models, non-linear models, alternative covariance structures   emmeans for extracting least squares means and contrasts   lmer test improved summary functions of lmer objects   dplyr data organization   forcats for managing categorical data   agridat has many agricultural data sets   agricolae has options for many common agricultural experimental designs    library(car) library(lme4) library(nlme) library(emmeans) #in older version of R, you may need to install \u0026quot;multcompView\u0026quot; separately to access full functionality of the emmeans package library(lmerTest) library(dplyr) library(forcats) library(agridat)  Formula Notation There are some consistent features across ANOVA methods in R. Formula notation is often used in the R syntax for ANOVA functions. It looks like this: $Y ~ X, where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments).\nmy_formula \u0026lt;- formula(Y ~ treatment1 + treatment2) class(my_formula)  ## [1] \u0026quot;formula\u0026quot;  my_formula  ## Y ~ treatment1 + treatment2  Often the independent variables (i,e, the treatments or the x variables) are expected to be factors, another type of R object:\nmy_var \u0026lt;- c(rep(\u0026quot;low\u0026quot;,5), rep(\u0026quot;high\u0026quot;, 5)) class(my_var) #check what variable type it is  ## [1] \u0026quot;character\u0026quot;  Although \u0026ldquo;my_var\u0026rdquo; is not type factor, it is type \u0026ldquo;character\u0026rdquo; which is automatically converted to a factor in lm(), lmer(), lme() and many other linear modeling functions. There are some packages that do not follow this convention, so it\u0026rsquo;s helpful to read function documentation, especially if you get unexpected results.\nVariables like year, which are often imported as a number or integer, do need to be converted to a factor or a character variable prior to analysis. Otherwise, they will be interpreted as a number in linear modelling and treated as a covariate, e.g, 2020 would be 2,020. Here is one way to do this conversion:\nmy_factor \u0026lt;- as.character(my_var) # convert to a character class(my_factor) # check variable type to confirm  ## [1] \u0026quot;character\u0026quot;  my_factor \u0026lt;- as.factor(my_var) # convert to a factor class(my_factor) # check variable type again to confirm  ## [1] \u0026quot;factor\u0026quot;  The choice of whether to convert a categorical variable to a character or factor depends on the comfort of the user with these structures and package requirements.\nSometimes, there is a need to alter the order of treatment levels (that is, how R sees those levels). The default behavior of R is to order categorical levels alphanumerically. However, sometimes there are reasons you may not want this (for example, you want to set a particular reference level as the first factor level).\nBelow is one example of how to reorder factor levels in a variable. The first step is to see which levels are present in the variable and how they are ordered:\nlevels(my_factor)  ## [1] \u0026quot;high\u0026quot; \u0026quot;low\u0026quot;  Once that is known, you can use that information to manually set the levels and their order. Note that spelling of each level much match what is actually present in the variable. Unmatched levels in the variable will be set to NA automatically by R in the following step.\nmy_factor \u0026lt;- factor(my_factor, levels = c(\u0026quot;low\u0026quot;, \u0026quot;high\u0026quot;)) levels(my_factor) # check the new ordering  ## [1] \u0026quot;low\u0026quot; \u0026quot;high\u0026quot;  Knowing the level order is important because in the implementation of ANOVA in R, the first level is treated as the reference level. Manipulating factors is a challenging task in R. The package forcats contains a collection of accessory functions for managing factors (\u0026ldquo;forcats\u0026rdquo; = for categories). The tutorial uses the forcats function fct_drop().\nMore on formulas:\nThe formula first shown, Y ~ treatment1 + treatment2, includes main effects only. Other formula notation includes the symbols : and *, indicating notation for interaction only and main effects plus the interaction term, respectively.\nformula(Y ~ treatment1:treatment2) # interaction only  ## Y ~ treatment1:treatment2  formula(Y ~ treatment1*treatment2) # interaction plus main effects  ## Y ~ treatment1 * treatment2  These two formulas are equivalent:\nformula(Y ~ treatment1 + treatment2 + treatment1:treatment2) formula(Y ~ treatment1*treatment2)  Perhaps you can see from these examples that formulas are a really just a collections of characters (that is, a string) and exist independent of any data set. Later, we will need to link these formulas to a data set to actually construct a linear model and conduct statistical analysis.\nANOVA for fixed effects models Here is a function for reporting the number of missing data in each column. There are other ways to do this, but I find this function easy enough to write and use.\ncount_na \u0026lt;- function(df) { apply(df, 2, function(x) sum(is.na(x))) }  Completely Randomised design First, load the data set \u0026ldquo;warpbreaks\u0026rdquo; (a data set from base R). This is an old data set with variables for wool type (A and B) and tension on the loom (L, M or H). The response variable is \u0026ldquo;breaks\u0026rdquo;, the number of times the wool thread breaks on industrial looms.\nI always like to have a quick look at the data before running any statistical tests. So, here we go:\ndata(warpbreaks) count_na(warpbreaks)  ## breaks wool tension ## 0 0 0  str(warpbreaks)  ## 'data.frame':\t54 obs. of 3 variables: ## $ breaks : num 26 30 54 25 70 52 51 26 67 18 ... ## $ wool : Factor w/ 2 levels \u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tension: Factor w/ 3 levels \u0026quot;L\u0026quot;,\u0026quot;M\u0026quot;,\u0026quot;H\u0026quot;: 1 1 1 1 1 1 1 1 1 2 ...  warpbreaks$wool \u0026lt;- factor(warpbreaks$wool, levels = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;)) table(warpbreaks$wool, warpbreaks$tension)  ## ## L M H ## A 9 9 9 ## B 9 9 9 ## C 0 0 0  hist(warpbreaks$breaks, col = \u0026quot;gold\u0026quot;)  boxplot(breaks ~ wool, data = warpbreaks, col = \u0026quot;orangered\u0026quot;)  boxplot(breaks ~ tension, data = warpbreaks, col = \u0026quot;chartreuse\u0026quot;) #why not have colorful plots?  This data set has 2 treatments. We don\u0026rsquo;t know if there is an interaction between the variables, yet. A good start is to run a linear model using lm() function, the linear regression function. As a reminder, ANOVA is a special case of the linear regression model where the predictors (the experimental treatments) are categories rather than a continuous variable.\n# run standard linear model for main effects only lm.mod1 \u0026lt;- lm(breaks ~ wool + tension, data = warpbreaks) # extract type III sums of squares from that model Anova(lm.mod1, type = \u0026quot;3\u0026quot;)  ## Anova Table (Type III tests) ## ## Response: breaks ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 20827.0 1 154.3226 \u0026lt; 2.2e-16 *** ## wool 450.7 1 3.3393 0.073614 . ## tension 2034.3 2 7.5367 0.001378 ** ## Residuals 6747.9 50 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # run a linear model with main effects and interactions lm.mod2 \u0026lt;- lm(breaks ~ wool*tension, data = warpbreaks) # ...and type III sums of squares Anova(lm.mod2, type = \u0026quot;III\u0026quot;)  ## Anova Table (Type III tests) ## ## Response: breaks ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 17866.8 1 149.2757 2.426e-16 *** ## wool 1200.5 1 10.0301 0.0026768 ** ## tension 2468.5 2 10.3121 0.0001881 *** ## wool:tension 1002.8 2 4.1891 0.0210442 * ## Residuals 5745.1 48 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  FYI\nfunctions only shown as an example and not actually run.\n# this function runs type II sums of squares: Anova(lm.mod2, type = \u0026quot;II\u0026quot;) # this function runs type I sums of squares: anova(lm.mod2)  A few comments on types of sums of squares: \nAs a reminder, the type of sums of squares used in statistical tests can impact the results and subsequent interpretation. Type I, sums of squares tests for statistical significance by adding one variable to the model at time (and hence is also called \u0026ldquo;sequential\u0026rdquo;). If there is any unbalance in the treatments, the type I sums of squares are dependent on the order variables are added to the model and hence is often not the best choice for many agricultural experiment. Type III sums of squares (also called \u0026ldquo;partial\u0026rdquo; or \u0026ldquo;marginal\u0026rdquo;) evaluates the statistical significance of variable or interaction, assuming that the other variables are in the model. This is a decent default option. The last option is Type II sums of squares, which is the best option when you are sure there are no interactions between variables. If there is complete balance among treatments (each treatment is observed the same number of times with no missing data), then there is no need to concern yourself with these different types of sums of squares.\nCompare Models # conduct an F test comparing the models anova(lm.mod1, lm.mod2)  ## Analysis of Variance Table ## ## Model 1: breaks ~ wool + tension ## Model 2: breaks ~ wool * tension ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 50 6747.9 ## 2 48 5745.1 2 1002.8 4.1891 0.02104 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # also, consider doing a stepwise approach for finding the best model: step(lm.mod2)  ## Start: AIC=264.02 ## breaks ~ wool * tension ## ## Df Sum of Sq RSS AIC ## \u0026lt;none\u0026gt; 5745.1 264.02 ## - wool:tension 2 1002.8 6747.9 268.71  ## ## Call: ## lm(formula = breaks ~ wool * tension, data = warpbreaks) ## ## Coefficients: ## (Intercept) woolB tensionM tensionH woolB:tensionM ## 44.56 -16.33 -20.56 -20.00 21.11 ## woolB:tensionH ## 10.56  Model diagnostics plot(lm.mod2) #this will produce 4 plots of residuals  shapiro.test(resid(lm.mod2)) #standard shapiro-wilk test.  ## ## Shapiro-Wilk normality test ## ## data: resid(lm.mod2) ## W = 0.98686, p-value = 0.8162  # this variable could be analyzed with a log-normal model instead  Least squares means \u0026amp; contrasts The emmeans package is a flexible package for extracting the estimated marginal means (in SAS, the \u0026ldquo;least squares means\u0026rdquo;) from different linear models. It is compatible with a large number of R linear modelling packages.\nHere is some code for extracting the marginal means and conducting contrasts.\n# extract least squares means for 'tension' (lsm \u0026lt;- emmeans(lm.mod2, ~ tension))  ## NOTE: Results may be misleading due to involvement in interactions  ## tension emmean SE df lower.CL upper.CL ## L 36.4 2.58 48 31.2 41.6 ## M 26.4 2.58 48 21.2 31.6 ## H 21.7 2.58 48 16.5 26.9 ## ## Results are averaged over the levels of: wool ## Confidence level used: 0.95  emmeans(lm.mod2, \u0026quot;wool\u0026quot;)  ## NOTE: Results may be misleading due to involvement in interactions  ## wool emmean SE df lower.CL upper.CL ## A 31.0 2.11 48 26.8 35.3 ## B 25.3 2.11 48 21.0 29.5 ## ## Results are averaged over the levels of: tension ## Confidence level used: 0.95  All pairwise comparisons within each level of tension:\ncontrast(lsm, \u0026quot;pairwise\u0026quot;)  ## contrast estimate SE df t.ratio p.value ## L - M 10.00 3.65 48 2.742 0.0229 ## L - H 14.72 3.65 48 4.037 0.0006 ## M - H 4.72 3.65 48 1.295 0.4049 ## ## Results are averaged over the levels of: wool ## P value adjustment: tukey method for comparing a family of 3 estimates  Conduct custom contrasts comparing \u0026lsquo;Low\u0026rsquo; tension versus \u0026lsquo;Medium\u0026rsquo; and \u0026lsquo;High\u0026rsquo; and \u0026lsquo;High\u0026rsquo; versus \u0026lsquo;Medium\u0026rsquo; and \u0026lsquo;Low\u0026rsquo;.\n# see the order of each level in a factor levels(warpbreaks$tension)  ## [1] \u0026quot;L\u0026quot; \u0026quot;M\u0026quot; \u0026quot;H\u0026quot;  # construct a list of constructs # each item must be same length as the the number of levels present in the variable tension # use numbers and fracions to indicate the contrasting levels # the numbers must sum to zero cList \u0026lt;- list(LvMH = c(1, -0.5, -0.5), # low vs high + medium HvLM = c(0.5, 0.5, -1)) # high vs low + medium # check that each contrast sums to zero lapply(cList, sum)  ## $LvMH ## [1] 0 ## ## $HvLM ## [1] 0  # perform custom contrast and include a Bonferroni adjustment summary(contrast(lsm, cList, adjust = \u0026quot;bonferroni\u0026quot;))  ## contrast estimate SE df t.ratio p.value ## LvMH 12.36 3.16 48 3.914 0.0006 ## HvLM 9.72 3.16 48 3.078 0.0069 ## ## Results are averaged over the levels of: wool ## P value adjustment: bonferroni method for 2 tests  Randomised Complete Block Design (RCBD) - fixed effects model This example uses rapeseed yield data from multiple locations, years and cultivars. Within a single location or year, the replication is often balanced.\nLoad Data and examine:\ndata(shafii.rapeseed) # from the 'agridat' package rapeseed1987 \u0026lt;- shafii.rapeseed %\u0026gt;% filter(year == 87) %\u0026gt;% mutate(block = fct_drop(rep), Cv = fct_drop(gen), loc = fct_drop(loc)) str(rapeseed1987)  ## 'data.frame':\t216 obs. of 7 variables: ## $ year : int 87 87 87 87 87 87 87 87 87 87 ... ## $ loc : Factor w/ 9 levels \u0026quot;GGA\u0026quot;,\u0026quot;ID\u0026quot;,\u0026quot;MT\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ rep : Factor w/ 4 levels \u0026quot;R1\u0026quot;,\u0026quot;R2\u0026quot;,\u0026quot;R3\u0026quot;,..: 1 2 3 4 1 2 3 4 1 2 ... ## $ gen : Factor w/ 6 levels \u0026quot;Bienvenu\u0026quot;,\u0026quot;Bridger\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ yield: num 961 1329 1781 1698 1605 ... ## $ block: Factor w/ 4 levels \u0026quot;R1\u0026quot;,\u0026quot;R2\u0026quot;,\u0026quot;R3\u0026quot;,..: 1 2 3 4 1 2 3 4 1 2 ... ## $ Cv : Factor w/ 6 levels \u0026quot;Bienvenu\u0026quot;,\u0026quot;Bridger\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ...  count_na(rapeseed1987)  ## year loc rep gen yield block Cv ## 0 0 0 0 0 0 0  table(rapeseed1987$Cv, rapeseed1987$loc) #experiment has 1 rep per block  ## ## GGA ID MT NC OR SC TGA TX WA ## Bienvenu 4 4 4 4 4 4 4 4 4 ## Bridger 4 4 4 4 4 4 4 4 4 ## Cascade 4 4 4 4 4 4 4 4 4 ## Dwarf 4 4 4 4 4 4 4 4 4 ## Glacier 4 4 4 4 4 4 4 4 4 ## Jet 4 4 4 4 4 4 4 4 4  hist(rapeseed1987$yield, col = \u0026quot;gold\u0026quot;)  boxplot(yield ~ Cv, data = rapeseed1987, col = \u0026quot;orangered\u0026quot;)  boxplot(yield ~ loc, data = rapeseed1987, col = \u0026quot;chartreuse\u0026quot;)  Analyse experiment:\n# for this example, the analysis will only be done for a single year # block is nested within location # if each block had a unique name, 'Error(block)' would suffce shaf.aov \u0026lt;- aov(yield ~ Cv*loc + Error(block), data = rapeseed1987) summary(shaf.aov)  ## ## Error: block ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Residuals 3 336565 112188 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Cv 5 3203992 640798 2.645 0.025111 * ## loc 8 318197192 39774649 164.165 \u0026lt; 2e-16 *** ## Cv:loc 40 22707425 567686 2.343 0.000103 *** ## Residuals 159 38523267 242285 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  emmeans(shaf.aov, ~ Cv | loc)  ## Note: re-fitting model with sum-to-zero contrasts  ## loc = GGA: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1442 245 161 959 1926 ## Bridger 1363 245 161 880 1847 ## Cascade 1505 245 161 1021 1988 ## Dwarf 1295 245 161 811 1779 ## Glacier 1681 245 161 1197 2164 ## Jet 1091 245 161 607 1575 ## ## loc = ID: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1242 245 161 759 1726 ## Bridger 947 245 161 463 1430 ## Cascade 773 245 161 290 1257 ## Dwarf 932 245 161 448 1415 ## Glacier 1111 245 161 627 1595 ## Jet 1064 245 161 580 1548 ## ## loc = MT: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 2616 245 161 2132 3100 ## Bridger 2828 245 161 2345 3312 ## Cascade 2916 245 161 2433 3400 ## Dwarf 3452 245 161 2968 3935 ## Glacier 3307 245 161 2823 3790 ## Jet 3660 245 161 3177 4144 ## ## loc = NC: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1001 245 161 517 1485 ## Bridger 1064 245 161 581 1548 ## Cascade 745 245 161 262 1229 ## Dwarf 1014 245 161 530 1497 ## Glacier 1229 245 161 746 1713 ## Jet 1674 245 161 1190 2157 ## ## loc = OR: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 4556 245 161 4072 5039 ## Bridger 2530 245 161 2046 3013 ## Cascade 3336 245 161 2852 3819 ## Dwarf 3932 245 161 3448 4415 ## Glacier 4185 245 161 3702 4669 ## Jet 3220 245 161 2736 3703 ## ## loc = SC: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 2500 245 161 2016 2983 ## Bridger 2705 245 161 2221 3189 ## Cascade 2119 245 161 1635 2602 ## Dwarf 1894 245 161 1410 2377 ## Glacier 2717 245 161 2234 3201 ## Jet 2833 245 161 2349 3316 ## ## loc = TGA: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1258 245 161 774 1741 ## Bridger 1868 245 161 1384 2351 ## Cascade 1708 245 161 1224 2191 ## Dwarf 873 245 161 389 1356 ## Glacier 1453 245 161 970 1937 ## Jet 954 245 161 470 1438 ## ## loc = TX: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 838 245 161 354 1322 ## Bridger 1069 245 161 585 1553 ## Cascade 735 245 161 251 1218 ## Dwarf 988 245 161 505 1472 ## Glacier 952 245 161 468 1435 ## Jet 1408 245 161 925 1892 ## ## loc = WA: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 4375 245 161 3891 4859 ## Bridger 4604 245 161 4120 5087 ## Cascade 4464 245 161 3981 4948 ## Dwarf 3974 245 161 3490 4458 ## Glacier 4740 245 161 4256 5224 ## Jet 4344 245 161 3861 4828 ## ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95  ANOVA for mixed models (models with random and fixed effects)\nRandom effects are often those treatments levels drawn from a large population of possible treatment levels and there is interest in understanding the distribution and variance of that population. This in contrast to fixed effects, where the inferences are restricted to the treatment levels tested.\nBlocking factors and Year are often considered random factors because a researcher is not interested in particular years or a blocking factor. When there is unbalanced replication, the variance components should be estimated with maximum likelihood or REML, which implies use of the packages \u0026ldquo;lmer\u0026rdquo; and/or \u0026ldquo;nlme\u0026rdquo;.\nRandomised Complete Block Design (RCBD) - mixed effects The \u0026ldquo;shafii.rapeseed\u0026rdquo; data set will be used for this section.\nAnalyse experiment using a mixed model:\nThis uses the function lme() from the package \u0026ldquo;nlme\u0026rdquo;. Functionally, it is very similar to calling lme4::lmer(). The degrees of freedom are different (lmer() is using Satterthwaite\u0026rsquo;s approximation), but the p-values are the same.\n# turn year into the factor \u0026quot;Year\u0026quot; shafii.rapeseed$Year \u0026lt;- as.factor(shafii.rapeseed$year) # create a blocking variable that is unique for each location-by-year combination # so R doesn't conflate \u0026quot;R1\u0026quot; from one location/year with another location/year shafii.rapeseed$Rep \u0026lt;- as.factor(paste(shafii.rapeseed$loc, shafii.rapeseed$year, shafii.rapeseed$rep, sep = \u0026quot;_\u0026quot;)) shaf.lme \u0026lt;- lme(fixed = yield ~ gen*loc + Year, random = ~ 1|Rep, data = shafii.rapeseed, method = \u0026quot;REML\u0026quot;) # view sum of squares table # when anova() is called for an lme object, the function called is actually anova.lme() anova(shaf.lme, type = \u0026quot;marginal\u0026quot;) # \u0026quot;marginal\u0026quot; is equivalent to type III sums of squares  ## numDF denDF F-value p-value ## (Intercept) 1 470 16.204597 0.0001 ## gen 5 470 1.092341 0.3637 ## loc 13 92 13.074492 \u0026lt;.0001 ## Year 2 92 2.035054 0.1365 ## gen:loc 65 470 2.575753 \u0026lt;.0001  Anova(shaf.lme, type = \u0026quot;3\u0026quot;)  ## Analysis of Deviance Table (Type III tests) ## ## Response: yield ## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 16.2046 1 5.686e-05 *** ## gen 5.4617 5 0.3622 ## loc 169.9684 13 \u0026lt; 2.2e-16 *** ## Year 4.0701 2 0.1307 ## gen:loc 167.4239 65 5.579e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # FYI: use \u0026quot;anova(model.lme)\u0026quot; for type I sums of squares # lmer notation shaf.lmer \u0026lt;- lmer(yield ~ gen*loc + Year + (1|Rep), data = shafii.rapeseed, REML = T) anova(shaf.lmer, type = \u0026quot;marginal\u0026quot;)  ## Marginal Analysis of Variance Table with Satterthwaite's method ## Sum Sq Mean Sq NumDF DenDF F value Pr(\u0026gt;F) ## gen 1860586 372117 5 470.00 1.0923 0.3637 ## loc 57901484 4453960 13 159.37 13.0745 \u0026lt; 2.2e-16 *** ## Year 1386524 693262 2 92.00 2.0351 0.1365 ## gen:loc 57034691 877457 65 470.00 2.5758 5.499e-09 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Anova(shaf.lmer, type = \u0026quot;3\u0026quot;)  ## Analysis of Deviance Table (Type III Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 16.2046 1 5.686e-05 *** ## gen 5.4617 5 0.3622 ## loc 169.9684 13 \u0026lt; 2.2e-16 *** ## Year 4.0701 2 0.1307 ## gen:loc 167.4239 65 5.579e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Diagnostics, model building plot(shaf.lme)  qqnorm(shaf.lme, abline = c(0, 1))  Least squares means # for cultivar (lme.means.cv \u0026lt;- emmeans(shaf.lme, \u0026quot;gen\u0026quot;))  ## NOTE: Results may be misleading due to involvement in interactions  ## gen emmean SE df lower.CL upper.CL ## Bienvenu 2432 112 92 2211 2654 ## Bridger 2314 112 92 2092 2536 ## Cascade 2184 112 92 1962 2406 ## Dwarf 2308 112 92 2087 2530 ## Glacier 2463 112 92 2242 2685 ## Jet 2304 112 92 2082 2525 ## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95  # for location (lme.means.loc \u0026lt;- emmeans(shaf.lme, \u0026quot;loc\u0026quot;))  ## NOTE: Results may be misleading due to involvement in interactions  ## loc emmean SE df lower.CL upper.CL ## GGA 1682 329 92 1030 2335 ## ID 4217 261 92 3698 4736 ## KS 1120 476 92 174 2066 ## MS 2204 476 92 1258 3150 ## MT 3339 474 92 2398 4280 ## NC 1328 329 92 676 1981 ## NY 3139 476 92 2193 4085 ## OR 3292 329 92 2640 3945 ## SC 1819 261 92 1300 2338 ## TGA 1028 261 92 509 1547 ## TN 2543 476 92 1597 3490 ## TX 827 329 92 174 1479 ## VA 2282 328 92 1631 2932 ## WA 3861 261 92 3342 4380 ## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95  # for cultivar means within each location lme.means.int \u0026lt;- emmeans(shaf.lme, ~ gen | loc + Year) # this code would produce location means within each cultivar # emmeans(model.lme, ~ loc | gen)) # also: # emmeans(model.lme, ~ loc | gen)) provides the same estimates as 'emmeans(model.lme, ~ gen | loc))'  Pairwise Contrasts: # all pairwise pairs(lme.means.cv)  ## contrast estimate SE df t.ratio p.value ## Bienvenu - Bridger 118.57 87.6 470 1.353 0.7548 ## Bienvenu - Cascade 248.34 87.6 470 2.834 0.0539 ## Bienvenu - Dwarf 124.11 87.6 470 1.417 0.7170 ## Bienvenu - Glacier -31.00 87.6 470 -0.354 0.9993 ## Bienvenu - Jet 128.70 87.6 470 1.469 0.6843 ## Bridger - Cascade 129.77 87.6 470 1.481 0.6765 ## Bridger - Dwarf 5.54 87.6 470 0.063 1.0000 ## Bridger - Glacier -149.57 87.6 470 -1.707 0.5277 ## Bridger - Jet 10.13 87.6 470 0.116 1.0000 ## Cascade - Dwarf -124.23 87.6 470 -1.418 0.7161 ## Cascade - Glacier -279.34 87.6 470 -3.188 0.0190 ## Cascade - Jet -119.64 87.6 470 -1.366 0.7477 ## Dwarf - Glacier -155.10 87.6 470 -1.770 0.4861 ## Dwarf - Jet 4.59 87.6 470 0.052 1.0000 ## Glacier - Jet 159.70 87.6 470 1.823 0.4521 ## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## P value adjustment: tukey method for comparing a family of 6 estimates  # plot results plot(lme.means.cv, comparison = T)  plot(lme.means.loc, comparison = T, horizontal = F) # rotate plots to vertical position  ## Warning: Comparison discrepancy in group \u0026quot;1\u0026quot;, GGA - OR: ## Target overlap = 0.0083, overlap on graph = -0.0111  # blue bars = lsmeans confidence 95% confidence intervals # red arrows. pairwise differences (overlapping arrows = not significantly different)  For those who want the letters assigned to treatments based on all pairwise comparisons, it\u0026rsquo;s an unwieldy road:\nlibrary(multcomp) # this will need to be installed if you do not already have it tukey \u0026lt;- glht(shaf.lme, linfct = mcp(loc = \u0026quot;Tukey\u0026quot;)) ### extract information cld_tukey \u0026lt;- cld(tukey) print(cld_tukey)  ## GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA ## \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ac\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;bc\u0026quot; \u0026quot;a\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;a\u0026quot; \u0026quot;a\u0026quot; \u0026quot;bc\u0026quot;  Interaction plots can also be done:\n(but, it gets unwieldy)\nplot(lme.means.int, comparison = T, adjust = \u0026quot;tukey\u0026quot;)  Other pre-set contrasts # compare to a control, e.g. \u0026quot;Bridger\u0026quot; levels(shafii.rapeseed$gen)  ## [1] \u0026quot;Bienvenu\u0026quot; \u0026quot;Bridger\u0026quot; \u0026quot;Cascade\u0026quot; \u0026quot;Dwarf\u0026quot; \u0026quot;Glacier\u0026quot; \u0026quot;Jet\u0026quot;  # Bridger is listed in position 2 of the factor 'shafii.rapeseed$gen' # so '2' is set as the reference level in the following contrast statement: # \u0026quot;trt.vs.ctrlk\u0026quot; (treatment versus control treatment k) is a specific option to compare all treatment levels to a user-defined level # by default, it will use the last level as the reference level contrast(lme.means.cv, \u0026quot;trt.vs.ctrlk\u0026quot;, ref = 2)  ## contrast estimate SE df t.ratio p.value ## Bienvenu - Bridger 118.57 87.6 470 1.353 0.5118 ## Cascade - Bridger -129.77 87.6 470 -1.481 0.4315 ## Dwarf - Bridger -5.54 87.6 470 -0.063 0.9998 ## Glacier - Bridger 149.57 87.6 470 1.707 0.3034 ## Jet - Bridger -10.13 87.6 470 -0.116 0.9990 ## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## P value adjustment: dunnettx method for 5 tests  Search ?contrast.emmGrid to see full list of options for preset contrasts.\nCustom contrasts # example: contrast Western locations versus Southern locations # first, find out what levels are present unique(shafii.rapeseed$loc)  ## [1] GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA ## Levels: GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA  # next create a contrast list # this is a list of coefficients as long your list of treatment levels # indicating what coefficients to give each treatment level # in this example, levels \u0026quot;ID\u0026quot;, \u0026quot;MT\u0026quot;, \u0026quot;OR\u0026quot;, and \u0026quot;WA\u0026quot; are contrasted versus # \u0026quot;NC\u0026quot;, \u0026quot;SC\u0026quot;, \u0026quot;MS\u0026quot;, \u0026quot;TN\u0026quot;, \u0026quot;TX\u0026quot; and \u0026quot;VA\u0026quot; cList \u0026lt;- list(West_V_South = c(0, 1/4, 0, -1/6, 1/4, -1/6, 0, 1/4, -1/6, 0, -1/6, -1/6, -1/6, 1/4)) # check that each contrast sums to zero: lapply(cList, sum)  ## $West_V_South ## [1] 5.551115e-17  lme.means.loc2 \u0026lt;- emmeans(shaf.lme, \u0026quot;loc\u0026quot;, contr = cList)  ## NOTE: Results may be misleading due to involvement in interactions  summary(lme.means.loc2)  ## $emmeans ## loc emmean SE df lower.CL upper.CL ## GGA 1682 329 92 1030 2335 ## ID 4217 261 92 3698 4736 ## KS 1120 476 92 174 2066 ## MS 2204 476 92 1258 3150 ## MT 3339 474 92 2398 4280 ## NC 1328 329 92 676 1981 ## NY 3139 476 92 2193 4085 ## OR 3292 329 92 2640 3945 ## SC 1819 261 92 1300 2338 ## TGA 1028 261 92 509 1547 ## TN 2543 476 92 1597 3490 ## TX 827 329 92 174 1479 ## VA 2282 328 92 1631 2932 ## WA 3861 261 92 3342 4380 ## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## West_V_South 1843 233 92 7.910 \u0026lt;.0001 ## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment  # same contrast can also be done within each level of 'gen': emmeans(shaf.lme, ~ loc | gen, contr = cList)  ## $emmeans ## gen = Bienvenu: ## loc emmean SE df lower.CL upper.CL ## GGA 1785 379 92 1032.31 2537 ## ID 4742 303 92 4140.13 5345 ## KS 1179 546 92 94.60 2263 ## MS 2455 546 92 1371.47 3539 ## MT 2825 544 92 1745.38 3904 ## NC 1330 379 92 577.36 2082 ## NY 2934 546 92 1849.69 4018 ## OR 4118 379 92 3365.98 4870 ## SC 1844 303 92 1241.42 2446 ## TGA 893 303 92 290.99 1496 ## TN 2965 546 92 1880.59 4049 ## TX 919 379 92 167.04 1671 ## VA 2124 378 92 1373.34 2875 ## WA 3943 303 92 3340.44 4545 ## ## gen = Bridger: ## loc emmean SE df lower.CL upper.CL ## GGA 1470 379 92 718.17 2223 ## ID 3591 303 92 2989.15 4194 ## KS 1091 546 92 7.35 2175 ## MS 2478 546 92 1393.89 3562 ## MT 3037 544 92 1957.63 4117 ## NC 1479 379 92 727.28 2232 ## NY 3130 546 92 2045.60 4214 ## OR 2564 379 92 1811.99 3316 ## SC 2282 303 92 1679.58 2884 ## TGA 1603 303 92 1000.66 2205 ## TN 2485 546 92 1401.33 3569 ## TX 851 379 92 99.08 1604 ## VA 2397 378 92 1646.76 3148 ## WA 3935 303 92 3332.27 4537 ## ## gen = Cascade: ## loc emmean SE df lower.CL upper.CL ## GGA 1758 379 92 1006.25 2511 ## ID 4081 303 92 3479.04 4684 ## KS 891 546 92 -193.40 1975 ## MS 1598 546 92 514.04 2682 ## MT 3125 544 92 2045.63 4205 ## NC 1062 379 92 309.61 1814 ## NY 2586 546 92 1502.21 3670 ## OR 2806 379 92 2053.82 3558 ## SC 1982 303 92 1379.70 2584 ## TGA 1492 303 92 889.83 2094 ## TN 2006 546 92 922.37 3090 ## TX 796 379 92 43.59 1548 ## VA 2191 378 92 1440.56 2942 ## WA 4203 303 92 3600.69 4805 ## ## gen = Dwarf: ## loc emmean SE df lower.CL upper.CL ## GGA 1538 379 92 785.71 2290 ## ID 4326 303 92 3723.81 4928 ## KS 1208 546 92 123.85 2292 ## MS 1966 546 92 881.69 3050 ## MT 3661 544 92 2581.14 4740 ## NC 1321 379 92 568.53 2073 ## NY 3645 546 92 2561.26 4729 ## OR 3594 379 92 2841.40 4346 ## SC 1292 303 92 690.10 1895 ## TGA 451 303 92 -151.81 1053 ## TN 2688 546 92 1603.57 3771 ## TX 654 379 92 -98.64 1406 ## VA 2250 378 92 1499.12 3000 ## WA 3726 303 92 3123.52 4328 ## ## gen = Glacier: ## loc emmean SE df lower.CL upper.CL ## GGA 2031 379 92 1278.35 2783 ## ID 4299 303 92 3696.61 4901 ## KS 1268 546 92 183.85 2352 ## MS 2861 546 92 1776.82 3945 ## MT 3516 544 92 2436.14 4595 ## NC 1452 379 92 699.82 2204 ## NY 3301 546 92 2217.49 4385 ## OR 3472 379 92 2719.36 4224 ## SC 2025 303 92 1422.97 2628 ## TGA 1109 303 92 506.90 1712 ## TN 2265 546 92 1180.58 3348 ## TX 720 379 92 -31.85 1473 ## VA 2363 378 92 1612.64 3114 ## WA 3807 303 92 3205.02 4410 ## ## gen = Jet: ## loc emmean SE df lower.CL upper.CL ## GGA 1511 379 92 758.95 2263 ## ID 4262 303 92 3659.68 4864 ## KS 1082 546 92 -2.40 2166 ## MS 1866 546 92 781.68 2950 ## MT 3869 544 92 2789.89 4949 ## NC 1326 379 92 573.58 2078 ## NY 3237 546 92 2152.80 4321 ## OR 3199 379 92 2446.70 3951 ## SC 1488 303 92 886.13 2091 ## TGA 622 303 92 19.27 1224 ## TN 2853 546 92 1768.63 3937 ## TX 1020 379 92 267.99 1772 ## VA 2364 378 92 1613.85 3115 ## WA 3554 303 92 2952.19 4157 ## ## Results are averaged over the levels of: Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts ## gen = Bienvenu: ## contrast estimate SE df t.ratio p.value ## West_V_South 1968 267 92 7.359 \u0026lt;.0001 ## ## gen = Bridger: ## contrast estimate SE df t.ratio p.value ## West_V_South 1286 267 92 4.811 \u0026lt;.0001 ## ## gen = Cascade: ## contrast estimate SE df t.ratio p.value ## West_V_South 1948 267 92 7.286 \u0026lt;.0001 ## ## gen = Dwarf: ## contrast estimate SE df t.ratio p.value ## West_V_South 2132 267 92 7.972 \u0026lt;.0001 ## ## gen = Glacier: ## contrast estimate SE df t.ratio p.value ## West_V_South 1826 267 92 6.828 \u0026lt;.0001 ## ## gen = Jet: ## contrast estimate SE df t.ratio p.value ## West_V_South 1902 267 92 7.112 \u0026lt;.0001 ## ## Results are averaged over the levels of: Year ## Degrees-of-freedom method: containment  To perform custom contrasts on a another variable, a cList and emmeans call for that variable is required.\nANCOVA (analysis of covariance) From a R programming perspective, this is no different than running a standard linear model. A data set from agridat, \u0026ldquo;theobald.covariate\u0026rdquo; comparing corn silage yields across multiple years, locations and cultivars. The data set includes a covariate, \u0026ldquo;chu\u0026rdquo; (corn heat units, a bit like growing degree days).\nLoad data and examine:\ndata(theobald.covariate) str(theobald.covariate)  ## 'data.frame':\t256 obs. of 5 variables: ## $ year : int 1990 1990 1990 1990 1990 1991 1991 1991 1991 1991 ... ## $ env : Factor w/ 7 levels \u0026quot;E1\u0026quot;,\u0026quot;E2\u0026quot;,\u0026quot;E3\u0026quot;,..: 1 2 3 4 7 1 2 3 4 5 ... ## $ gen : Factor w/ 10 levels \u0026quot;G01\u0026quot;,\u0026quot;G02\u0026quot;,\u0026quot;G03\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ yield: num 6.27 5.57 8.45 7.35 6.5 6.71 5.59 8.36 7.25 8.09 ... ## $ chu : num 2.57 2.53 2.72 2.72 2.48 2.44 2.55 2.75 2.75 2.61 ...  count_na(theobald.covariate)  ## year env gen yield chu ## 0 0 0 0 0  Exploratory plots:\n# distributions of continuous variables hist(theobald.covariate$yield, col = \u0026quot;gold\u0026quot;)  hist(theobald.covariate$chu, col = \u0026quot;gray70\u0026quot;)  # relationship between reponse variable and covariate: with(theobald.covariate, plot(chu, yield))  length(unique(theobald.covariate$chu))  ## [1] 21  # the usual boxplots: boxplot(yield ~ env, data = theobald.covariate, col = \u0026quot;orangered\u0026quot;)  boxplot(yield ~ year, data = theobald.covariate, col = \u0026quot;chartreuse\u0026quot;)  boxplot(yield ~ gen, data = theobald.covariate, col = \u0026quot;darkcyan\u0026quot;)  Check the extent of replication:\ntheobald.covariate$Year \u0026lt;- as.factor(theobald.covariate$year) replications(yield ~ Year + env + gen, data = theobald.covariate)  ## $Year ## Year ## 1990 1991 1992 1993 1994 ## 40 63 60 45 48 ## ## $env ## env ## E1 E2 E3 E4 E5 E6 E7 ## 35 35 44 36 36 36 34 ## ## $gen ## gen ## G01 G02 G03 G04 G05 G06 G07 G08 G09 G10 ## 29 29 29 29 22 29 23 18 24 24  # with(theobald.covariate, table(gen, env, Year)) # lots of useful output  The treatments are not fully crossed, so a fully specified model of the form yield ~ Year*env*gen*chu cannot be tested. The treatments and interactions were tested in reduced models and compared (not shown). The final \u0026ldquo;best\u0026rdquo; model is shown below.\n# the covariate, chu, is added in like any other effect. theobald.lm2 \u0026lt;- lm(yield ~ Year + env*chu, data = theobald.covariate) Anova(theobald.lm2, type = \u0026quot;III\u0026quot;)  ## Anova Table (Type III tests) ## ## Response: yield ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 4.309 1 6.8321 0.009524 ** ## Year 76.589 4 30.3607 \u0026lt; 2.2e-16 *** ## env 13.473 6 3.5607 0.002138 ** ## chu 11.831 1 18.7596 2.187e-05 *** ## env:chu 13.376 6 3.5350 0.002268 ** ## Residuals 150.096 238 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # how to extract the covariate slope(s): emtrends(theobald.lm2, ~ env, \u0026quot;chu\u0026quot;)  ## env chu.trend SE df lower.CL upper.CL ## E1 7.015 1.62 238 3.82 10.21 ## E2 0.979 4.44 238 -7.76 9.72 ## E3 4.099 3.15 238 -2.11 10.31 ## E4 -2.884 3.54 238 -9.87 4.10 ## E5 8.222 2.70 238 2.90 13.54 ## E6 3.425 2.72 238 -1.93 8.78 ## E7 -0.359 2.55 238 -5.38 4.66 ## ## Results are averaged over the levels of: Year ## Confidence level used: 0.95  # emmeans extracted as usual: emmeans(theobald.lm2, ~ env)  ## NOTE: Results may be misleading due to involvement in interactions  ## env emmean SE df lower.CL upper.CL ## E1 6.67 0.175 238 6.32 7.01 ## E2 5.13 0.256 238 4.63 5.64 ## E3 6.66 0.482 238 5.71 7.61 ## E4 7.22 0.508 238 6.22 8.22 ## E5 6.61 0.138 238 6.34 6.88 ## E6 6.43 0.236 238 5.97 6.90 ## E7 6.32 0.397 238 5.54 7.10 ## ## Results are averaged over the levels of: Year ## Confidence level used: 0.95  emmeans(theobald.lm2, ~ Year)  ## Year emmean SE df lower.CL upper.CL ## 1990 6.97 0.189 238 6.60 7.34 ## 1991 6.75 0.170 238 6.41 7.08 ## 1992 7.07 0.187 238 6.70 7.44 ## 1993 5.39 0.208 238 4.98 5.80 ## 1994 6.00 0.218 238 5.57 6.43 ## ## Results are averaged over the levels of: env ## Confidence level used: 0.95  Split-plot Load \u0026ldquo;Oats\u0026rdquo; from nlme. Nitrogen level (\u0026ldquo;nitro\u0026rdquo;) is the main plot, cultivar (\u0026ldquo;Variety\u0026rdquo;) is the sub-plot and \u0026ldquo;Block\u0026rdquo; describes the blocking layout.\ndata(Oats) str(Oats)  ## Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':\t72 obs. of 4 variables: ## $ Block : Ord.factor w/ 6 levels \u0026quot;VI\u0026quot;\u0026lt;\u0026quot;V\u0026quot;\u0026lt;\u0026quot;III\u0026quot;\u0026lt;..: 6 6 6 6 6 6 6 6 6 6 ... ## $ Variety: Factor w/ 3 levels \u0026quot;Golden Rain\u0026quot;,..: 3 3 3 3 1 1 1 1 2 2 ... ## $ nitro : num 0 0.2 0.4 0.6 0 0.2 0.4 0.6 0 0.2 ... ## $ yield : num 111 130 157 174 117 114 161 141 105 140 ... ## - attr(*, \u0026quot;formula\u0026quot;)=Class 'formula' language yield ~ nitro | Block ## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt; ## - attr(*, \u0026quot;labels\u0026quot;)=List of 2 ## ..$ y: chr \u0026quot;Yield\u0026quot; ## ..$ x: chr \u0026quot;Nitrogen concentration\u0026quot; ## - attr(*, \u0026quot;units\u0026quot;)=List of 2 ## ..$ y: chr \u0026quot;(bushels/acre)\u0026quot; ## ..$ x: chr \u0026quot;(cwt/acre)\u0026quot; ## - attr(*, \u0026quot;inner\u0026quot;)=Class 'formula' language ~Variety ## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt;  count_na(Oats)  ## Block Variety nitro yield ## 0 0 0 0  Oats$N \u0026lt;- as.factor(Oats$nitro) replications(yield ~ Variety*N*Block, data = Oats)  ## Variety N Block Variety:N Variety:Block ## 24 18 12 6 4 ## N:Block Variety:N:Block ## 3 1  table(Oats$Variety, Oats$N)  ## ## 0 0.2 0.4 0.6 ## Golden Rain 6 6 6 6 ## Marvellous 6 6 6 6 ## Victory 6 6 6 6  hist(Oats$yield, col = \u0026quot;gold\u0026quot;)  boxplot(yield ~ N, data = Oats, col = \u0026quot;dodgerblue1\u0026quot;)  boxplot(yield ~ Variety, data = Oats, col = \u0026quot;red3\u0026quot;)  Balanced Trial Analysis\nThe format for specifying split-plot error terms is Error(blocking factor/main plot).\n#contrasts(\u0026quot;contr.sum\u0026quot;) spl.oats \u0026lt;- aov(yield ~ Variety*N + Error(Block:N), data = Oats) summary(spl.oats)  ## ## Error: Block:N ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## N 3 20020 6673 7.556 0.00143 ** ## Residuals 20 17663 883 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Variety 2 1786 893.2 2.930 0.0649 . ## Variety:N 6 322 53.6 0.176 0.9818 ## Residuals 40 12194 304.8 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  emmeans(spl.oats, \u0026quot;N\u0026quot;)  ## Note: re-fitting model with sum-to-zero contrasts  ## NOTE: Results may be misleading due to involvement in interactions  ## N emmean SE df lower.CL upper.CL ## 0 79.4 7 20 64.8 94 ## 0.2 98.9 7 20 84.3 114 ## 0.4 114.2 7 20 99.6 129 ## 0.6 123.4 7 20 108.8 138 ## ## Results are averaged over the levels of: Variety ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95  emmeans(spl.oats, ~ Variety)  ## Note: re-fitting model with sum-to-zero contrasts ## NOTE: Results may be misleading due to involvement in interactions  ## Variety emmean SE df lower.CL upper.CL ## Golden Rain 104.5 4.55 46.1 95.3 114 ## Marvellous 109.8 4.55 46.1 100.6 119 ## Victory 97.6 4.55 46.1 88.5 107 ## ## Results are averaged over the levels of: N ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95  Unbalanced Trial Analysis\nspl.oats2 \u0026lt;- lmer(yield ~ N*Variety + (1|Block:N), data = Oats) Anova(spl.oats2, type = \u0026quot;3\u0026quot;)  ## Analysis of Deviance Table (Type III Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 77.1670 1 \u0026lt; 2.2e-16 *** ## N 13.9028 3 0.003041 ** ## Variety 2.2747 2 0.320663 ## N:Variety 1.0554 6 0.983423 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  emmeans(spl.oats2, \u0026quot;N\u0026quot;)  ## NOTE: Results may be misleading due to involvement in interactions  ## N emmean SE df lower.CL upper.CL ## 0 79.4 7 20 64.8 94 ## 0.2 98.9 7 20 84.3 114 ## 0.4 114.2 7 20 99.6 129 ## 0.6 123.4 7 20 108.8 138 ## ## Results are averaged over the levels of: Variety ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95  emmeans(spl.oats2, ~ Variety)  ## NOTE: Results may be misleading due to involvement in interactions  ## Variety emmean SE df lower.CL upper.CL ## Golden Rain 104.5 4.55 46.1 95.3 114 ## Marvellous 109.8 4.55 46.1 100.6 119 ## Victory 97.6 4.55 46.1 88.5 107 ## ## Results are averaged over the levels of: N ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95  Other Designs There are many other experimental designs commonly used in agricultural trials (split-split plot, split-block, alpha lattice, etc). We have written an online resource for routine incorporation of spatial covariates into field trial analysis that includes information on how to analyze different designs. You could also consider using the agricolae package.\nExtra Functions for extracting model parameters, diagnostics and other model information\nThese work differently with different R object types. That is, different output will result depending on if a \u0026ldquo;lm\u0026rdquo;, \u0026ldquo;lme\u0026rdquo; or \u0026ldquo;merMod\u0026rdquo; (lmer) object is used in the function call.\n# extract model summary summary() #extract coefficients: coef() #extract residuals resid() rstudent() residuals() # extract predicted values fits() # make diagnostic plots plot() # extract influence measures: influence.measures() #other fir diagnostics: cooks.distance() dffits() dfbeta() hat()  To see the all functions available for a particular type of linear model object, use:\nmethods(class = \u0026quot;lm\u0026quot;) # for lm objects  ## [1] add1 addterm alias ## [4] anova Anova attrassign ## [7] avPlot Boot bootCase ## [10] boxcox boxCox brief ## [13] case.names ceresPlot coerce ## [16] concordance confidenceEllipse confint ## [19] Confint cooks.distance crPlot ## [22] deltaMethod deviance dfbeta ## [25] dfbetaPlots dfbetas dfbetasPlots ## [28] drop1 dropterm dummy.coef ## [31] durbinWatsonTest effects emm_basis ## [34] extractAIC family formula ## [37] hatvalues hccm infIndexPlot ## [40] influence influencePlot initialize ## [43] inverseResponsePlot kappa labels ## [46] leveneTest leveragePlot linearHypothesis ## [49] logLik logtrans mcPlot ## [52] mmp model.frame model.matrix ## [55] ncvTest nextBoot nobs ## [58] outlierTest plot powerTransform ## [61] predict Predict print ## [64] proj qqnorm qqPlot ## [67] qr recover_data residualPlot ## [70] residualPlots residuals rstandard ## [73] rstudent S show ## [76] sigmaHat simulate slotsFromS3 ## [79] spreadLevelPlot summary symbox ## [82] variable.names vcov ## see '?methods' for accessing help and source code  methods(class = \u0026quot;lme\u0026quot;) # for lme4 objects  ## [1] ACF anova Anova augPred ## [5] coef comparePred confint Confint ## [9] deltaMethod deviance emm_basis extractAIC ## [13] fitted fixef formula getData ## [17] getGroups getGroupsFormula getResponse getVarCov ## [21] influence intervals linearHypothesis logLik ## [25] matchCoefs nobs pairs plot ## [29] predict print qqnorm ranef ## [33] recover_data residuals S sigma ## [37] simulate summary update VarCorr ## [41] Variogram vcov ## see '?methods' for accessing help and source code  methods(class = \u0026quot;merMod\u0026quot;) # for nlme objects  ## [1] anova Anova as.function coef ## [5] confint cooks.distance deltaMethod deviance ## [9] df.residual drop1 emm_basis extractAIC ## [13] family fitted fixef formula ## [17] getData getL getME hatvalues ## [21] influence isGLMM isLMM isNLMM ## [25] isREML linearHypothesis logLik matchCoefs ## [29] model.frame model.matrix na.action ngrps ## [33] nobs plot predict print ## [37] profile ranef recover_data refit ## [41] refitML rePCA residuals rstudent ## [45] show sigma simulate summary ## [49] terms update VarCorr vcov ## [53] vif weights ## see '?methods' for accessing help and source code  The package emmeans also supports a large number of models.\n","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"a1d58233f436a629f860db1f7ebc5c18","permalink":"/post/anova-in-r/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/post/anova-in-r/","section":"post","summary":"Introduction ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility.","tags":["ANOVA","linear models","lme4","emmeans"],"title":"Applied ANOVA in R","type":"post"},{"authors":["William Price"],"categories":["errors","statistics","SAS"],"content":"You need to get that analysis done so you can write that report/paper/dissertation/thesis/presentation to meet that deadline to please the boss \u0026hellip; BUT \u0026hellip; SAS is giving you a heartburn! Don\u0026rsquo;t panic. Although SAS errors look confusing, SAS is really trying to help you find the problem. Really! Look in the Log Window for any error messages. They are marked in RED. Now, remember that SAS runs sequentially, top to bottom, so any mistakes at the top of the program will propagate more errors down the line. Always try to fix the first error you see, then rerun the program. You may find that when the initial problems are fixed, the rest of the errors will vanish. When SAS gives an error message, it also lists a numeric code, like 201 or 76 indicating the line and character in the line where it had trouble. SAS will also underline the offending part of the program and mark it with the numeric code so you can match an error message with the problem area. It can\u0026rsquo;t always find the exact place where the problem occurred, but it usually close by. For more debugging information take a look at the TIPS below on common mistakes and problems. Good Luck!\nMissing Quotes: If you\u0026rsquo;ve recieved the \u0026ldquo;longer than 200 characters\u0026rdquo; message then the cause of the problem is easy to find. You have left out a quote somewhere. Carefully go back through the program and check for both beginning and ending quotes in any statements that use quoted strings such as TITLE, INFILE, or AXIS statements . Quoted strings in SAS are color coded as a magenta color to make them easier to see. Large blocks of text appearing this color are an indicator of a missing quote. Also note that single quotes are different than double quote characters. If you start a string with one of these and end with the other type, SAS will indicate a missing quote.\nAfter you find the culprit, fix it. But wait! Correcting the problem is not quite that easy (somehow you knew that!). SAS builds up the code you run sequentially. So if you try to run the \u0026ldquo;corrected\u0026rdquo; code, SAS will STILL be looking for an ending quote from last time. The quotes will still be unbalanced and you\u0026rsquo;ll still get an error message. This can be very frustrating, but here is the solution. First, correct the code, but before you run it, open a new editor window. Next, give SAS what it is looking for, the offending single quote. In the new editor window enter this: '; and submit that as a program. This will satisfy SAS (ignore any error messages at this point). Then, go back to the original program and run it. The \u0026ldquo;quote\u0026rdquo; problem should go away.\nMissing Semicolons: Leaving out a semicolon is probably the easiest mistake to make. The error message you get may vary here depending on where the mistake was made. Usually you get a message like \u0026ldquo;variable XXX not found\u0026rdquo; or \u0026ldquo;invalid option\u0026rdquo;. SAS is trying to interpret the errant statement as part of the preceding statement and, therefore, gets it\u0026rsquo;s cyberfeet tangled. The solution is obvious: put in the missing ; and resubmit the program.\nMissing RUN Statement: Sometimes while running a program several times you\u0026rsquo;ll notice that the PROC LOTTERY you had at the end of your program is missing from the output listing. So, thinking that SAS maybe just \u0026ldquo;forgot\u0026rdquo; to run it the first time, you run the program again. Now you find it at the top of the output listing instead of the bottom! What the heck is going on!? Remember, SAS runs both sequentially and cumulatively. The first time you ran the program, SAS saw the PROC LOTTERY at the end, but didn\u0026rsquo;t find a RUN; statement. Without that it will hold on to the PROC LOTTERY and wait to execute it when it sees the next DATA or PROC step. When you run the program for the second time SAS first executes the PROC it is holding on to and then runs the submitted program (again leaving off the last PROC). This can cause confusion to no end (no pun intended!). The solution, of course, is to put in the missing RUN;. By the way, before you go running to your SAS manuals looking for PROC LOTTERY, remember this is only and example!\nLISTING Window/File: Like other portions of SAS, the Listing and Log windows or output files may be cumulative. If you know you have changed your calculations for yield from lbs to kilograms, but SAS seems to keep printing lbs in the output, make sure you have cleared out the Listing window. You are probably looking at an \u0026ldquo;old\u0026rdquo; output created before you changed things. This also applies to the Log window.\nCase problems: SAS is creating too many levels of a variable, for example, when running PROC MIXED, PROC GLIMMIX or PROC FREQ. Go back and check the data very carefully. Alphabetic variables are case sensitive so the value \u0026ldquo;Trt1\u0026rdquo; is different from \u0026ldquo;TRT1\u0026rdquo;. This problem often creeps into programs when data has been combined over several years or locations. If you have this type of data, try to formulate a consistent template for yourself and others to use for entering data. Define what the treatment codes and variable names will be ahead of time. The only way to handle this problem is to change the data or use a data step and the IF statement to correct the problem.\n0 and O problems: SAS keeps insisting that it can\u0026rsquo;t find your variable TRT0, but you know it\u0026rsquo;s there in the INPUT statement! Check things carefully. You may have used O (capital oh) instead of a zero. This is very easy to do and is made worse by the fact that the two are physically close together on most keyboards. Similar to problems with Case above, this mix up may also show up as a problem when reading in data. The only solution is to change the problem characters to the appropriate values and try again. If this problem occurs a lot, you might consider trying a different font which uses a dot or slash when writing a zero.\nGeneral Debugging Tips: Debugging SAS can be a trying experience. Here are some techniques I use to help the process.\n  Build long complex programs a step at a time. Start by reading in the data. Print it. Check it. Then do any calculations, then add PROCs and other DATA steps. Don\u0026rsquo;t try to write the whole thing at once. Verify that a piece of code works and then move on.\n  Use Comments. Try to isolate problem code by commenting out sections of the program. This is a very useful technique. See Comments.\n  Try writing programs with consistent and readable structure. Use line breaks and indentation where needed and throw in comments to explain what is happening in the program. This will help organize the program flow in your mind and point out where things are going wrong. See Programming Habits.\n  Stay calm! Learning the quirks of SAS takes time. If all else fails let us know!\n  Some SAS Resources for Error Corrections:   Simply copy and paste the error message into a search engine like Google.\n  Ask a question on the SAS forums:\n  Types of Errors in SAS\n  Beginning SAS books\n  Tips and Strategies for Mixed Modeling\n  ","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"986bccb7753c0e7b812d0057ff373c0f","permalink":"/post/sas-troubleshooting/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/sas-troubleshooting/","section":"post","summary":"Common Errors and Corrections in SAS","tags":["SAS","Errors"],"title":"Common Errors and Solutions in SAS","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Install R: You can download R here. Get the correct R distribution for your operating system. Once downloaded, click on downloaded file, and follow the installation instructions.\nNote that R is updated several times per year. If your installation is a year old or more, consider updating your version of R to the latest version.\nInstall RStudio Rstudio is not R, rather, it is a user interface for accessing R. It is a complicated interface with many features for developers. Despite its complexity, RStudio is nevertheless a very helpful R user interface for users of all abilities. It can downloaded here. For most users, the free version of \u0026ldquo;RStudio Desktop\u0026rdquo; should be chosen. Once downloaded, click on downloaded file, and follow the installation instructions.\nInstall Rtools (optional) Only Windows users need to consider this step. This app is for compiling R packages with C, C++ and Fortran code. It is a separate piece of software that has to be downloaded and installed (it is not an R package). Rtools is not needed by all users and if you don\u0026rsquo;t know if you need this, it is absolutely fine to skip this step. If you do think you need this, You can find it here. Download and install.\nSetting up RStudio Setup (optional) This is an optional step, but it is highly recommended. This step will prevent RStudio from saving all of your objects in a session to .Rdata file that is then automatically loaded whenever you open R.\ninstall.packages(\u0026quot;usethis\u0026quot;); library(usethis) usethis::use_blank_slate()  You can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nWhy is automatic loading of an .Rdata file not recommended? Because it makes your work less reproducible. You may have created test objects that will unexpectedly interfere with downstream operations or analysis. You may have changed the original data source, but an older version is saved in the .Rdata file. More explanation is given by RStudio.\nIf you are used to opening R and seeing all of your previous objects automatically loaded into the objects pane, this will be an adjustment. The solution is to save your processes into .R scripts that capture all information from packages loaded, file import, all data manipulations and other operations important. If these steps are slow and there is a need to access intermediate objects, these can be saved in tabular formats readable by many applications (e.g. .txt or .csv) or saved as a specific R object (see saveRDS() in the R help files) and reloaded in another session.\nSet up version control (optional) If you use Git or SVN, you can perform Git operations directions from RStudio and interact with remote repositories. If you don\u0026rsquo;t use version control, this step can be skipped. If you do use version control, the command line or other third-party software (e.g. Gitkraken) are fine to use instead or in addition to RStudio\u0026rsquo;s interface. The implementation of git in R is very minimal and supports only a limited number of actions, so you are likely to need other software to perform complicated git actions. It is useful for file additions, commits, pushes and pulls.\nYou can set up Git by going to Tools \u0026ndash;\u0026gt; Global Options \u0026ndash;\u0026gt; Git/SVN.\nThis is not the right space to provide detailed instructions for using git as an R user, but Jenny Bryan has written a very helpful tutorial covering this subject.\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"8b70804b6afa070131995606b8772ebd","permalink":"/post/getting-r-setup/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/getting-r-setup/","section":"post","summary":"Some instructions for R installation and your R setup to support reproducible research.","tags":["R","reproducible research"],"title":"Getting R Set Up","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Make sure your Rstudio session is not saving .RData automatically: Note: this step requires the usethis package; please install this package if you do not already have it installed.\nStep 1 is to disable automatic saving of your objects to a .RData file. This file is automatically loaded when R restarts. Since we often create all sorts of miscellaneous objects during a session with a clear record of why, loading all objects without a clear sense of their provenance is often not reproducible by other.\nusethis::use_blank_slate()  You can read more about this function in its documentation.\nYou can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nSave all code you run in an .R or .Rmd file This is your source code. It\u0026rsquo;s as real and as important as your input data. This file should capture a set of actions that can be repeated by another person (e.g. your PI, other colleagues yourself in the future) including packages loaded, files imported, all data manipulations and the outputs from these actions (e.g. visualisations, analytical outcomes). The idea is to capture your thought process and specific actions so this can be repeated in full. In most analyses, it is extremely likely* you will revisit a project and need to repeat what has already been done! Keeping a record of actions will save you considerable time because you will not have to attempt to recall and/or reconstruct exactly what you did in previous sessions.\n*Consider yourself very lucky if this does not happen!\nRegularly restart your R session Yes, that means wiping all the loaded packaged and objects from the session (if you followed the first recommendation in these instructions), but the upside is that your analysis are reproducible. This means future you can repeat those analyses and get the same results back you did earlier.\nYou can restart R by manually closing and opening RStudio. You can also restart the R session with RStudio by navigating to the menu item Session \u0026ndash;\u0026gt; Restart R.\nUse R projects This is optional, but it will make your life easier. Whenever you start a new analytical endeavor in R, create an R project by navigating to File \u0026ndash;\u0026gt; New Project in RStudio. There are many options available for setting the [project directory (where the .Rproj file lives), the type of project (e.g. R package, Shiny app or blank), and options to initialise a git repo. The simplest option is to choose New Project (no special type) in a dedicated directory. The main advantage of projects is that by opening an .Rproj file, the working directory is automatically set to that directory. If you are using a cloud solution for working across different computers or working with collaborators, this will make things easier because you can use relative paths for importing data and outputting files. There would be no more need for this at the top of your script:\nsetwd(\u0026quot;specific/path/to/my/computer\u0026quot;)  Additionally, for setting up gitbooks through \u0026lsquo;bookdown\u0026rsquo;, R packages, Shiny apps, and other complicated R endeavors, the automated set-up through R projects can be immensely helpful. This is sometimes referred to as \u0026ldquo;project-oriented workflow.\u0026rdquo; In addition to using R projects with a dedicated directory for each research project, I also prefer to have a consistent directory structure for each project like this one:\ntop-level-directory │ README.md │ └───data │ │ file011.txt │ │ file012.txt │ │ │ └───spatial_files │ │ file208.dbf │ │ file208.shp │ │ file208.shx │ └───scripts │ │ eda.R │ │ analysis.R │ │ plots.R │ │ final_report.Rmd | └───outputs │ │ plot1.png │ │ blups.csv | └───extra │ some_paper.pdf │ ...  I put all raw data needed for analysis into the \u0026lsquo;data\u0026rsquo; directory, any and all programming scripts in the \u0026lsquo;scripts\u0026rsquo; directory, all outputs (plots, tables, intermediate data object) in the \u0026lsquo;outputs\u0026rsquo; directory and everything else ends up \u0026lsquo;extra\u0026rsquo;. Naturally, there are many different directory structures to use and this is just one example. Find something that works best for your needs!\nUse the \u0026lsquo;here\u0026rsquo; package. This is also optional. It works like R projects for setting the working directory. However, for an R project to work, you have to open the .Rproj file in RStudio. What if you or your collaborators prefer to open R files directly and start using those? Here will look for the next directory level which there is a .Rproj file and set the working directory there.\nIf you want to import a file, \u0026ldquo;datafile.csv\u0026rdquo; that located in the data directory. Your .R script is actually located in the \u0026lsquo;scripts\u0026rsquo; directory. Normally, if you try to read that in, you need to specify the full path to \u0026ldquo;mydata.csv\u0026rdquo; or set the working directory and use a relative path. Again, these paths will not work if you switch computers or your collaborators are running these scripts on their own systems. This system gets even more complicated when working with an .Rmd file. Here\u0026rsquo;s an alternative approach that works the same across files and systems:\nFirst, make sure you have .Rproj file to define the top-level directory.\nlibrary(here) mydata \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;datafile.csv\u0026quot;))  This code will construct this path: \u0026ldquo;data/datafile.csv\u0026rdquo; and execute that command under the assumption that wherever that .rproj is located (going up one directory at a time until it finds it) is where the working directory is set. Putting library(here) into every .R or .Rmd file in a project will resolve these issues.\nUse R environments. Again: optional, but it will make your life easier.\nOften in academia, I might do an analysis, move on to something else and then have to return that analysis months or years later. I probably will have updated R and some or all of the packages used in that analysis. As a result of these updates, my original code may not work at all or may not do the intended actions. What I need are both the older version of R and the older packages. The package \u0026lsquo;renv\u0026rsquo; is a solution. It captures the versions of R and the loaded packages. It also builds a custom package library for your package (and caches this information across other projects using renv).\nStart here: (you need to also be using Rprojects since renv is searching for .Rproj file)\nlibrary(renv) renv::init()  If you have a mature project that\u0026rsquo;s not undergoing any further development at this time, this is all you need to do.\nIf you continue to develop your project and install new packages, update your R environment like thus to ensure new or updated packaged are included:\nrenv::snapshot()  If you\u0026rsquo;re familiar with Packrat, this is a replacement for that. This is particularly helpful for things that may have a long life span, like Shiny apps. The renv package has extensive documentation worth reading.\nFinal Comments There are many more resources and recommendations for conducting reproducible research in R. There an entire CRAN task view devoted to this!\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"32bd8072205d33315c2c1a506db82c8c","permalink":"/post/reproducible-r/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/reproducible-r/","section":"post","summary":"A few steps you can take to make your workflow in R more reproducible and less painful for you to deal with.","tags":["R","Reproducible Research"],"title":"Quick Tricks and Tips for Reproducible Research in R","type":"post"},{"authors":["Statistical Programs"],"categories":null,"content":"","date":1617982200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617982200,"objectID":"6b45b7554e1c28d868704d01bf784ba1","permalink":"/events/presentations/spatial-seminar-2021-04-09/","publishdate":"2021-04-09T15:30:00Z","relpermalink":"/events/presentations/spatial-seminar-2021-04-09/","section":"events","summary":"A brief introduction into how to integrate spatial covariates into ANOVA-based analysis of field trials laid out in a lattice pattern.","tags":["spatial statistics","field experiments"],"title":"Routine Incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"events"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"R  What They Forgot to Teach You (about using R)  SAS  Data Import and Manipulation Linear Regression Mixed Model ANOVA Mixed Model ANCOVA and Dummy Variable Regression Generalized Mixed Model ANOVA Power and Sample Size Estimation Graphics and Plotting  Other Resources  Spatial Analysis for Agricultural Field Experiments Rstats4Ag (using R and SAS) Multivariate Statistical Machine Learning Methods for Genomic Prediction Basic Introduction to Linear models Mixed models in R Data Science for Agriculture in R Mixed Models for Agriculture in R  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a88848674f199fc27d42cdcdd983e4c","permalink":"/tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/","section":"","summary":"R  What They Forgot to Teach You (about using R)  SAS  Data Import and Manipulation Linear Regression Mixed Model ANOVA Mixed Model ANCOVA and Dummy Variable Regression Generalized Mixed Model ANOVA Power and Sample Size Estimation Graphics and Plotting  Other Resources  Spatial Analysis for Agricultural Field Experiments Rstats4Ag (using R and SAS) Multivariate Statistical Machine Learning Methods for Genomic Prediction Basic Introduction to Linear models Mixed models in R Data Science for Agriculture in R Mixed Models for Agriculture in R  ","tags":null,"title":"Tutorials","type":"page"}]